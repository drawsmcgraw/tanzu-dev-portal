<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>VMware Tanzu Developer Center ‚Äì CI/CD</title>
    <link>/guides/ci-cd/</link>
    <description>Recent content in CI/CD on VMware Tanzu Developer Center</description>
    <generator>Hugo -- gohugo.io</generator>
    
	  <atom:link href="/guides/ci-cd/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      
      <title>Guides: What is CI/CD?</title>
      
      <link>/guides/ci-cd/ci-cd-what-is/</link>
      <pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/guides/ci-cd/ci-cd-what-is/</guid>
      <description>

        
        &lt;p&gt;Modern distributed architectures together with cloud environments offer a powerful toolkit for delivering applications that quickly deliver business value. No industry is immune to disruption, however, and you can no longer afford a plodding release cycle with nebulous review boards and heavyweight gates that slow development to a crawl. You need to move fast but you cannot afford to break things. You need deployment pipelines.&lt;/p&gt;
&lt;p&gt;So, how can you ensure releases don‚Äôt bring down production? By leveraging continuous integration (CI) and continuous delivery (CD), you can rapidly deliver features and functionality while still getting a good night‚Äôs sleep!&lt;/p&gt;
&lt;h2 id=&#34;how-did-we-get-here&#34;&gt;How did we get here?&lt;/h2&gt;
&lt;p&gt;In the early days of web development, servers were handcrafted and builds were bespoke. Deployment ‚Äúscripts‚Äù were often littered with manual steps and, unsurprisingly, mistakes were common. Such an approach worked, for some definition of worked, at least when software development was measured in quarterly releases. But you don‚Äôt live in that time in human history. Today, many companies release new versions of their software products continuously, sometimes multiple times a day.&lt;/p&gt;
&lt;p&gt;To support these more dynamic environments, companies are increasingly turning to automation to ensure a consistent, repeatable delivery process. While artisanal coffee may make your morning better, a build process that cannot be replicated isn‚Äôt anyone&amp;rsquo;s definition of a good way to start your day. You need a reliable process. You need a deployment pipeline.&lt;/p&gt;
&lt;p&gt;CI/CD puts you and your customers in the driver&amp;rsquo;s seat by allowing you to deploy features and functionality on your schedule. A deployment pipeline does &lt;strong&gt;not&lt;/strong&gt; require code to go from commit to production in minutes, though robust pipelines can &lt;em&gt;enable&lt;/em&gt; that timeline and companies with years of experience may choose to release quickly, at least with low-risk changes. But if you‚Äôre releasing code early and often, how do you avoid the pain of broken software and late nights in a windowless conference room?&lt;/p&gt;
&lt;p&gt;Automation to the rescue! Instead of thoughts and prayers every time you deploy software, pipelines ensure code that gets to production has endured a gauntlet. Code must survive a set of automated tests, code quality scans, performance tests, chaos engineering, and whatever other gates you need to give you confidence.&lt;/p&gt;
&lt;h3 id=&#34;what-is-ci&#34;&gt;What is CI?&lt;/h3&gt;
&lt;p&gt;Continuous Integration (CI) has been around for many years; it was originally defined as one of Kent Beck‚Äôs core practices of &lt;a href=&#34;https://www.amazon.com/Extreme-Programming-Explained-Embrace-Change-ebook/dp/B00N1ZN6C0/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Extreme Programming&lt;/a&gt;. CI solves the age-old problem of merge hell, when two (or more!) developers are working in similar parts of the codebase at the same time. While source code management tools could sometimes automatically merge code, developers often spent a significant amount of time staring at diffs trying to amalgamate conflicting changes. Problems were compounded if developers spent days‚Äîor worse, weeks‚Äîworking before merging their code.&lt;/p&gt;
&lt;p&gt;CI could be summarized as merge early, merge often. But it is &lt;a href=&#34;https://martinfowler.com/articles/continuousIntegration.html&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;more than that&lt;/a&gt;. It starts with an automated build separate from the IDE. Code isn‚Äôt just built, though; it is also subject to a barrage of automated tests. From traditional unit tests all the way through to various integration or end-to-end tests, automated testing provides a safety net for developers. A robust test suite allows developers to change code at will because they know that a broken test will alert them to any issues their changes may have caused. And code quality scans can be employed to catch common mistakes and antipatterns, which helps to ensure adherence to agreed-upon standards. Again, deviation is detected and fixed early.&lt;/p&gt;
&lt;p&gt;CI is also a cultural shift. Developers commit early and commit often to main. A CI server like &lt;a href=&#34;https://jenkins.io&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Jenkins&lt;/a&gt; or &lt;a href=&#34;https://concourse-ci.org&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Concourse&lt;/a&gt; monitors the code repository, kicking off the build process on every commit. These builds must be fast (seconds or minutes); if a test or quality scan fails, developers are notified and the problem is handled quickly or the change is rolled back. The goal is to maintain a stable base so that you can be responsive to the challenging environments that companies exist in today.&lt;/p&gt;
&lt;p&gt;Ultimately, CI reduces risk. Developers work in small batches. Bugs are found and fixed quickly. Instead of a frustrating game of whack-a-mole, unintended side effects are manifested early and can be remediated before they grow into unwieldy monsters. With your code in a releasable state (most of the time) you are empowered to deliver functionality on your schedule.&lt;/p&gt;
&lt;h3 id=&#34;what-is-cd&#34;&gt;What is CD?&lt;/h3&gt;
&lt;p&gt;Continuous Delivery (CD) builds upon the foundations of CI and takes it to the next step, where code is released. By adding automation to the release process, it allows you to decide which cadence is most appropriate. The goal is to find issues before your code hits production servers. &lt;a href=&#34;https://martinfowler.com/bliki/ContinuousDelivery.html&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;CD&lt;/a&gt; takes the deployable unit coming out of your CI process and moves it from a development region all the way through to production. This process typically involves a pass through QA and a customer acceptance gate, both of which may involve a manual sign off.&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4 id=&#34;delivery-vs-deployment&#34;&gt;Delivery vs. Deployment&lt;/h4&gt;
&lt;p&gt;There is some ambiguity about the definition of the D in CD. Arguably the most common understanding is that continuous &lt;em&gt;deployment&lt;/em&gt; is one step beyond continuous &lt;em&gt;delivery&lt;/em&gt; where changes that successfully pass through all the deployment pipeline gates are &lt;strong&gt;automatically&lt;/strong&gt; moved to production. In other words, changes are serving live customer traffic within minutes of a commit. Continuous Deployment requires a significant investment in testing to ensure changes do not cause havoc in production. It also typically involves feature flags and other advanced techniques.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;From there, releases move into a staging environment. There should be relatively few issues in staging; they should have been discovered earlier in the process. Staging allows you to throw real-world traffic at your code without actually exposing it to customers validating monitoring, performance, user experience, and whatever other factors matter for your application. If it all checks out in staging, it‚Äôs time to move to a canary release.&lt;/p&gt;
&lt;p&gt;A canary release is literally the canary in the coal mine; you expose a percentage of your production workload to the new version of your code. Canaries serve real-world traffic; if you find errors, you roll back to the previous version. With a robust deployment pipeline, you should have very few issues in production as they should have surfaced earlier in the process. Not only does this allow you to deliver business value on a regular basis, it builds credibility with your customers while significantly reducing your stress level. It is a win-win!&lt;/p&gt;
&lt;h3 id=&#34;how-do-you-move-forward&#34;&gt;How do you move forward?&lt;/h3&gt;
&lt;p&gt;It can be daunting to create a build pipeline from scratch. While your organization probably already has CI tooling in place, there may be little guidance provided for how to start. A blank editor is terrifying. Don‚Äôt start at zero, use &lt;a href=&#34;https://spring.io/blog/2018/11/13/spring-cloud-pipelines-to-cloud-pipelines-migration&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Cloud Pipelines&lt;/a&gt; as a base. Whether you use it for inspiration or as a set of starter scripts that you customize, Cloud Pipelines is worth some of your time.&lt;/p&gt;
&lt;p&gt;If you are new to the CI/CD journey, don‚Äôt neglect the cultural shift inherent in any technical change. Some developers reject the build break notifications, some going so far as removing themselves from the email list. Be sure everyone understands the benefits of CI/CD, including increased speed-to-market, stable builds, and reduced drama around releases. Software development has changed dramatically in recent years; no longer can you afford to say ‚ÄúThat‚Äôs how we‚Äôve always done it.‚Äù Applications are evolving rapidly, which requires you to &lt;a href=&#34;https://github.blog/2015-12-15-move-fast/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Move Fast and Fix Things&lt;/a&gt;. CI/CD will not just help you deliver for your customers; it will help you sleep better at night.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: Automated Code to URL on Kubernetes using Cloud Native Buildpacks, Knative and ArgoCD</title>
      
      <link>/guides/ci-cd/cnbp-knative-argocd/</link>
      <pubDate>Wed, 12 May 2021 00:00:00 +0000</pubDate>
      
      <guid>/guides/ci-cd/cnbp-knative-argocd/</guid>
      <description>

        
        &lt;p&gt;Technologies like Docker and Kubernetes simplify the process of building, running and maintaining cloud native applications. At the same time taking source code, building it into a container image and turning it into a deployed application on Kubernetes can be a time consuming process. A large part of the process can be automated with Continuous Integration Systems (CI) and Continuous Deployment(CD) Systems. However there are stages in the build and deployment phases that still need to be defined manually. Most CI systems aren&amp;rsquo;t application aware. They cannot build automated Docker images from source code unless explicitly defined in a spec file like &lt;a href=&#34;https://docs.docker.com/engine/reference/builder/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Dockerfiles&lt;/a&gt; or a config file that a CI system can understand. Due to which, Apart from writing application code, you also need to manually define and test Dockerfiles or config files to convert source code into an executable container image. When deploying the image onto Kubernetes, you need to then define various Kubernetes constructs needed to run the application. Like &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/deployment/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Deployments&lt;/a&gt;, &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;StatefulSets&lt;/a&gt;, &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/service/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Services&lt;/a&gt;, &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/ingress/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Ingress&lt;/a&gt; etc. This process can add errors, security gaps and overhead.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/cnbp-knative-argocd/dev-process.png&#34; alt=&#34;Building and Running Applications on Kubernetes&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;In this Guide we are going to show how to automate building of Container Images and buildout of Kubernetes Resources using Cloud Native Buildpacks and Knative respectively. We will also demonstrate how this can process can be further optimized using &lt;strong&gt;GitOps&lt;/strong&gt; style practices via ArgoCD. We will work with a sample application based on Spring and run the application on Kubernetes with an addressable URL. We will use:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://buildpacks.io/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Cloud Native Buildpacks&lt;/a&gt; to automate Container Image build process. Cloud Native Buildpacks is a specification that defines how OCI compliant containers can be build, removing the need to specify or build &lt;code&gt;Dockerfiles&lt;/code&gt;. They can automatically detect the language an application is written in and determine the best and most secure way to package applications in a container image. Cloud Native Buildpacks can also be used to update container images easily for any changes. For this guide we will use an implementation of Cloud Native Buildpacks called &lt;a href=&#34;https://github.com/pivotal/kpack&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;kpack&lt;/a&gt;. kpack lets you use deploy Cloud Native Buildpacks on a Kubernetes Cluster. (See &lt;a href=&#34;../../containers/cnb-what-is/&#34;&gt;What are Cloud Native Buildpacks?&lt;/a&gt; for more on Cloud Native Buildpacks, and kpack.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://knative.dev/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Knative&lt;/a&gt; to automatically generate an Ingress Service with URL and other Kubernetes Resources for the container image that was built using Cloud Native Buildpacks. Knative Serving automates the process of creating Kubernetes objects needed for an application like Deployment, Replicasets, Services etc.,  eliminating the need to write complex Kubernetes YAML files.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://argoproj.github.io/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;ArgoCD&lt;/a&gt; to automate deployment pipeline pushing container images on to Kubernetes using Knative. ArgoCD helps deploy application continuously using GitOps methodology. It can take specifications like Kubernetes resources, Knative, Kustomize etc. to deploy application on Kubernetes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A sample application called &lt;a href=&#34;https://github.com/Boskey/spring-petclinic&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Petclinic&lt;/a&gt; that is based on &lt;a href=&#34;https://spring.io/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Spring&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://kind.sigs.k8s.io/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Kind&lt;/a&gt; as a local Kubernetes Cluster&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In summary, our overall workflow will be to take the sample application in Spring, use Cloud Native Buildpacks/kpack to convert source code into a container image, use Knative Serving to create a deployment using ArgoCD. This process will eliminate the need to write &lt;code&gt;Dockerfiles&lt;/code&gt; or any &lt;code&gt;Kubernetes&lt;/code&gt; resource &lt;code&gt;YAML&lt;/code&gt; files.&lt;/p&gt;
&lt;h2 id=&#34;assumptions-and-prerequisites&#34;&gt;Assumptions and prerequisites&lt;/h2&gt;
&lt;p&gt;There are a few things you will need before getting started&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;You have &lt;a href=&#34;https://kubernetes.io/docs/tasks/tools/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;kubectl&lt;/a&gt;, a tool to interact with Kubernetes Cluster installed.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.docker.com/products/docker-desktop&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Docker Desktop&lt;/a&gt; is installed on your laptop/machine with at least 4 GB of memory and 4 CPU&amp;rsquo;s allocated to Docker Resources.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You have access to &lt;a href=&#34;https://hub.docker.com/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Docker Hub Repository&lt;/a&gt; to store container images.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You have an account in &lt;a href=&#34;https://github.com/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Github&lt;/a&gt; to clone the app &lt;em&gt;Petclinic&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;1-prepare-a-kubernetes-cluster-and-clone-sample-application&#34;&gt;1. Prepare a Kubernetes Cluster and clone Sample Application&lt;/h3&gt;
&lt;p&gt;We will deploy a Kind cluster using Docker Desktop and install &lt;a href=&#34;https://projectcontour.io/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Contour&lt;/a&gt; on it to help provide Ingress management. Contour along with &lt;a href=&#34;https://www.envoyproxy.io/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Envoy&lt;/a&gt; Proxy will help create service and URL management for Knative.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;brew install kind
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Create a Kubernetes Cluster called &lt;em&gt;tdp-guide&lt;/em&gt; and set &lt;strong&gt;Kubectl&lt;/strong&gt; context to &lt;em&gt;tdp-guide&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kind create cluster --name tdp-guide
kubectl cluster-info --context kind-tdp-guide
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Log onto Github and Fork the repository for our sample app &lt;a href=&#34;https://github.com/Boskey/spring-petclinic&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Petclinic&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;2-install-knative-serving&#34;&gt;2. Install Knative Serving&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;kubectl apply -f https://github.com/knative/serving/releases/download/v0.22.0/serving-crds.yaml
kubectl apply -f https://github.com/knative/serving/releases/download/v0.22.0/serving-core.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;3-install-contour-ingress-controller&#34;&gt;3. Install Contour Ingress Controller&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;kubectl apply -f https://github.com/knative/net-contour/releases/download/v0.22.0/contour.yaml
kubectl apply -f https://github.com/knative/net-contour/releases/download/v0.22.0/net-contour.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Change Knative Serving config to use Contour Ingress&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl patch configmap/config-network \
  --namespace knative-serving \
  --type merge \
  --patch &#39;{&amp;quot;data&amp;quot;:{&amp;quot;ingress.class&amp;quot;:&amp;quot;contour.ingress.networking.knative.dev&amp;quot;}}&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;4-install-and-configure-cloud-native-buildpack-using-kpack&#34;&gt;4. Install and Configure Cloud Native Buildpack using &lt;em&gt;kpack&lt;/em&gt;&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;kubectl apply -f https://github.com/pivotal/kpack/releases/download/v0.2.2/release-0.2.2.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Cloud Native Buildpacks will need a Repository to Store container images that it will be building. This could be any OCI compliant repository, for this guide we will use Docker Hub. You can easily create and account in Docker Hub if you don&amp;rsquo;t have one.&lt;/p&gt;
&lt;p&gt;We need to create a Docker Hub account credentials &lt;em&gt;secret&lt;/em&gt; in Kubernetes. Use the below command and change the &lt;code&gt;docker-username&lt;/code&gt; to the your repo name in Docker Hub. Change the &lt;code&gt;docker-password&lt;/code&gt; to your account password.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create secret docker-registry tutorial-registry-credentials \
    --docker-username=abc \
    --docker-password=********* \
    --docker-server=https://index.docker.io/v1/\
    --namespace default
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Cloud Native Buildpacks create Container images using a &lt;code&gt;builder&lt;/code&gt; that uses a predefined &lt;code&gt;stack&lt;/code&gt; of container image layers. You can define custom stack, store and builders. For this guide, we are using standard definitions.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl apply -f https://raw.githubusercontent.com/Boskey/spring-petclinic/main/kpack-config/sa.yaml
kubectl apply -f https://raw.githubusercontent.com/Boskey/spring-petclinic/main/kpack-config/store.yaml
kubectl apply -f https://raw.githubusercontent.com/Boskey/spring-petclinic/main/kpack-config/stack.yaml
kubectl apply -f https://raw.githubusercontent.com/Boskey/spring-petclinic/main/kpack-config/builder.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;5-install-and-configure-argocd&#34;&gt;5. Install and Configure ArgoCD&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;kubectl create namespace argocd
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Install ArgoCD CLI&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;brew install argocd
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Since ArgoCD is installed on a Kind cluster, it does not have a &lt;em&gt;Kubernetes Load Balancing Service type&lt;/em&gt; to expose the ArgoCD service. We will manually expose the ArgoCD service using &lt;code&gt;port-forward&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;On a new Terminal,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl port-forward svc/argocd-server -n argocd 8080:443
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Fetch ArgoCD credentials to login via CLI&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=&amp;quot;{.data.password}&amp;quot; | base64 -d
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Copy the output of the above command, that is the &lt;code&gt;admin&lt;/code&gt; password for ArgoCD
Login to ArgoCD&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;argocd login localhost:8080
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;em&gt;username: &lt;code&gt;admin&lt;/code&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;password: &lt;code&gt;&amp;lt;copy-paste from the command above&amp;gt;&lt;/code&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We have now installed Knative Serving, Cloud Native Buildpacks and ArgoCD. Its time to implement a workflow that will take our source code and convert it into a URL.&lt;/p&gt;
&lt;h3 id=&#34;6-build-container-image-using-cloud-native-buildpacks&#34;&gt;6. Build Container Image using Cloud Native Buildpacks&lt;/h3&gt;
&lt;p&gt;We will be using the &lt;em&gt;Petclinic&lt;/em&gt; app, the file &lt;code&gt;petclinic-image-build.yaml&lt;/code&gt; tells kpack where the source code is via the &lt;code&gt;spec.source.git.url&lt;/code&gt; , where to upload and what tag to use for the final container image using &lt;code&gt;spec.tag&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Copy this file and change the &lt;code&gt;spec.tag&lt;/code&gt; to your &lt;code&gt;&amp;lt;docker-repo&amp;gt;/&amp;lt;app-name&amp;gt;&lt;/code&gt; and change the &lt;code&gt;spec.source.git.url&lt;/code&gt; to your Git Repo for Petclinic you forked in Step 1&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: kpack.io/v1alpha1
kind: Image
metadata:
  name: petclinic-image
  namespace: default
spec:
  tag: boskey/app
  serviceAccount: tutorial-service-account
  builder:
    name: my-builder
    kind: Builder
  source:
    git:
      url: https://github.com/spring-projects/spring-petclinic
      revision: 82cb521d636b282340378d80a6307a08e3d4a4c4
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Apply the file using &lt;code&gt;Kubectl&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl apply -f petclinic-image-build.yaml&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This process will take around 5 -10 Minutes to finish depending on the resources Docker Desktop has. Keep watching the images CRD to see if the images is finished building.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl get images.kpack.io
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once the image is build you should see the output of the Docker Hub URL where the Container image is located. The output should be similar to this.
Capture the Image location from your command to list images.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[bsavla] ci-cd üêòkubectl get images.kpack.io
NAME        LATESTIMAGE                                                                                          READY
ttv-image   index.docker.io/boskey/app@sha256:949a72120f888b2f37fdf3be0c439422ce4d2225529aa533aae6f6cb85da9424   True
[bsavla] ci-cd üêò
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;7-update-knative-service-definition&#34;&gt;7. Update Knative Service Definition&lt;/h3&gt;
&lt;p&gt;So far we have built a container image based on the sample app &lt;em&gt;Petclinic&lt;/em&gt;, Now we will deploy the app onto Kubernetes via Knative Serving. ArgoCD will help us automate the deployment. We now need to define a &lt;code&gt;knative-serving&lt;/code&gt; spec for our app. The Git repository you forked already has the below spec in a file called &lt;code&gt;knative-service.yaml&lt;/code&gt; under the &lt;code&gt;knative&lt;/code&gt; folder. This spec file tells Knative where to fetch the container image for the application that needs to be deployed on Kubernetes. Edit the file &lt;code&gt;knative-service.yaml&lt;/code&gt; in the Git repository you forked in step 1. Change the &lt;code&gt;image&lt;/code&gt; property to the image URL you got from kpack (step 6). The file should be under the folder &lt;code&gt;knative&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: serving.knative.dev/v1 # Current version of Knative
kind: Service
metadata:
  name: petclinic-knative # The name of the app
  namespace: default # The namespace the app will use
spec:
  template:
    spec:
      containers:
       - image: index.docker.io/boskey/app@sha256:9595c435357a6105bbd26405d6eaa15bd6a7d4ae4b2e5f42946b169ef9257f76  # generated by kpack
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;8-use-argocd-to-deploy-application&#34;&gt;8. Use ArgoCD to deploy application.&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s create an application using ArgoCD, &lt;strong&gt;Replace the  &lt;code&gt;--repo&lt;/code&gt; URL with the Github repo you forked in Step 1&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;argocd app create petclinic --repo https://github.com/Boskey/spring-petclinic.git --path knative --dest-server https://kubernetes.default.svc --dest-namespace default
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This tells ArgoCD to create an application called &lt;em&gt;petclinic&lt;/em&gt; on the local cluster and the &lt;em&gt;&lt;code&gt;--path&lt;/code&gt;&lt;/em&gt; variable tells ArgoCD &lt;em&gt;how&lt;/em&gt; to deploy. In our case the &lt;code&gt;--path&lt;/code&gt; variable points to the &lt;code&gt;knative-serving&lt;/code&gt; specification. So ArgoCD will pass the &lt;code&gt;knative-serving&lt;/code&gt; specification to the local Kubernetes cluster, where the CRD for Knative will understand how to deploy the application and will automatically create Deployments, resource pools, Services etc.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s Sync the app in ArgoCD&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;argocd app sync petclinic
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This will deploy PetClinic on the Kubernetes Cluster along with a URL petclinic-knative.default.example.com. You can look at all the resource that were created&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl get all
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You can look at the application deployed.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl -n contour-external port-forward svc/envoy 8080:80 
curl -H &amp;quot;Host: petclinic-knative.default.example.com&amp;quot; localhost:8080
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If you want to look at the application on your browser, create a DNS entry in your &lt;code&gt;/etc/hosts&lt;/code&gt; file as below&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;127.0.0.1       petclinic-knative.default.example.com
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And browse to &lt;a href=&#34;http://localhost:8080&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;http://localhost:8080&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You should see the Petclinic Application deployed there.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s say you have an update to the PetClinic application, you apply your changes and push them to the repo on Github. To deploy the newer version, all you have to do is create a new container image using &lt;code&gt;kpack&lt;/code&gt; and update the knative-serving specification file with the new image location at &lt;code&gt;knative-service.yaml&lt;/code&gt;. When synced, ArgoCD will detect the change in the file, and re-deploy the application with the newer container image using knative-serving. Knative will detect that this is an updated version of the same application and will deploy the new version with an updated revision.&lt;/p&gt;
&lt;p&gt;You can also create a new deployment for different pipelines like &lt;code&gt;staging&lt;/code&gt; by creating a new application in ArgoCD and pointing them to the same &lt;code&gt;knative-service.yaml&lt;/code&gt; file.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: Getting Started with Tekton Part 1: Hello World</title>
      
      <link>/guides/ci-cd/tekton-gs-p1/</link>
      <pubDate>Mon, 15 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/guides/ci-cd/tekton-gs-p1/</guid>
      <description>

        
        &lt;p&gt;&lt;a href=&#34;https://github.com/tektoncd/pipeline&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Tekton&lt;/a&gt; provides a set of open source Kubernetes resources to build and run &lt;a href=&#34;/guides/ci-cd/ci-cd-what-is/&#34;&gt;CI/CD&lt;/a&gt; pipelines, such as parameterized tasks, inputs and outputs, as well as runtime definitions. This guide will walk you through setting up Tekton on Minikube as well as setting up your first task.&lt;/p&gt;
&lt;h2 id=&#34;before-you-begin&#34;&gt;Before You Begin&lt;/h2&gt;
&lt;p&gt;There&amp;rsquo;s just a few things you&amp;rsquo;ll need before you get started:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/tasks/tools/install-minikube/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Minikube&lt;/a&gt;: Tekton runs on Kubernetes, so to keep things simple, this guide will assume you&amp;rsquo;re using Minikube to get up and running quickly&lt;/li&gt;
&lt;li&gt;A &lt;a href=&#34;https://hub.docker.com/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Docker Hub&lt;/a&gt; account: You&amp;rsquo;ll use this registry to push up container images built from your pipelines.&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;https://github.com/tektoncd/cli&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Tekton CLI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;setting-up-tekton&#34;&gt;Setting Up Tekton&lt;/h2&gt;
&lt;p&gt;While Tekton can run on any Kubernetes cluster, this guide assumes you will be using Minikube. If you&amp;rsquo;d prefer to run Tekton differently, make sure to reference the &lt;a href=&#34;https://github.com/tektoncd/pipeline/blob/master/docs/install.md&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Installation Guide&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;First, create a Minikube cluster with 4GB of memory and 10GB of storage:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;minikube start --memory&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;4096&lt;/span&gt; --disk-size&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;10g
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once your Minikube environment is created, you can install Tekton by applying the YAML from the &lt;a href=&#34;https://github.com/tektoncd/pipeline/releases&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;latest release&lt;/a&gt;:&lt;/p&gt;
&lt;h3 id=&#34;install-tekton&#34;&gt;Install Tekton&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl apply --filename https://storage.googleapis.com/tekton-releases/pipeline/latest/release.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You can monitor the progress of the install by watching the pods in the newly created &lt;code&gt;tekton-pipelines&lt;/code&gt; namespace:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl get pods --namespace tekton-pipelines --watch
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once the install is complete, you&amp;rsquo;ll see two newly created pods:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;tekton-pipelines-webhook-69796f78cf-b28z4      1/1     Running             &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;          9s
tekton-pipelines-controller-6d55778887-df59t   1/1     Running             &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;          13s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Finally, in a couple of the examples, you&amp;rsquo;ll be pushing up container images to Docker Hub, so create a secret that Tekton can use to log in to Docker Hub with, substituting the placeholder values with your own:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl create secret docker-registry dockercreds --docker-server&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;https://index.docker.io/v1/ --docker-username&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&amp;lt;DOCKERHUB_USERNAME&amp;gt; --docker-password&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&amp;lt;DOCKERHUB_PASSWORD&amp;gt; --docker-email &amp;lt;DOCKERHUB_EMAIL&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;create-your-own-task&#34;&gt;Create Your Own Task&lt;/h2&gt;
&lt;p&gt;What better place to start than with a good &amp;lsquo;ole &amp;ldquo;Hello World&amp;rdquo; example? For this example, you&amp;rsquo;ll start with the most basic building block of a pipeline: a task. This task will simply start up a container, write &amp;ldquo;Hello World&amp;rdquo;, and end:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;tekton.dev/v1beta1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Task&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;echo-hello-world&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;steps&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;echo&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;ubuntu&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;command&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;echo&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;- &lt;span class=&#34;s2&#34;&gt;&amp;#34;Hello World&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Take a moment to digest this part. While a &lt;code&gt;task&lt;/code&gt; can become much more complex, this specific task has no inputs, no outputs, and just a single step. That step (named &lt;code&gt;echo&lt;/code&gt;) uses the &lt;a href=&#34;https://hub.docker.com/_/ubuntu&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Ubuntu image from Docker Hub&lt;/a&gt;, and executes the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;echo &amp;quot;Hello World&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Tekton automatically stores the output of all tasks that are run, so even though the container that will run these tasks will quickly go away, you can reference the results of that task after the fact.&lt;/p&gt;
&lt;p&gt;However, in order to actually run the task, you need to create one more resource, a &lt;code&gt;TaskRun&lt;/code&gt;. This resource defines how to run specific tasks. Consider the following for this example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;tekton.dev/v1beta1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;TaskRun&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;echo-hello-world-task-run&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;taskRef&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;echo-hello-world&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Again, keeping the example as simple as possible, &lt;code&gt;TaskRun&lt;/code&gt; definitions &lt;strong&gt;could&lt;/strong&gt; fill in any parameters required by a task, but in this case, since the task that is being run takes no parameters, this resource only defines the task that it is going to run: &lt;code&gt;echo-hello-world&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;As with any Kubernetes custom resources definition (CRD), this can all be done in one file, which you can find on &lt;a href=&#34;https://github.com/BrianMMcClain/tekton-examples/blob/main/hello-task.yml&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;GitHub&lt;/a&gt;, and use to apply these two examples directly:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl apply -f https://raw.githubusercontent.com/BrianMMcClain/tekton-examples/main/hello-task.yml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If you run &lt;code&gt;kubectl get pods&lt;/code&gt; ‚Äî even if you do so quickly ‚Äî you&amp;rsquo;ll likely see this task already completed:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;NAME                                  READY   STATUS      RESTARTS   AGE
echo-hello-world-task-run-pod-vm6f5   0/1     Completed   &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;          12s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;But as previously mentioned, Tekton stores the results of a &lt;code&gt;TaskRun&lt;/code&gt;, and that&amp;rsquo;s where the &lt;a href=&#34;https://github.com/tektoncd/cli&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Tekton CLI&lt;/a&gt; comes in. First, check out Tekton&amp;rsquo;s description of your &lt;code&gt;TaskRun&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;tkn taskrun describe echo-hello-world-task-run
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;Name:        echo-hello-world-task-run
Namespace:   default
Task Ref:    echo-hello-world
Timeout:     1h0m0s
Labels:
 app.kubernetes.io/managed-by&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;tekton-pipelines
 tekton.dev/task&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;echo-hello-world

üå°Ô∏è  Status

STARTED          DURATION    STATUS
&lt;span class=&#34;m&#34;&gt;10&lt;/span&gt; minutes ago   &lt;span class=&#34;m&#34;&gt;7&lt;/span&gt; seconds   Succeeded

üì® Input Resources

 No input resources

üì° Output Resources

 No output resources

‚öì Params

 No params

ü¶∂ Steps

 NAME     STATUS
 ‚àô &lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt;   ---

üöó Sidecars

No sidecars
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Since the task has no inputs, outputs, or resources, the above is a bit bare. What&amp;rsquo;s important is that the &lt;code&gt;Status&lt;/code&gt; is set to &lt;code&gt;Succeeded&lt;/code&gt;. Great! Next, take a look at the output of that &lt;code&gt;TaskRun&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;tkn taskrun logs echo-hello-world-task-run
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;echo&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; Hello World
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Just as you would expect, the output of the &lt;code&gt;echo&lt;/code&gt; task was &amp;ldquo;Hello World&amp;rdquo;.&lt;/p&gt;
&lt;h2 id=&#34;keep-learning&#34;&gt;Keep Learning&lt;/h2&gt;
&lt;p&gt;In &lt;a href=&#34;/guides/ci-cd/tekton-gs-p2/&#34;&gt;part two of this guide&lt;/a&gt;, you&amp;rsquo;ll learn about inputs into and outputs from tasks, as well as how you can use Cloud Native Buildpacks with Tekton. If you want to dive deeper into what has already been covered, give the &lt;a href=&#34;https://github.com/tektoncd/pipeline#-tekton-pipelines&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;official documentation&lt;/a&gt; a read to learn more!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: Getting Started with Tekton Part 2: Building a Container</title>
      
      <link>/guides/ci-cd/tekton-gs-p2/</link>
      <pubDate>Mon, 15 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/guides/ci-cd/tekton-gs-p2/</guid>
      <description>

        
        &lt;p&gt;In &lt;a href=&#34;/guides/ci-cd/tekton-gs-p1/&#34;&gt;part one of this guide&lt;/a&gt;, you learned how to install Tekton on Minikube, as well as what a basic &lt;code&gt;Task&lt;/code&gt; looks like. In part two, you&amp;rsquo;ll create a more complex &lt;code&gt;Task&lt;/code&gt;, which will use &lt;a href=&#34;https://github.com/GoogleContainerTools/kaniko&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Kaniko&lt;/a&gt; to build and publish a container image. After that, you&amp;rsquo;ll learn how to use a preexisting &lt;code&gt;Task&lt;/code&gt; and provide parameters to build your code using &lt;a href=&#34;https://buildpacks.io/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Cloud Native Buildpacks&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;before-you-begin&#34;&gt;Before You Begin&lt;/h2&gt;
&lt;p&gt;If you went through the lessons in &lt;a href=&#34;/guides/ci-cd/tekton-gs-p1/&#34;&gt;part one of this guide&lt;/a&gt;, you&amp;rsquo;re all set! This guide picks up where that guide left off, using the same Tekton installation on top of Minikube, with the same secrets, service accounts, and other resources defined. If you haven&amp;rsquo;t gone through part one yet, make sure you start there.&lt;/p&gt;
&lt;h2 id=&#34;building-a-container-with-kaniko&#34;&gt;Building a Container with Kaniko&lt;/h2&gt;
&lt;p&gt;Since Tekton is a tool for automating CI/CD pipelines, you probably want to learn how to create and publish container images. For this example, you&amp;rsquo;ll use &lt;a href=&#34;https://github.com/GoogleContainerTools/kaniko&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Kaniko&lt;/a&gt;, a tool used to build container images from a Dockerfile on top of Kubernetes. Kaniko provides its own container image that you can use as a base. By adding your own code and Dockerfile, Kaniko will build and publish a container image based on that Dockerfile.&lt;/p&gt;
&lt;p&gt;You can see the complete example &lt;a href=&#34;https://github.com/BrianMMcClain/tekton-examples/blob/main/kaniko-task.yml&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;here on GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;First, since you&amp;rsquo;ll be pushing the resulting container image to Docker Hub, you&amp;rsquo;ll need to create a service account that uses the secret that you created earlier:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;ServiceAccount&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;dockerhub-service&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;secrets&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;dockercreds&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Next, you&amp;rsquo;ll need to define one input for the code that will be built, and one output for where to publish the container image:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;tekton.dev/v1alpha1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;PipelineResource&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;sinatra-hello-world-git&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;type&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;git&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;params&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;revision&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;value&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;main&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;url&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;value&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;https://github.com/BrianMMcClain/sinatra-hello-world&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This introduces a new concept‚Äîa &lt;code&gt;PipelineResource&lt;/code&gt;, e‚Äîwhich defines an input into, or an output from, a &lt;code&gt;Task&lt;/code&gt;. If you want to learn more, make sure to check out the &lt;a href=&#34;https://github.com/tektoncd/pipeline/blob/master/docs/resources.md&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;code&gt;PipelineResource&lt;/code&gt; documentation&lt;/a&gt;. This &lt;code&gt;PipelineResource&lt;/code&gt; is of type &lt;code&gt;git&lt;/code&gt;, which points to the branch named &lt;code&gt;main&lt;/code&gt; of the code to build on GitHub. It also gives it the name &amp;ldquo;sinatra-hello-world-git&amp;rdquo;, which is what you‚Äôll use to reference it later on in the example.&lt;/p&gt;
&lt;p&gt;You&amp;rsquo;ll need one other &lt;code&gt;PipelineResource&lt;/code&gt; to define where to publish the container image:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;tekton.dev/v1alpha1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;PipelineResource&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;sinatra-hello-world-tekton-demo-image&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;type&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;params&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;url&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;value&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;&amp;lt;DOCKER_USERNAME&amp;gt;/sinatra-hello-world-tekton-demo&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This &lt;code&gt;PipelineResource&lt;/code&gt; is of type &lt;code&gt;image&lt;/code&gt;, as in a container image. It&amp;rsquo;s also been given the name &amp;ldquo;sinatra-hello-world-tekton-demo-image&amp;rdquo;. In this case, it simply takes the image name and tag. Since no full URL is provided, it&amp;rsquo;s assumed that it will be published to Docker Hub, but you can also point to your own container registry.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; Make sure to replace &amp;lt;DOCKER_USERNAME&amp;gt; with your Docker Hub username&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;With your input and output defined, it&amp;rsquo;s time to create the &lt;code&gt;Task&lt;/code&gt; that will build the container. Take some time to carefully read this through:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;tekton.dev/v1beta1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Task&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;build-docker-image-from-git-source&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;params&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;pathToDockerFile&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;type&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;string&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;description&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;The path to the dockerfile to build&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;default&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;$(resources.inputs.docker-source.path)/Dockerfile&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;pathToContext&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;type&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;string&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;description&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;        The build context used by Kaniko
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;        (https://github.com/GoogleContainerTools/kaniko#kaniko-build-contexts)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;default&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;$(resources.inputs.docker-source.path)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;resources&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;inputs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;docker-source&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;type&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;git&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;outputs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;builtImage&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;type&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;steps&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;build-and-push&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;gcr.io/kaniko-project/executor:v0.17.1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# specifying DOCKER_CONFIG is required to allow kaniko to detect docker credential&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;env&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;DOCKER_CONFIG&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;value&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;/tekton/home/.docker/&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;command&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;/kaniko/executor&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;- --&lt;span class=&#34;l&#34;&gt;dockerfile=$(params.pathToDockerFile)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;- --&lt;span class=&#34;l&#34;&gt;destination=$(resources.outputs.builtImage.url)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;- --&lt;span class=&#34;l&#34;&gt;context=$(params.pathToContext)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Here, a new &lt;code&gt;Task&lt;/code&gt; named &amp;ldquo;build-docker-image-from-git-source&amp;rdquo; is created. The best way to understand this is to walk through the spec step by step.&lt;/p&gt;
&lt;p&gt;First, there are two &lt;code&gt;params&lt;/code&gt; that the &lt;code&gt;Task&lt;/code&gt; will expect:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;pathToDockerFile&lt;/code&gt; ‚Äî Where the Dockerfile is in your code, defaulting to the root directory.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pathToContext&lt;/code&gt; ‚Äî The directory in which Kaniko should look for your code. If no alternative directory is provided, it assumes that the root directory of your code is the build context.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Next, it defines two &lt;code&gt;resources&lt;/code&gt; that it expects. It expects one &lt;code&gt;input&lt;/code&gt; (which it will refer to as &amp;ldquo;docker-source&amp;rdquo;) of type &lt;code&gt;git&lt;/code&gt;. It also expects one output (referred to as &lt;code&gt;builtImage&lt;/code&gt;) of type &lt;code&gt;image&lt;/code&gt;. As a reminder, a &lt;code&gt;Task&lt;/code&gt; is simply outlining what inputs and output it expects, but it&amp;rsquo;s not yet defining them. You might expect that these will match the two &lt;code&gt;PipelineResource&lt;/code&gt; objects that were defined earlier, and you&amp;rsquo;d be right. The final piece of YAML that you&amp;rsquo;ll define later will tie the two together.&lt;/p&gt;
&lt;p&gt;Finally, the &lt;code&gt;Task&lt;/code&gt; needs to define what &lt;code&gt;steps&lt;/code&gt; to take. Since Kaniko contains all the logic it needs inside the container image, there&amp;rsquo;s just a single step. Using the Kaniko container image, this step runs the &lt;code&gt;/kaniko/executor&lt;/code&gt; command with three flags: &lt;code&gt;--dockerfile&lt;/code&gt;, &lt;code&gt;--destination&lt;/code&gt;, and &lt;code&gt;--context.&lt;/code&gt; Each of these flags takes in the information defined in the &lt;code&gt;params&lt;/code&gt; and &lt;code&gt;resources&lt;/code&gt; sections.&lt;/p&gt;
&lt;p&gt;Phew, that was a lot to digest. Take a moment to make sure you understand each of these sections. At a high level, this Task takes two parameters with two inputs and runs one executable.&lt;/p&gt;
&lt;p&gt;There&amp;rsquo;s one final piece, which is the &lt;code&gt;TaskRunner&lt;/code&gt; to run this &lt;code&gt;Task&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;tekton.dev/v1beta1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;TaskRun&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;build-docker-image-from-git-source-task-run&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;serviceAccountName&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;dockerhub-service&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;taskRef&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;build-docker-image-from-git-source&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;params&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;pathToDockerFile&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;value&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Dockerfile&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;resources&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;inputs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;docker-source&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;resourceRef&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;sinatra-hello-world-git&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;outputs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;builtImage&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;resourceRef&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;sinatra-hello-world-tekton-demo-image&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This &lt;code&gt;TaskRun&lt;/code&gt; object says that you want to run the &lt;code&gt;build-docker-image-from-git-source&lt;/code&gt; &lt;code&gt;Task&lt;/code&gt; that you just defined and provide the two &lt;code&gt;PipelineResource&lt;/code&gt; objects that you defined as resources. This is how Tekton knows that it should use the &lt;code&gt;sinatra-hello-world-git&lt;/code&gt; &lt;code&gt;PipelineResource&lt;/code&gt; for the &lt;code&gt;docker-source&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;One other thing to notice is that the &lt;code&gt;pathToDockerFile&lt;/code&gt; parameter was defined, despite being the same as the default value. This is done to show how &lt;code&gt;params&lt;/code&gt; are defined in &lt;code&gt;TaskRun&lt;/code&gt; objects, but note as well that &lt;code&gt;pathToContext&lt;/code&gt; is omitted. If &lt;code&gt;params&lt;/code&gt; have a default value, they do not necessarily need to be defined in your &lt;code&gt;TaskRun&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If you want an easy way apply this all at once, you can store your Docker Hub username in a Bash variable:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;DOCKER_USERNAME&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&amp;lt;DOCKERHUB_USERNAME&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Then you can run the following one-liner to apply all of the objects at once:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;wget -O - https://raw.githubusercontent.com/BrianMMcClain/tekton-examples/main/kaniko-task.yml &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; sed -e &lt;span class=&#34;s2&#34;&gt;&amp;#34;s/\&amp;lt;DOCKER_USERNAME\&amp;gt;/&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$DOCKER_USERNAME&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;/&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; kubectl apply -f -
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once applied, make sure to check the status of the &lt;code&gt;TaskRun&lt;/code&gt; using the Tekton CLI:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;tkn taskrun describe build-docker-image-from-git-source-task-run
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;ame:              build-docker-image-from-git-source-task-run
Namespace:         default
Task Ref:          build-docker-image-from-git-source
Service Account:   dockerhub-service
Timeout:           1h0m0s
Labels:
 app.kubernetes.io/managed-by&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;tekton-pipelines
 tekton.dev/task&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;build-docker-image-from-git-source

üå°Ô∏è  Status

STARTED         DURATION    STATUS
&lt;span class=&#34;m&#34;&gt;8&lt;/span&gt; seconds ago   ---         Running

üì® Input Resources

 NAME              RESOURCE REF
 ‚àô docker-source   sinatra-hello-world-git

üì° Output Resources

 NAME           RESOURCE REF
 ‚àô builtImage   sinatra-hello-world-tekton-demo-image

‚öì Params

 NAME                 VALUE
 ‚àô pathToDockerFile   Dockerfile

ü¶∂ Steps

 NAME                                         STATUS
 ‚àô image-digest-exporter-grgxm                ---
 ‚àô git-source-sinatra-hello-world-git-2w7hp   ---
 ‚àô create-dir-builtimage-dzt9g                ---
 ‚àô build-and-push                             ---

üöó Sidecars

No sidecars
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In this case, it looks like the &lt;code&gt;Status&lt;/code&gt; is already &lt;code&gt;Running&lt;/code&gt;, great! Take a look at the logs to monitor the build:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;tkn taskrun logs build-docker-image-from-git-source-task-run -f
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If all goes well, once the logs finish, you should see your new image up in Docker Hub!&lt;/p&gt;
&lt;h2 id=&#34;cloud-native-buildpacks&#34;&gt;Cloud-Native Buildpacks&lt;/h2&gt;
&lt;p&gt;So far, you&amp;rsquo;ve been defining your own tasks and steps to run. However, one of the benefits of Tekton&amp;rsquo;s design is that since each component is shareable through YAML files, you can plug in a &lt;code&gt;Task&lt;/code&gt; developed by someone else. For this example, you&amp;rsquo;ll be bringing in a &lt;code&gt;Task&lt;/code&gt; that&amp;rsquo;s already defined, specifically one to &lt;a href=&#34;https://github.com/tektoncd/catalog/tree/master/buildpacks&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;use Cloud Native Buildpacks&lt;/a&gt;. If you&amp;rsquo;re unfamiliar with Cloud Native Buildpacks, make sure to check out &lt;a href=&#34;/guides/containers/cnb-what-is/&#34;&gt;Cloud Native Buildpacks: What Are They?&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To install the &lt;code&gt;Task&lt;/code&gt;, you can use &lt;code&gt;kubectl apply&lt;/code&gt;, passing the URL to the YAML directly:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl apply -f https://raw.githubusercontent.com/tektoncd/catalog/master/buildpacks/buildpacks-v3.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Much like how you can use the Tekton CLI to describe a &lt;code&gt;TaskRun&lt;/code&gt;, you can also use it to describe a &lt;code&gt;Task&lt;/code&gt; to see what resources, parameters, and steps it defines:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;tkn task describe buildpacks-v3
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;Name:        buildpacks-v3
Namespace:   default

üì® Input Resources

 NAME       TYPE
 ‚àô &lt;span class=&#34;nb&#34;&gt;source&lt;/span&gt;   git

üì° Output Resources

 NAME      TYPE
 ‚àô image   image

‚öì Params

 NAME               TYPE     DESCRIPTION              DEFAULT VALUE
 ‚àô BUILDER_IMAGE    string   The image on which ...   ---
 ‚àô CACHE            string   The name of the per...   empty-dir
 ‚àô USER_ID          string   The user ID of the ...   &lt;span class=&#34;m&#34;&gt;1000&lt;/span&gt;
 ‚àô GROUP_ID         string   The group ID of the...   &lt;span class=&#34;m&#34;&gt;1000&lt;/span&gt;
 ‚àô PROCESS_TYPE     string   The default process...   web
 ‚àô SOURCE_SUBPATH   string   A subpath within th...

ü¶∂ Steps

 ‚àô prepare
 ‚àô detect
 ‚àô analyze
 ‚àô restore
 ‚àô build
 ‚àô &lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt;

üóÇ  Taskruns

NAME                               STARTED        DURATION    STATUS
build-spring-api-with-buildpacks   &lt;span class=&#34;m&#34;&gt;18&lt;/span&gt; hours ago   &lt;span class=&#34;m&#34;&gt;7&lt;/span&gt; minutes   Succeeded
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Here you can see this &lt;code&gt;Task&lt;/code&gt; expects an input resource of type &lt;code&gt;git&lt;/code&gt; and an output resource of type &lt;code&gt;image&lt;/code&gt;. You can define these just as you did in the previous example. For this example, you&amp;rsquo;ll be building a different application, &lt;a href=&#34;https://github.com/BrianMMcClain/spring-boot-api-demo&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;in Spring&lt;/a&gt;. Start by creating the Service Account to authenticate against Docker Hub, the input &lt;code&gt;git&lt;/code&gt; resource, and the output &lt;code&gt;image&lt;/code&gt; resource:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nn&#34;&gt;---&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;ServiceAccount&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;dockerhub-service&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;secrets&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;regcred&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# Create secret for your container registry&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;---&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;tekton.dev/v1alpha1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;PipelineResource&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;spring-api-git&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;type&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;git&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;params&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;revision&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;value&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;main&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;url&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;value&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;https://github.com/BrianMMcClain/spring-boot-api-demo&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;---&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;tekton.dev/v1alpha1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;PipelineResource&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;spring-api-tekton-demo&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;type&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;params&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;url&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;value&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;&amp;lt;DOCKER_USERNAME&amp;gt;/spring-api-tekton-demo&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This should all look familiar from the previous example. The service account uses the secret defined at the beginning of the guide, the &lt;code&gt;git&lt;/code&gt; &lt;code&gt;PipelineResource&lt;/code&gt; points to the code that you&amp;rsquo;ll be building, and the image &lt;code&gt;PipelineResource&lt;/code&gt; will tell Tekton where to send the resulting image.&lt;/p&gt;
&lt;p&gt;Finally, define the &lt;code&gt;TaskRun&lt;/code&gt; to tie it all together:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;tekton.dev/v1alpha1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;TaskRun&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;build-spring-api-with-buildpacks&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;serviceAccountName&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;dockerhub-service&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;taskRef&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;buildpacks-v3&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;inputs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;resources&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;source&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;resourceRef&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;spring-api-git&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;params&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;BUILDER_IMAGE&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;value&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;cloudfoundry/cnb:bionic&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;outputs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;resources&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;resourceRef&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;spring-api-tekton-demo&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;As you might have expected, this denotes your two &lt;code&gt;PipelineResource&lt;/code&gt; objects as the input and output resources. It also declares that you&amp;rsquo;ll be using the &lt;code&gt;cloudfoundry/cnb:bionic&lt;/code&gt; image for the buildpack builder.&lt;/p&gt;
&lt;p&gt;As with the previous example, you can apply this all at once by first storing your Docker Hub username in a Bash variable:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;DOCKER_USERNAME&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&amp;lt;DOCKERHUB_USERNAME&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Then you can apply the YAML directly:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;wget -O - https://raw.githubusercontent.com/BrianMMcClain/tekton-examples/main/cnb-spring-api-demo.yml &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; sed -e &lt;span class=&#34;s2&#34;&gt;&amp;#34;s/\&amp;lt;DOCKER_USERNAME\&amp;gt;/&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$DOCKER_USERNAME&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;/&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; kubectl apply -f -
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Check the status with &lt;code&gt;tkn taskrun describe&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;tkn taskrun describe build-spring-api-with-buildpacks
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;Name:              build-spring-api-with-buildpacks
Namespace:         default
Task Ref:          buildpacks-v3
Service Account:   dockerhub-service
Timeout:           1h0m0s
Labels:
 app.kubernetes.io/managed-by&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;tekton-pipelines
 tekton.dev/task&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;buildpacks-v3

üå°Ô∏è  Status

STARTED         DURATION    STATUS
&lt;span class=&#34;m&#34;&gt;2&lt;/span&gt; seconds ago   ---         Running&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;Pending&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;

üì® Input Resources

 NAME       RESOURCE REF
 ‚àô &lt;span class=&#34;nb&#34;&gt;source&lt;/span&gt;   spring-api-git

üì° Output Resources

 NAME      RESOURCE REF
 ‚àô image   spring-api-tekton-demo

‚öì Params

 NAME              VALUE
 ‚àô BUILDER_IMAGE   cloudfoundry/cnb:bionic

ü¶∂ Steps

 NAME                                STATUS
 ‚àô analyze                           ---
 ‚àô detect                            ---
 ‚àô prepare                           ---
 ‚àô &lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt;                            ---
 ‚àô build                             ---
 ‚àô restore                           ---
 ‚àô git-source-spring-api-git-sg9vs   ---
 ‚àô create-dir-image-8fk7w            ---
 ‚àô image-digest-exporter-sxrxt       ---

üöó Sidecars

No sidecars
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You can also follow along with the logs with &lt;code&gt;tkn taskrun logs&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;tkn taskrun logs build-spring-api-with-buildpacks -f
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once complete, you&amp;rsquo;ll see your newly created container image up in Docker Hub! Note that there was never a Dockerfile created or any other set of instructions on how to build this container. Instead, Cloud Native Buildpacks looked at your code and determined what it needed in terms of runtime, dependencies, etc.&lt;/p&gt;
&lt;h2 id=&#34;keep-learning&#34;&gt;Keep Learning&lt;/h2&gt;
&lt;p&gt;There&amp;rsquo;s still more to learn, and the best place to go next is the &lt;a href=&#34;https://github.com/tektoncd/pipeline#-tekton-pipelines&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;official documentation&lt;/a&gt;. There are also some great &lt;a href=&#34;https://github.com/tektoncd/pipeline/tree/master/examples&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;examples&lt;/a&gt; for those looking to get some hands-on learning.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: Deploy Cloud Native Apps Using GitLab CI/CD and Cloud Native Buildpacks</title>
      
      <link>/guides/ci-cd/gitlab-ci-cd-cnb/</link>
      <pubDate>Thu, 11 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/guides/ci-cd/gitlab-ci-cd-cnb/</guid>
      <description>

        
        &lt;p&gt;GitLab is a single application built from the ground up for all stages of the DevOps lifecycle that enables  product, development, QA, security, and operations teams to work on the same project concurrently. It provides teams with a single data store, user interface, and permission model across the DevOps lifecycle. Teams can collaborate and work on a project utilizing a single conversation, which significantly reduces cycle time, allowing developers to focus exclusively on building great software quickly.
This tutorial will explain how to create a sample CI/CD pipeline in GitLab and use Cloud Native Buildpacks to package the project source code into deployable containers.&lt;/p&gt;
&lt;h2 id=&#34;what-are-cloud-native-buildpacks&#34;&gt;What Are Cloud Native Buildpacks?&lt;/h2&gt;
&lt;p&gt;Traditionally, in the build stage of the CI/CD cycle, the source code and its dependencies are packaged into containers that can be deployed against any proper container-hosting platform, either on-prem or in the cloud. A Dockerfile is usually used to pass all the commands required to assemble an image.&lt;/p&gt;
&lt;p&gt;The challenge with this process‚Äîespecially for large-scale deployments done frequently‚Äîis making sure that each and every build is identical and complies with the security, software currency, and build rules of the organization.&lt;/p&gt;
&lt;p&gt;This is where buildpacks come in. A buildpack represents a package of all the tools and scripts required to produce a &lt;a href=&#34;https://www.opencontainers.org&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;standards-based&lt;/a&gt;, compliant container runtime.  The result is that developers no longer need to worry about maintaining Dockerfiles, and operators can make sure that all the containers are built using standard, preconfigured, tested, and approved images.
Not only that, because the software development is now abstracted from the underlying build process, it is possible to run a complete rebase for a whole environment (dev, test, staging, production) whether as part of a planned OS upgrade/patching process or in response to a newly identified vulnerability.&lt;/p&gt;
&lt;h2 id=&#34;how-to-use-cloud-native-buildpacks-with-gitlab-cicd&#34;&gt;How to Use Cloud Native Buildpacks with GitLab CI/CD&lt;/h2&gt;
&lt;p&gt;GitLab pipelines are defined in the &lt;code&gt;.gitlab-ci.yml&lt;/code&gt; file; they consist of one or more jobs grouped into stages. If no stages are defined in a pipeline, the following three are assumed: build, test, and deploy. The jobs defined in the &lt;code&gt;.gitlab-ci.yml&lt;/code&gt; file handle the CI/CD tasks required to get the code built, tested, verified, staged, and deployed to one or more target platforms.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/gitlab-ci-cd-cnb/screenshots/image3.png&#34; alt=&#34;&#34;  /&gt;&lt;/p&gt;
&lt;h3 id=&#34;create-a-new-project-in-gitlab-saas--or-using-your-gitlab-instance-if-you-have-a-self-managed-one&#34;&gt;Create a New Project in GitLab SaaS  (or Using Your GitLab Instance If You Have a Self-Managed One)&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Log in to GitLab.com.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Click on ‚ÄúNew Project.‚Äù&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/gitlab-ci-cd-cnb/screenshots/image8.png&#34; alt=&#34;&#34;  /&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;As we are going to use Cloud Native Buildpacks, let‚Äôs create the new project based on the Spring project template by clicking ‚ÄúCreate from Template‚Äù and then choosing the Spring template. This will create a sample Java Spring project, which by default will include a Dockerfile.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/gitlab-ci-cd-cnb/screenshots/image10.png&#34; alt=&#34;&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/gitlab-ci-cd-cnb/screenshots/image12.png&#34; alt=&#34;&#34;  /&gt;&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;As we are going to use a buildpack to build the project, let‚Äôs rename the Dockerfile to &lt;code&gt;backup-dockerfile&lt;/code&gt; to ensure it won‚Äôt be used during the build process. The easiest way to do this is to click the Web IDE link in the top right.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/gitlab-ci-cd-cnb/screenshots/image5.png&#34; alt=&#34;&#34;  /&gt;&lt;/p&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;Click the down arrow next to &lt;code&gt;Dockerfile&lt;/code&gt;, and rename &lt;code&gt;Dockerfile&lt;/code&gt; to &lt;code&gt;backup-dockerfile&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/gitlab-ci-cd-cnb/screenshots/image4.png&#34; alt=&#34;&#34;  /&gt;&lt;/p&gt;
&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;In GitLab, the project pipeline is configured in the &lt;code&gt;.gitlab-ci.yml&lt;/code&gt; file. To add a pipeline file to the project, click the new file icon, and in the ‚ÄúCreate New File‚Äù dialog click  &lt;code&gt;.gitlab-ci.yml&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/gitlab-ci-cd-cnb/screenshots/image11.png&#34; alt=&#34;&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/gitlab-ci-cd-cnb/screenshots/image7.png&#34; alt=&#34;&#34;  /&gt;&lt;/p&gt;
&lt;ol start=&#34;7&#34;&gt;
&lt;li&gt;
&lt;p&gt;To add GitLab Auto DevOps templates, click the template dropdown and choose ‚ÄúAuto DevOps Template.‚Äù&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;This is the whole &lt;a href=&#34;https://docs.gitlab.com/ee/topics/autodevops/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;GitLab Auto DevOps&lt;/a&gt; template, which was built by GitLab engineers based on CI/CD best practices. Auto DevOps aims to simplify the setup and execution of a mature and modern software development lifecycle, but as using the whole Auto DevOps template covers every stage in the CI/CD lifecycle and requires a GitLab Ultimate license, we will trim down the &lt;code&gt;.gitlab-ci.yml&lt;/code&gt; file to only include the build stage. To that end, remove all but the following in the &lt;code&gt;include&lt;/code&gt; section:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;include&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;template&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Jobs/Build.gitlab-ci.yml &lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# https://gitlab.com/gitlab-org/gitlab/blob/master/lib/gitlab/ci/templates/Jobs/Build.gitlab-ci.yml&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;9&#34;&gt;
&lt;li&gt;
&lt;p&gt;In order to instruct the build job to use Cloud Native Buildpacks, add &lt;code&gt;AUTO_DEVOPS_BUILD_IMAGE_CNB_ENABLED: &amp;quot;true&amp;quot;&lt;/code&gt; under the variables section.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;By default, the &lt;code&gt;heroku/buildpacks:18&lt;/code&gt; builder will be used to build the output containers. This can be changed by assigning a different builder to the &lt;code&gt;AUTO_DEVOPS_BUILD_IMAGE_CNB_BUILDER&lt;/code&gt; variable, for example, &lt;code&gt;AUTO_DEVOPS_BUILD_IMAGE_CNB_BUILDER: paketobuildpacks/builder:base&lt;/code&gt;. If you have the &lt;code&gt;pack&lt;/code&gt; CLI installed locally, you can see all of the suggested buildpacks by running &lt;code&gt;pack builder suggest&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;Suggested builders:
	Google:                gcr.io/buildpacks/builder:v1      Ubuntu &lt;span class=&#34;m&#34;&gt;18&lt;/span&gt; base image with buildpacks &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; .NET, Go, Java, Node.js, and Python                                              
	Heroku:                heroku/buildpacks:18              heroku-18 base image with buildpacks &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; Ruby, Java, Node.js, Python, Golang, and PHP                                       
	Paketo Buildpacks:     paketobuildpacks/builder:base     Ubuntu bionic base image with buildpacks &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; Java, .NET Core, NodeJS, Go, Ruby, NGINX, and Procfile                        
	Paketo Buildpacks:     paketobuildpacks/builder:full     Ubuntu bionic base image with buildpacks &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; Java, .NET Core, NodeJS, Go, PHP, Ruby, Apache HTTPD, NGINX, and Procfile     
	Paketo Buildpacks:     paketobuildpacks/builder:tiny     Tiny base image &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;bionic build image, distroless-like run image&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt; with buildpacks &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; Java Native Image and Go   
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;11&#34;&gt;
&lt;li&gt;Pass the environment variables to the running jobs. This can be accomplished several ways:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Directly adding them in the pipeline file (&lt;code&gt;.gitlab-ci.yml&lt;/code&gt;):&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/gitlab-ci-cd-cnb/screenshots/image6.png&#34; alt=&#34;&#34;  /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Defining them on the pipeline level before running the pipeline:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/gitlab-ci-cd-cnb/screenshots/image1.png&#34; alt=&#34;&#34;  /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Adding them under &amp;ldquo;Settings&amp;rdquo; in the left menu -&amp;gt; CI/CD, expanding the &amp;ldquo;Variables&amp;rdquo; tab, clicking the edit pen and changing the value to any other buildpack URL (diagram below), then triggering the pipeline again.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/gitlab-ci-cd-cnb/screenshots/image2.png&#34; alt=&#34;&#34;  /&gt;&lt;/p&gt;
&lt;ol start=&#34;11&#34;&gt;
&lt;li&gt;Some buildpacks, like Google and Paketo, make the generated container available on port 8080. So if you are using the Gitlab Deploy template (part of GitLab Auto DevOps), you will need to change the listening port for the readiness probe from &lt;code&gt;5000&lt;/code&gt; (the default in the template) to &lt;code&gt;8080&lt;/code&gt;. This can be done easily by setting an environment variable named &lt;code&gt;HELM_UPGRADE_EXTRA_ARGS&lt;/code&gt; value to &lt;code&gt;--set service.internalPort=8080 --set service.externalPort=8080&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/gitlab-ci-cd-cnb/screenshots/image9.png&#34; alt=&#34;&#34;  /&gt;&lt;/p&gt;
&lt;h3 id=&#34;use-cloud-native-buildpacks-with-gitlab-in-gitlab-build-job-without-using-the-gitlab-build-template&#34;&gt;Use Cloud Native Buildpacks with GitLab in GitLab Build Job WITHOUT Using the GitLab Build Template&lt;/h3&gt;
&lt;p&gt;GitLab CI/CD also allows you to use your own build script if you so wish. Let‚Äôs look at a build script that reads the &lt;code&gt;AUTO_DEVOPS_BUILD_IMAGE_CNB_BUILDER&lt;/code&gt; environment variable to determine which buildpack to use, which we saw how to set in Step 11:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;build_using_passed_builder&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;stage&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;build&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;script&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;&amp;gt;- &lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;if ! docker info &amp;amp;&amp;gt;/dev/null; then&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;if [ -z &amp;#34;$DOCKER_HOST&amp;#34; ] &amp;amp;&amp;amp; [ &amp;#34;$KUBERNETES_PORT&amp;#34; ]; then&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;export DOCKER_HOST=&amp;#39;tcp://localhost:2375&amp;#39;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;fi&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;fi&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;echo $AUTO_DEVOPS_BUILD_IMAGE_CNB_BUILDER&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;&amp;gt;- &lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;if [  -z &amp;#34;$AUTO_DEVOPS_BUILD_IMAGE_CNB_BUILDER&amp;#34;]; then&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;export AUTO_DEVOPS_BUILD_IMAGE_CNB_BUILDER=&amp;#39;heroku/buildpacks:18&amp;#39;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;fi&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;docker login -u gitlab-ci-token -p $CI_JOB_TOKEN $CI_REGISTRY &lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;wget https://github.com/buildpacks/pack/releases/download/v0.17.0/pack-v0.17.0-linux.tgz&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;tar -xvf pack-v0.17.0-linux.tgz&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;chmod +x pack &lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;mv pack /usr/local/bin/&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;pack build $IMAGE --builder $AUTO_DEVOPS_BUILD_IMAGE_CNB_BUILDER &lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;docker push $IMAGE&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Let‚Äôs walk through this step by step to make sure we understand exactly what‚Äôs happening:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We first determine where Docker, which will be used to build our code, is running.&lt;/li&gt;
&lt;li&gt;We then look at the &lt;code&gt;AUTO_DEVOPS_BUILD_IMAGE_CNB_BUILDER&lt;/code&gt; environment variable to determine which buildpack to use. If this variable isn‚Äôt set, we default to using &lt;code&gt;heroku/buildpacks:18&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Next, we authenticate against Docker using the &lt;code&gt;CI_JOB_TOKEN&lt;/code&gt; variable provided automatically to us by GitLab.&lt;/li&gt;
&lt;li&gt;We then download the &lt;code&gt;pack&lt;/code&gt; CLI and make sure it has the proper permissions to make it executable.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;pack&lt;/code&gt; CLI is uses the buildpack defined in the &lt;code&gt;AUTO_DEVOPS_BUILD_IMAGE_CNB_BUILDER&lt;/code&gt; environment variable to build our code, tagging the container with the name we expect to be provided in the &lt;code&gt;IMAGE&lt;/code&gt; variable.&lt;/li&gt;
&lt;li&gt;Finally, we &lt;code&gt;docker push&lt;/code&gt; the image to the GitLab container registry.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you save this at &lt;code&gt;jobs/build.gitlab-ci.yml&lt;/code&gt;, for example, you can update your &lt;code&gt;.gitlab-ci.yml&lt;/code&gt; file to use this custom build job instead, with the following change:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;include&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;jobs/build.gitlab-ci.yml&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;add-kubernetes-clusters-to-the-gitlab-project&#34;&gt;Add Kubernetes Clusters to the GitLab Project&lt;/h2&gt;
&lt;p&gt;Kubernetes clusters can be added to the GitLab project so that applications can be deployed to them directly from the CI/CD pipeline. To add to the K8s cluster project, please follow the steps in the &lt;a href=&#34;https://docs.gitlab.com/ee/user/project/clusters/add_remove_clusters.html&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Add and Removing Kubernetes Cluster&lt;/a&gt; guide.&lt;/p&gt;
&lt;h2 id=&#34;deploy-the-packaged-container-to-the-kubernetes-clusters&#34;&gt;Deploy the Packaged Container to the Kubernetes Clusters&lt;/h2&gt;
&lt;p&gt;GitLab automates and simplifies the deployment of containers to Kubernetes through the provided AutoDeploy template. Similar to the AutoBuild template, it‚Äôs based on CI/CD best practices and can save operators/developers the hassle of composing and maintaining long deployment scripts and &lt;a href=&#34;/guides/kubernetes/helm-what-is/&#34;&gt;Helm charts&lt;/a&gt;. To use the AutoDeploy template we‚Äôll include the &lt;code&gt;Deploy.gitlab-ci.yml&lt;/code&gt; template under the ‚Äúinclude‚Äù section in the &lt;code&gt;.gitlab-ci.yml&lt;/code&gt; file, so it will look like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;include&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;template&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Jobs/Build.gitlab-ci.yml &lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# https://gitlab.com/gitlab-org/gitlab/blob/master/lib/gitlab/ci/templates/Jobs/Build.gitlab-ci.yml&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;template&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Jobs/Deploy.gitlab-ci.yml &lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# https://gitlab.com/gitlab-org/gitlab/blob/master/lib/gitlab/ci/templates/Jobs/Deploy.gitlab-ci.yml&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Including the template will once again kick off the pipeline, this time adding a second job to deploy to your Kubernetes cluster.
While the use of the AutoDeploy template requires a GitLab Ultimate license, you can get one by starting a &lt;a href=&#34;https://about.gitlab.com/free-trial/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;30-day trial license&lt;/a&gt;. Whether you use the AutoDeploy template or write your own build step, you can build CI/DI pipelines for your GitLab projects for free.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: What is Tekton?</title>
      
      <link>/guides/ci-cd/tekton-what-is/</link>
      <pubDate>Fri, 29 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/guides/ci-cd/tekton-what-is/</guid>
      <description>

        
        &lt;p&gt;&lt;a href=&#34;https://tekton.dev&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Tekton&lt;/a&gt; is a framework for building &lt;a href=&#34;/guides/ci-cd/ci-cd-what-is/&#34;&gt;CI/CD&lt;/a&gt; pipelines on Kubernetes. It provides a set of building blocks to craft a system that meets your exact needs by breaking things down into individual &lt;code&gt;Task&lt;/code&gt; resources, which are in turn chained together in a user-defined &lt;code&gt;Pipeline&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;why-is-it-important&#34;&gt;Why Is It Important?&lt;/h2&gt;
&lt;p&gt;The flexibility of Tekton, combined with it being built with Kubernetes in mind, provides a unique scenario: You can use it to build a pipeline, pick it up, and move it to any other Kubernetes cluster. Both individual tasks and complete pipelines can be shared, pieced together, and tracked in source control.&lt;/p&gt;
&lt;p&gt;Adding your own functionality to Tekton is also straightforward. Since &lt;code&gt;Task&lt;/code&gt; resources are essentially a container image and a list of commands to run, you can create your own tasks to piece into a pipeline if one doesn‚Äôt exist. If a task to build Java code didn‚Äôt exist, for example, you could create a container that has the JDK and Maven.&lt;/p&gt;
&lt;h2 id=&#34;how-does-it-work&#34;&gt;How Does It Work?&lt;/h2&gt;
&lt;p&gt;Tekton can be thought of as the composition of two different resources:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;Task&lt;/code&gt; ‚Äì A resource that accomplishes a specific action. Tasks are the pieces that take specific inputs and produce specific outputs.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Pipeline&lt;/code&gt;‚Äì Defines a series of &lt;code&gt;Task&lt;/code&gt; resources, the order in which to run them, as well as their inputs, outputs, and parameters.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For example, you may want to develop a &lt;code&gt;Pipeline&lt;/code&gt; to test and build some Go code and then create a container image to run the code. You would probably define your Git repository as an input, and both an artifact repository and container registry as outputs. You‚Äôd then define three &lt;code&gt;Task&lt;/code&gt; resources:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Task 1&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Input&lt;/strong&gt;: Git repository&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Step&lt;/strong&gt;: Test the Go code&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: None&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Task 2&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Input&lt;/strong&gt;: Git repository&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Step&lt;/strong&gt;: Build the Go code&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: Artifact repository to store the built code&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Task 4&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Input&lt;/strong&gt;: Compiled Go code from the artifact repository&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Step&lt;/strong&gt;: Create the container image&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: Registry to store the container image&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Of course, the exact tasks depend on the tools, languages, and frameworks that you‚Äôre using. But thanks to the flexibility of Tekton, you‚Äôre not limited to just what it can do out of the box. If you need to create a &lt;code&gt;Task&lt;/code&gt; to build some code in a language that‚Äôs not currently supported, it‚Äôs as easy as providing Tekton with a container that has the tools to build the code and telling it which commands to run.&lt;/p&gt;
&lt;h2 id=&#34;how-can-i-use-it&#34;&gt;How Can I Use It?&lt;/h2&gt;
&lt;p&gt;There are some great resources to get started with Tekton.  &lt;a href=&#34;/guides/ci-cd/tekton-gs-p1/&#34;&gt;Getting Started with Tekton&lt;/a&gt; walks you through how to install Tekton and create your first Task. In &lt;a href=&#34;/guides/ci-cd/tekton-gs-p2/&#34;&gt;part 2&lt;/a&gt; of the guide, we cover how to build a pipeline that builds a container image for your code using &lt;a href=&#34;https://github.com/GoogleContainerTools/kaniko&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Kaniko&lt;/a&gt;. Meanwhile, the &lt;a href=&#34;https://tekton.dev/docs/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Tekton Docs&lt;/a&gt; do a great job of explaining the concepts and components, and even include some interactive tutorials right in the browser!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: Getting Started with ArgoCD on Kubernetes</title>
      
      <link>/guides/ci-cd/argocd-gs/</link>
      <pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/guides/ci-cd/argocd-gs/</guid>
      <description>

        
        &lt;p&gt;ArgoCD is a declarative GitOps tool built to deploy applications to Kubernetes. While the continuous delivery (CD) space is seen by some as crowded these days, ArgoCD does bring some interesting capabilities to the table.&lt;/p&gt;
&lt;p&gt;Unlike other tools, ArgoCD is lightweight and easy to configure. It is purpose-built to deploy applications to Kubernetes so it doesn‚Äôt have the UI overhead of many tools built to deploy to multiple locations.&lt;/p&gt;
&lt;p&gt;It was also built with a &lt;a href=&#34;https://www.weave.works/technologies/gitops/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;GitOps&lt;/a&gt; flow in mind. That means everything ArgoCD sees as its source of truth is stored in a repository, which makes permissions and access control easier to handle since no one can change files locally to impact the behavior of ArgoCD. It also increases security by not storing any of these configuration files locally.&lt;/p&gt;
&lt;p&gt;And it‚Äôs fast! After using it I am sure you will agree that ArgoCD is very performant. It feels like a native, local application even though it&amp;rsquo;s running distributed microservices on Kubernetes.&lt;/p&gt;
&lt;p&gt;In this guide, you will install ArgoCD onto Kubernetes. Then you will configure ArgoCD to pull Kubernetes configuration files from GitHub, and a Docker image to run from Docker Hub. Then you will ‚Äúsync‚Äù that image to another Kubernetes cluster.&lt;/p&gt;
&lt;p&gt;Ready to get started? Here you go!&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;Before you get started, you will need to do a number of things.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Install &lt;a href=&#34;https://minikube.sigs.k8s.io/docs/start/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Minikube&lt;/a&gt;&lt;/strong&gt;: You will use Minikube to build the Kubernetes clusters referenced in this guide. Other options for running local Kubernetes clusters may also work but have not been tested in this guide.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Install &lt;a href=&#34;https://kubernetes.io/docs/tasks/tools/install-kubectl/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;kubectl&lt;/a&gt;&lt;/strong&gt;: If you have worked with Kubernetes before, you likely already have kubectl installed. If not, you will need it to manage your clusters, as well as give ArgoCD a way to connect to them through the kubeconfig file.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Install &lt;a href=&#34;https://docs.docker.com/desktop/#download-and-install&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Docker&lt;/a&gt; and log in (optional)&lt;/strong&gt;: If you choose, there are some optional steps in this guide for building your own demo application. To perform these steps, you will use Docker to build and push your container to Docker Hub.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Have a &lt;a href=&#34;http://dockerhub.com/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;DockerHub&lt;/a&gt; account (optional)&lt;/strong&gt;: As discussed above, if you choose to go through the optional steps, you will need a Docker Hub account to push your container to.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Set aside 10-15 minutes&lt;/strong&gt;: About how long this guide will take to complete unless you do the optional steps.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;create-the-kubernetes-clusters&#34;&gt;Create the Kubernetes clusters&lt;/h2&gt;
&lt;p&gt;Once you have these config files, the next step is to create two Kubernetes clusters: On one, you will install ArgoCD; the other is where you will push your application and run it.&lt;/p&gt;
&lt;p&gt;You will use &lt;a href=&#34;https://minikube.sigs.k8s.io/docs/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Minikube&lt;/a&gt; to create these clusters. Other options may work, but Minikube is an easy tool for creating multiple clusters without too much troubleshooting when it comes to managing ingress and external connections.&lt;/p&gt;
&lt;p&gt;First, start the target Kubernetes cluster.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;minikube start -p target-k8s
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Then, start the cluster on which you will install ArgoCD.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;minikube start -p argocd-k8s
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In the next section, you will be working on the &lt;code&gt;argocd-k8s&lt;/code&gt; cluster to complete the install of ArgoCD. Because of the order you created these clusters in, your &lt;code&gt;kubectl context&lt;/code&gt; should already be pointed at this cluster, but it‚Äôs good to be sure. The following will confirm that your context is set correctly.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl config use-context argocd-k8s
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;optional-download-the-git-repo&#34;&gt;Optional: Download the git repo&lt;/h2&gt;
&lt;p&gt;As the ‚Äú&lt;a href=&#34;https://argoproj.github.io/argo-cd/getting_started/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Getting Started&lt;/a&gt;‚Äù documentation from ArgoCD will show, the install process is very straightforward (much of this guide uses commands found there). Even so, you will still need some configuration files to get your pipeline to start delivering applications to production.&lt;/p&gt;
&lt;p&gt;Rather than creating everything from scratch, the environment has been created for you. Look through these files and understand their function. If you have some experience with Kubernetes configuration files, they will be fairly straightforward.&lt;/p&gt;
&lt;p&gt;To make this viewing easier, you may decide to clone the repo locally. You will not be changing these files in this guide, as ArgoCD pulls them directly from the repository.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/anthonyvetter/argocd-getting-started.git &amp;amp;&amp;amp; cd argocd-getting-started
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You will have the option of creating these environments using your own GitHub and Docker Hub images later in this post, but it‚Äôs not required to get started.&lt;/p&gt;
&lt;h2 id=&#34;install-argocd&#34;&gt;Install ArgoCD&lt;/h2&gt;
&lt;p&gt;Now that you have created your Kubernetes clusters, you can start the install process. It‚Äôs really just two commands. The first is to create a namespace where you will install ArgoCD.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create namespace argocd
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The second‚Äîand mostly final step‚Äîis to apply this script from the ArgoCD team, which will take care of the rest.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This command will complete quickly, but pods will still be spinning up on the back end. These need to be in a running state before you can move forward. Use the watch command to ensure the pods are running and ready.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;watch kubectl get pods -n argocd
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;Troubleshooting&lt;/strong&gt;: I and several others have run into an issue at this step, where pods will enter a &lt;code&gt;CrashLoopBackOff&lt;/code&gt; or &lt;code&gt;ImgPullError&lt;/code&gt; state. These install steps are exactly the same as on the ArgoCD ‚ÄúGetting Started‚Äù guide. If it happens to you, know that I had good luck simply using Minikube to stop and start the Kubernetes cluster again and retrying.&lt;/p&gt;
&lt;p&gt;Once the pods are ready, ArgoCD will be running. But it will not be accessible from outside the cluster. Since this is a demo environment, use port-forward to expose a port to the service, and forward it to &lt;code&gt;localhost&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl port-forward svc/argocd-server -n argocd 8080:443
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once run, your ArgoCD cluster will be available at &lt;a href=&#34;https://localhost:8080&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;https://localhost:8080&lt;/a&gt;. Since you didn‚Äôt deploy any certificates, you will need to accept your browser‚Äôs certificate exception. The &lt;code&gt;port-forward&lt;/code&gt; command will also now be running in the foreground of your terminal. Open another terminal window or tab and cd back into the working directory.&lt;/p&gt;
&lt;p&gt;In the UI, you will not be able to log in yet. ArgoCD uses the unique name of its server pod as a default password, so every installation will be different. Pretty clever! The following command will list the pods and format the output to provide just the line you want. It will have the format &lt;code&gt;argocd-server-&amp;lt;number&amp;gt;-&amp;lt;number&amp;gt;&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl get pods -n argocd -l app.kubernetes.io/name=argocd-server -o name | cut -d&#39;/&#39; -f 2
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To log in to the ArgoCD UI, the default username is &lt;code&gt;admin&lt;/code&gt; and the default password is the output of the above command. Save this password; you will need it for the next step of installing and configuring the ArgoCD command-line agent.&lt;/p&gt;
&lt;p&gt;At this stage, take some time to familiarize yourself with the ArgoCD UI. While the rest of the steps in this guide can be done either through the UI or the CLI, you will be using the CLI.&lt;/p&gt;
&lt;h2 id=&#34;install-and-set-up-the-argocd-cli&#34;&gt;Install and set up the ArgoCD CLI&lt;/h2&gt;
&lt;p&gt;There are two primary methods for installing the ArgoCD CLI tool. If you are on a Mac computer running brew, there is a tap for argocd. Otherwise, you will need to install the binary from &lt;a href=&#34;https://github.com/argoproj/argo-cd/releases/latest&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;To install with brew&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;brew tap argoproj/tap &amp;amp;&amp;amp; brew install argoproj/tap/argocd
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;To install the binary&lt;/strong&gt;: First, you need to confirm the version of ArgoCD you are running. The script does install the latest stable version, but it‚Äôs good to ensure version compatibility.
Log in to the ArgoCD UI. In the upper left-hand corner, you will see the ArgoCD squid, and underneath that, you will see the major version listed. Hovering your cursor over it will show the full version string.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/argocd/screenshots/version.png&#34; alt=&#34;ArgoCD Version&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Then go to the &lt;a href=&#34;https://github.com/argoproj/argo-cd/releases&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;releases page&lt;/a&gt; on the ArgoCD GitHub site and find the version associated with your ArgoCD version. Under the Assets section, download the argocd version appropriate for your platform.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/argocd/screenshots/releases.png&#34; alt=&#34;ArgoCD Releases&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Once downloaded, rename the file and move it into your &lt;code&gt;$PATH&lt;/code&gt;. Modify this command for your platform.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mv argocd-darwin-amd64 argocd &amp;amp;&amp;amp; mv argocd /usr/local/bin
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now that the &lt;code&gt;argocd&lt;/code&gt; client is installed, you can log it into your ArgoCD installation. Use the password from the previous section.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;argocd login localhost:8080
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Again, as with the UI, you will need to accept the server certificate error.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Optional step&lt;/strong&gt;: You can change your password at this stage if you like. This command will prompt you for your current password, your new password, then to confirm that new password.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;argocd account update-password
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You are now almost ready to deploy your application. But first you need to tell ArgoCD about your deployment target. By default, if you do not add an additional Kubernetes cluster target, ArgoCD will deploy applications to the cluster on which it is installed. To add your target Kubernetes cluster to ArgoCD, use the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;argocd cluster add target-k8s
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This will add an ArgoCD service account onto the cluster, which will allow ArgoCD to deploy applications to it.&lt;/p&gt;
&lt;h2 id=&#34;optional-the-demo-application&#34;&gt;Optional: The demo application&lt;/h2&gt;
&lt;p&gt;For an application to deploy, you are going to be using &lt;a href=&#34;https://github.com/spring-projects/spring-petclinic&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;spring-petclinic&lt;/a&gt;. It‚Äôs an application that is pretty easy to understand, and packaged into a container to run on Kubernetes. It&amp;rsquo;s been packaged already on a public &lt;a href=&#34;https://hub.docker.com/repository/docker/anthonyvetter/spring-petclinic&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;DockerHub&lt;/a&gt; repo. No steps are required in this guide to package your own. You will configure ArgoCD to pull this image.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Optional steps&lt;/strong&gt;: You may, at some point, want to create your own container to pull from your own registry. It is recommended that you run through this guide once as-is so you understand how things work in ArgoCD. But if you are ready to package your own application, these are the steps.&lt;/p&gt;
&lt;p&gt;Within the optional directory of the GitHub repo you cloned, there is a Dockerfile for building spring-petclinic. To build it, run the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker build optional/. -t spring-petclinic
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This Dockerfile uses &lt;a href=&#34;https://maven.apache.org/what-is-maven.html&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Maven&lt;/a&gt; to package the application in an OCI container image; it will take some time to run. In the end, you will have a container on your local system called spring-petclinic.&lt;/p&gt;
&lt;p&gt;To push this container to DockerHub, first tag your container. Modify this command and the next to add your DockerHub username.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker tag spring-petclinic &amp;lt;your-dh-username&amp;gt;/spring-petclinic:latest
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Then push your container to Docker Hub.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker push &amp;lt;your-dh-username&amp;gt;/spring-petclinic
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The configuration files you may have cloned earlier are still written to call from my DockerHub account. And if you have run through this guide once already, you know that ArgoCD pulls those files directly from GitHub (modifying them locally will have no effect on this behavior).&lt;/p&gt;
&lt;p&gt;There are many methods to create your own configuration files for ArgoCD, but the easiest is probably to &lt;a href=&#34;https://help.github.com/en/github/getting-started-with-github/fork-a-repo&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;fork&lt;/a&gt; my repository into your own, then modify the &lt;code&gt;deployment.yml&lt;/code&gt; to pull the image from your DockerHub account. You will then need to modify the later &lt;code&gt;argo app create&lt;/code&gt; flags in the next section to use your GitHub repo.&lt;/p&gt;
&lt;h2 id=&#34;add-your-app-to-argocd&#34;&gt;Add your app to ArgoCD&lt;/h2&gt;
&lt;p&gt;You are now ready to add your application to ArgoCD to monitor and push to your target. But first you need to set up a couple of environment variables so as to make some of the following commands a little easier.&lt;/p&gt;
&lt;p&gt;This variable will tell the argocd CLI client where our ArgoCD installation resides. It gets the cluster information from your kubectl context, but it needs the namespace. Without setting this variable, you will need to add the &lt;code&gt;--port-forward-namespace&lt;/code&gt; flag to all commands run with argocd.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;export ARGOCD_OPTS=&#39;--port-forward-namespace argocd&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This variable you will use to tell ArgoCD where your target cluster API URL is.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;export MINIKUBE_IP=https://$(minikube ip -p target-k8s):8443
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;With these variables set, use argocd to create the application record. Here you are telling ArgoCD to pull in configuration files from my GitHub repository, and that the files are in the root directory. Then you are telling it to deploy that application onto your target cluster, in the default namespace.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;argocd app create spring-petclinic --repo https://github.com/anthonyvetter/argocd-getting-started.git --path . --dest-server $MINIKUBE_IP --dest-namespace default
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once this completes, you can see the status and configuration of your app by running the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;argocd app list
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Notice the &lt;code&gt;STATUS: OutOfSync&lt;/code&gt; and &lt;code&gt;HEALTH: Missing&lt;/code&gt;. That‚Äôs because ArgoCD creates applications with manual triggers by default. Automated triggers are available and are &lt;a href=&#34;https://argoproj.github.io/argo-cd/user-guide/auto_sync/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;straightforward to configure&lt;/a&gt;, but in this guide , you will stick with manual triggers in order to go through the process slowly.&lt;/p&gt;
&lt;p&gt;‚ÄúSync‚Äù is the terminology ArgoCD uses to describe the application on your target cluster as being up to date with the sources ArgoCD is pulling from. You have set up ArgoCD to monitor the GitHub repository with the configuration files as well as the spring-petclinic container image in Docker Hub. Once the initial sync is completed, a change to either of these sources will cause the status in ArgoCD to change to &lt;code&gt;OutOfSync&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For a more detailed view of your application configuration, run:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;argocd app get spring-petclinic
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now you are ready to sync your application to your target cluster. To do this, simply use the sync command for your application:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;argocd app sync spring-petclinic
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once completed, your target Kubernetes cluster will be creating the pod on which spring-petclinic will be running. But as with ArgoCD, the UI will not be available outside the cluster. To forward a port, you first need to change kubectl contexts to your target cluster.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl config use-context target-k8s
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Since the argocd CLI client uses your kubectl context to connect to your cluster, any argocd commands you run from this point won‚Äôt work. You will need to change contexts back to your argocd-k8s cluster to manage ArgoCD.&lt;/p&gt;
&lt;p&gt;Now simply forward the port as you did for the ArgoCD UI. Once completed, spring-petclinic will be available at &lt;a href=&#34;http://localhost:9090&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;http://localhost:9090&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl port-forward svc/spring-petclinic -n default 9090:8080
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And there you have it! You have a running application deployed to Kubernetes with ArgoCD. For further learning, try setting up your own environment using the optional steps provided throughout this guide. Find out how to set up &lt;a href=&#34;https://argoproj.github.io/argo-cd/user-guide/auto_sync/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;automated triggers&lt;/a&gt;, and maybe configure ArgoCD with your own custom Kubernetes application. Finally, look at adding ArgoCD into your CI/CD pipeline and deploying applications into external Kubernetes environments.&lt;/p&gt;
&lt;p&gt;For further learning, the &lt;a href=&#34;https://argoproj.github.io/argo-cd/operator-manual/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Operator Manual&lt;/a&gt; from ArgoCD is a terrific resource. If you want to look at developing a third-party integration for ArgoCD, see the Developer &lt;a href=&#34;https://argoproj.github.io/argo-cd/developer-guide/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Guides&lt;/a&gt;.&lt;/p&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/css/faq.css&#34;&gt;
&lt;div class=&#34;faqs&#34; id=&#34;faqs&#34;&gt;
    &lt;div class=&#34;flex-container jc-between&#34;&gt;&lt;/div&gt;
        &lt;h2 class=&#34;h2 mb-md&#34;&gt;Frequently Asked Questions&lt;/h2&gt;
        &lt;div class=&#34;faq&#34;&gt;
            
  &lt;div class=&#34;faq-item&#34; id=&#34;faq-item&#34;&gt;
    &lt;div class=&#34;flex jc-between ai-center&#34;&gt;
        &lt;h4 class=&#34;faq-question&#34;&gt;How do you install ArgoCD on Kubernetes?&lt;/h4&gt;
        &lt;i class=&#34;fa fa-angle-down&#34; id=&#34;arrow&#34;&gt;&lt;/i&gt;
    &lt;/div&gt;
    &lt;div class=&#34;faq-answer&#34;&gt;
        &lt;div&gt;After creating a Kubernetes cluster, ArgoCD can be installed with two simple commands. First, create a namespace to install the ArgoCD and run the command &lt;code&gt;kubectl create namespace argocd&lt;/code&gt;. Finally, apply the script &lt;code&gt;kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml&lt;/code&gt; to finish the install.&lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;
  &lt;div class=&#34;faq-item&#34; id=&#34;faq-item&#34;&gt;
    &lt;div class=&#34;flex jc-between ai-center&#34;&gt;
        &lt;h4 class=&#34;faq-question&#34;&gt;What are the benefits of using ArgoCD on Kubernetes?&lt;/h4&gt;
        &lt;i class=&#34;fa fa-angle-down&#34; id=&#34;arrow&#34;&gt;&lt;/i&gt;
    &lt;/div&gt;
    &lt;div class=&#34;faq-answer&#34;&gt;
        &lt;div&gt;Because ArgoCD can apply git repository configurations to Kubernetes, it assists in the lifecycle management and accelerated deployment of &lt;a href=&#34;https://tanzu.vmware.com/cloud-native&#34;&gt;cloud-native applications&lt;/a&gt;.&lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;
  &lt;div class=&#34;faq-item&#34; id=&#34;faq-item&#34;&gt;
    &lt;div class=&#34;flex jc-between ai-center&#34;&gt;
        &lt;h4 class=&#34;faq-question&#34;&gt;How do you install ArgoCD CLI?&lt;/h4&gt;
        &lt;i class=&#34;fa fa-angle-down&#34; id=&#34;arrow&#34;&gt;&lt;/i&gt;
    &lt;/div&gt;
    &lt;div class=&#34;faq-answer&#34;&gt;
        &lt;div&gt;On Mac, the ArgoCD CLI can be installed with &lt;code&gt;brew&lt;/code&gt;, where there is a tap for ArgoCD. Otherwise, the binary will need to be installed by navigating to ArgoCD releases page, installing the correct version appropriate for your platform, renaming the file, modifying the command, logging in, and deploying your application.&lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;
  &lt;div class=&#34;faq-item&#34; id=&#34;faq-item&#34;&gt;
    &lt;div class=&#34;flex jc-between ai-center&#34;&gt;
        &lt;h4 class=&#34;faq-question&#34;&gt;How do you deploy apps to ArgoCD in Kubernetes?&lt;/h4&gt;
        &lt;i class=&#34;fa fa-angle-down&#34; id=&#34;arrow&#34;&gt;&lt;/i&gt;
    &lt;/div&gt;
    &lt;div class=&#34;faq-answer&#34;&gt;
        &lt;div&gt;After Installation of the ArgoCD CLI, to deploy your applications with ArgoCD, first tell ArgoCD about your deployment target Kubernetes cluster using the command &lt;code&gt;argocd cluster add target-k8s&lt;/code&gt;, then configure ArgoCD to pull the image using &lt;a href=&#34;https://github.com/spring-projects/spring-petclinic&#34;&gt;spring-petclinic&lt;/a&gt;. Finally, push your container to DockerHub and create your own configuration files, or &lt;a href=&#34;https://docs.github.com/en/get-started/quickstart/fork-a-repo&#34;&gt;fork our repository&lt;/a&gt; into your own.&lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;
  &lt;div class=&#34;faq-item&#34; id=&#34;faq-item&#34;&gt;
    &lt;div class=&#34;flex jc-between ai-center&#34;&gt;
        &lt;h4 class=&#34;faq-question&#34;&gt;How do you add a Kubernetes cluster to ArgoCD?&lt;/h4&gt;
        &lt;i class=&#34;fa fa-angle-down&#34; id=&#34;arrow&#34;&gt;&lt;/i&gt;
    &lt;/div&gt;
    &lt;div class=&#34;faq-answer&#34;&gt;
        &lt;div&gt;Kubernetes clusters can be added to ArgoCD by installing the proper configuration files, installing ArgoCD on a Kubernetes cluster, then starting both the target cluster and the cluster in which you installed ArgoCD.&lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;
  &lt;div class=&#34;faq-item&#34; id=&#34;faq-item&#34;&gt;
    &lt;div class=&#34;flex jc-between ai-center&#34;&gt;
        &lt;h4 class=&#34;faq-question&#34;&gt;What is ArgoCD sync?&lt;/h4&gt;
        &lt;i class=&#34;fa fa-angle-down&#34; id=&#34;arrow&#34;&gt;&lt;/i&gt;
    &lt;/div&gt;
    &lt;div class=&#34;faq-answer&#34;&gt;
        &lt;div&gt;‚ÄúSync‚Äù is the terminology ArgoCD uses to describe the application on your target cluster as being up to date with the sources ArgoCD is pulling from.&lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;

        &lt;/div&gt;
    &lt;/div&gt;
    
&lt;/div&gt;

&lt;script type=&#34;text/javascript&#34;&gt;
    $(&#34;.faq-item&#34;).each( function() {
        $(this).click(function () {
            $(this).find(&#34;#arrow&#34;).toggleClass(&#34;flip&#34;); 
            $(this).find(&#34;.faq-answer&#34;).slideToggle(200); 
        });
    });
&lt;/script&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: Getting Started with Concourse CI</title>
      
      <link>/guides/ci-cd/concourse-gs/</link>
      <pubDate>Mon, 04 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/guides/ci-cd/concourse-gs/</guid>
      <description>

        
        &lt;p&gt;Writing code is one thing. Testing and deploying that code into production is another. Many tools exist to automate the workflow, from code commit to production release. Continuous Integration (CI), Continuous Deployment (CD), Continuous Delivery (CD again), artifact registries, code security scanners, and various other tools are used to achieve this goal. But it all starts with code integration.&lt;/p&gt;
&lt;p&gt;How can you make sure your code is ready to be integrated into a release? Continuous Integration (CI) is not a new concept for most developers, and‚Äîonce the system is implemented, it is rarely thought about deeply again. Even when it‚Äôs agreed that the current implementation is non-optimal, the CI system runs in the background, churning away. For the most part, it ‚Äújust works.‚Äù&lt;/p&gt;
&lt;p&gt;But what if there was a better system? One that was built for cloud native development paradigms on the foundation of a stateless architecture where all pipelines are built and treated as code? That‚Äôs where Concourse CI comes in.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/concourse/screenshots/overview.png&#34; alt=&#34;Concourse dashboard&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;An example Concourse CI dashboard showing the status of many pipelines&lt;/p&gt;
&lt;h2 id=&#34;concourse-ci&#34;&gt;Concourse CI&lt;/h2&gt;
&lt;p&gt;Concourse CI is a system built with a loosely coupled microservices architecture; a &lt;a href=&#34;https://concourse-ci.org/postgresql-node.html&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;database node&lt;/a&gt; using PostgreSQL is the only state saved by the system. All build histories, stored pipelines, as well as user and group access privileges are stored here.&lt;/p&gt;
&lt;p&gt;A &lt;a href=&#34;https://concourse-ci.org/concourse-web.html&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;web node&lt;/a&gt; service provides the user interface to Concourse CI. Here, developers and administrators can get a quick view of their pipelines, including their status. Broken pipelines can be easily identified so users can fix any issues.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://concourse-ci.org/concourse-worker.html&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Worker nodes&lt;/a&gt; run each of the tasks defined in a Concourse CI pipeline. They download container images, clone git repositories, and run tests as defined. And when they are done, the testing containers are entirely ephemeral, so you get a clean test every time.&lt;/p&gt;
&lt;p&gt;Concourse CI pipelines are built using three different abstraction paradigms: tasks, jobs, and resources.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://concourse-ci.org/tasks.html&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Tasks&lt;/a&gt; are the smallest unit of work Concourse CI does. They can be called to run a script or even just a single command within the testing container. Meanwhile, task output is provided as detailed log files, which can be parsed programmatically if needed.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://concourse-ci.org/jobs.html&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Jobs&lt;/a&gt; are a package of tasks. By bundling up a group of tasks as a job, Concourse CI users can make their pipeline code reusable for other systems. Such bundling provides a higher-level abstraction as pipelines get larger and more complex, which makes it easier for new team members to get up to speed.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://concourse-ci.org/resources.html&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Resources&lt;/a&gt; are what jobs perform actions on. A typical example would be a git repository; once configured, Concourse CI can pull in new code to test, run test scripts stored in git, or even push its own changes. And since everything in Concourse CI is configured as code, even resource configurations can be managed in git and reused across an organization.&lt;/p&gt;
&lt;p&gt;Intrigued? What follows here is a guide to get started with Concourse CI. You will deploy a small Concourse CI cluster locally on top of Kubernetes. Then you will push a new pipeline to your cluster, which will run a series of tests on a sample application, which you will clone.&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;Before you get started, you will need to do a number of things.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Install &lt;a href=&#34;https://www.docker.com/products/docker-desktop&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Docker Desktop&lt;/a&gt; and &lt;a href=&#34;https://docs.docker.com/docker-for-mac/#kubernetes&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;enable Kubernetes&lt;/a&gt;&lt;/strong&gt;: Other methods of deploying a local Kubernetes cluster like &lt;a href=&#34;https://kind.sigs.k8s.io/docs/user/quick-start/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;KIND&lt;/a&gt; or &lt;a href=&#34;https://kubernetes.io/docs/tasks/tools/install-minikube/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Minikube&lt;/a&gt; may also work. Cloud-based or other production Kubernetes deployments should work, too. This guide was written using Docker Desktop; other methods will require modification of commands and have not been fully tested for use in this guide.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Install &lt;a href=&#34;https://helm.sh/docs/intro/install/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Helm 3&lt;/a&gt;&lt;/strong&gt;: Helm 3 will be used to install Concourse CI.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Install &lt;a href=&#34;https://kubernetes.io/docs/tasks/tools/install-kubectl/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;kubectl&lt;/a&gt;&lt;/strong&gt;: This is the local client application you will use for interacting with your Kubernetes cluster. It is also how Helm will reach and interact with your cluster.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Secure a Slack instance&lt;/strong&gt;: One via which you have access to create webhooks and can post messages to a channel.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Set aside 15-20 minutes&lt;/strong&gt;: Roughly the time it will take to run through this guide.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;setting-up&#34;&gt;Setting up&lt;/h2&gt;
&lt;p&gt;To make the installation and configuration of Concourse CI and its pipelines a little easier, a GitHub repo is provided here along with some helpful files. Download it, then &lt;code&gt;cd&lt;/code&gt; into that directory. In this guide, commands will assume this as your working directory unless otherwise noted.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/anthonyvetter/concourse-getting-started.git &amp;amp;&amp;amp; cd concourse-getting-started
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In this repository are three directories: &lt;code&gt;install&lt;/code&gt;, &lt;code&gt;pipelines&lt;/code&gt;, and &lt;code&gt;test-scripts&lt;/code&gt;. You will explore the &lt;code&gt;pipelines&lt;/code&gt; and &lt;code&gt;test-scripts&lt;/code&gt; directories later on in this guide. They contain a helpful starter pipeline you will push to our Concourse CI cluster, as well as some unit test scripts. The &lt;code&gt;install&lt;/code&gt; directory contains an abbreviated version of the Concourse CI &lt;code&gt;values.yml&lt;/code&gt; file, and a (very) small BASH script for exposing access to Concourse CI locally. Let‚Äôs get into that next.&lt;/p&gt;
&lt;p&gt;Next, define a variable for your username of your GitHub account.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;export GH_USERNAME=your-github-username
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;installation&#34;&gt;Installation&lt;/h3&gt;
&lt;p&gt;Let‚Äôs get started by installing Concourse CI onto Kubernetes. This installation will be abbreviated; it‚Äôs intended for demonstration and learning purposes only. Full installation instructions using Helm can be found &lt;a href=&#34;https://github.com/concourse/concourse-chart&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;via the Concourse CI team&lt;/a&gt; from which the installation instructions in this guide borrow heavily.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Optional step&lt;/strong&gt;: Concourse CI installation in a local context is fairly straightforward. There are some default attributes contained in the &lt;code&gt;install/values.yml&lt;/code&gt; file; leaving these as their default values will get you a working installation. That said, there are a few values that you may choose to modify in order to slightly customize your installation and experiment with Concourse CI. Modify this file to suit your needs. Comments on each line will explain their respective functions.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vim install/values.yml # replace vim with your text editor of choice
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Helm 3 does not come with the chart repository for Concourse CI by default (it doesn‚Äôt include any repositories by default). So the next step is to add that to Helm and update Helm‚Äôs caches.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;helm repo add concourse https://concourse-charts.storage.googleapis.com/ &amp;amp;&amp;amp; helm repo update
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;These are pretty much the only required steps prior to install. But before you run the install command, be sure to check kubectl to ensure you are targeting the correct Kubernetes cluster. If you are using Docker Desktop, this should be your output.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;‚ùØ kubectl config get-contexts

CURRENT   NAME                 CLUSTER          AUTHINFO         NAMESPACE
*         docker-desktop       docker-desktop   docker-desktop
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Further instructions for managing contexts and clusters using kubectl can be found &lt;a href=&#34;https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Now you can install Concourse CI. This command will use Helm to install the cluster into the default Kubernetes namespace using our &lt;code&gt;values.yml&lt;/code&gt; file. If you left this file as default, then the &lt;code&gt;-f&lt;/code&gt; flag can be left out.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;helm install concourse concourse/concourse -f install/values.yml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This command will complete quickly, but there will still be pods spinning up on the back end. To see the status of the system as it is being deployed, use &lt;code&gt;watch&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;watch kubectl get pods
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This will show a list of pods in the default namespace and their status. Wait until they are all in a running status.&lt;/p&gt;
&lt;p&gt;Once all the pods are ready, Concourse CI will be up and running. But it won‚Äôt be accessible outside the cluster. To expose it, run the provided expose script.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;./install/expose-concourse.sh
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Running this script opens up a node port in your Kubernetes cluster and forwards it to your localhost. Assuming you left these values as default, your Concourse CI cluster should now be available at &lt;a href=&#34;http://localhost:8080&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;http://localhost:8080&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: This port-forward task is running in the foreground in your terminal. To keep UI access available, open a new terminal window or tab and cd back into your working directory.&lt;/p&gt;
&lt;p&gt;You can access the cluster by logging in using the credentials set in the &lt;code&gt;values.yml&lt;/code&gt; file. If you left them as default the username and the password are both &lt;code&gt;test&lt;/code&gt;. At this point, there are no pipelines set; you need to install the fly client application first.&lt;/p&gt;
&lt;h3 id=&#34;installing-fly&#34;&gt;Installing fly&lt;/h3&gt;
&lt;p&gt;Fly is the local client application developers and Concourse CI administrators use as their primary way to interact with the cluster from the command line. To install fly, download the binary from your Concourse CI cluster directly by clicking the link appropriate for your system, as shown here.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/concourse/screenshots/download-fly.png&#34; alt=&#34;Download Fly&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;The Concourse CI interface upon a new installation showing download links for the fly client application&lt;/p&gt;
&lt;p&gt;Once completed, make the binary executable, then move it into your &lt;code&gt;$PATH&lt;/code&gt;. That‚Äôs it!&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo chmod +x ~/Downloads/fly &amp;amp;&amp;amp; mv ~/Downloads/fly /usr/local/bin
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: These commands are for use on a Macintosh computer. They will need to be modified for other platforms.&lt;/p&gt;
&lt;p&gt;Now you need to let fly know about your Concourse CI cluster. Do that with the &lt;code&gt;target&lt;/code&gt; command for your fly client. Notice we are giving our Concourse CI a name of &lt;code&gt;demo&lt;/code&gt;. It can be any name you want, just keep it short. Every command run using fly must include the &lt;code&gt;--target&lt;/code&gt; flag to explicitly run commands on a specific cluster, so unless you &lt;code&gt;alias&lt;/code&gt; it, you will be typing it a lot.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;fly --target demo login --concourse-url http://127.0.0.1:8080 -u test -p test
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Then, to ensure compatible versions of Concourse CI and fly are running, use the &lt;code&gt;sync&lt;/code&gt; command.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;fly -t demo sync
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Notice you can abbreviate &lt;code&gt;--target&lt;/code&gt; with &lt;code&gt;-t&lt;/code&gt;, making commands shorter to type. There are many abbreviations in fly like this. You will use these abbreviations throughout this guide.&lt;/p&gt;
&lt;h2 id=&#34;creating-the-demo-application&#34;&gt;Creating the demo application&lt;/h2&gt;
&lt;p&gt;For this guide, you will use &lt;a href=&#34;https://github.com/spring-projects/spring-petclinic&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;spring-petclinic&lt;/a&gt; as an application to test against in your pipeline, which is a small Java application written in Spring. But there are no specific dependencies on this application other than the test scripts running Maven testing jobs.&lt;/p&gt;
&lt;p&gt;To follow along, &lt;a href=&#34;https://help.github.com/en/github/getting-started-with-github/fork-a-repo&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;fork&lt;/a&gt; this application (&lt;a href=&#34;https://github.com/spring-projects/spring-petclinic&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;linked here&lt;/a&gt;) into your own GitHub repository or provide your own application and modify the test scripts as needed (they are very rudimentary).&lt;/p&gt;
&lt;p&gt;Next, clone the repository locally and place it anywhere on your system. Then &lt;code&gt;cd&lt;/code&gt; into the directory.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/$GH_USERNAME/spring-petclinic.git &amp;amp;&amp;amp; cd spring-petclinic
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Next, you need to create a test branch. You can mimic an example GitOps flow where a new feature or bug fix is pulled into this branch for testing; the automated system takes it from there.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git checkout -b test
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This will create a test branch for your project, which will be monitored by Concourse CI. Next, push this branch to your repository so Concourse CI can monitor it.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git push origin test
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The result is that once you deploy the pipeline and push a change to this branch, Concourse CI will pick it up and run its configured automated tests.&lt;/p&gt;
&lt;h2 id=&#34;deploying-a-pipeline&#34;&gt;Deploying a pipeline&lt;/h2&gt;
&lt;p&gt;Now that your Concourse CI installation is up and running, it‚Äôs time to create your first pipeline. Go back to the concourse-getting-started folder for the configuration files. But before you set that pipeline to Concourse CI, take a look at each of the sections to understand what they are doing. Comments are provided within the pipeline YAML as well, to describe each section.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat pipelines/pipeline.yml | less
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This pipeline is going to pull in your demo application from your repository. Then it will pull in test scripts from a separate repository. And finally, it will run those tests, one at a time, in a dedicated container, all the while reporting out status via Slack. Review the comments in the file to understand the function of each section.&lt;/p&gt;
&lt;p&gt;You will notice there are a few &lt;code&gt;((variables))&lt;/code&gt; contained within the pipeline; you will define those next. The &lt;code&gt;credentials.yml&lt;/code&gt; file contains those variable assignments. Open the file and fill in the variables for your environment. Again, the comments in the file will help you understand the function of each line.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vim pipelines/credentials.yml # replace vim with your favorite text editor
&lt;/code&gt;&lt;/pre&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: This will include a step for setting up a Slack webhook integration. A link to the instructions from Slack to set it up is provided in the file, or you can view it &lt;a href=&#34;https://slack.com/help/articles/115005265063-Incoming-Webhooks-for-Slack&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Another note&lt;/strong&gt;: Using credentials files in this way provides an easy way to make changes to a pipeline. For example, by modifying just this one file in a straightforward way, the pipeline can be used flexibly across many environments, with many applications. However, in a production environment you would want to configure Concourse CI to use a &lt;a href=&#34;https://concourse-ci.org/creds.html&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;credential management system&lt;/a&gt; like &lt;a href=&#34;https://learn.hashicorp.com/vault/getting-started/install&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Vault&lt;/a&gt;, &lt;a href=&#34;https://docs.cloudfoundry.org/credhub/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;CredHub&lt;/a&gt;, or something similar.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That‚Äôs because using a credentials file provides just a simple translation done at the time when the pipeline is set. Which is not a big deal when it&amp;rsquo;s just URLs, but when these files contain access tokens, private SSH keys, passwords, and the like, you will want a more secure system.&lt;/p&gt;
&lt;p&gt;Now you will set the pipeline onto your Concourse CI deployment by using fly and the &lt;code&gt;set-pipeline&lt;/code&gt; command, which can be abbreviated &lt;code&gt;sp&lt;/code&gt;. Here the &lt;code&gt;-c&lt;/code&gt; flag denotes which configuration file you will use for your pipeline. The &lt;code&gt;-p&lt;/code&gt; flag will be the name of your pipeline in Concourse CI and the &lt;code&gt;-l&lt;/code&gt; flag will tell Concourse CI where to load variables from. Once it is run, type &lt;code&gt;y&lt;/code&gt; to accept the configuration changes.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;fly -t demo set-pipeline -c pipelines/pipeline.yml -p petclinic-tests -l pipelines/credentials.yml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Pipelines start ‚Äúpaused,‚Äù meaning they won‚Äôt start running until you tell them to, which you can do from the UI or the fly CLI tool. But before you unpause, take a look at the UI to confirm your pipeline has been created successfully.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/concourse/screenshots/dashboard.png&#34; alt=&#34;Concourse Pipeline&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;The main Concourse CI UI showing your pipeline set&lt;/p&gt;
&lt;p&gt;When clicking into that pipeline, you will see the jobs and resources depicted as connected entities. The jobs are the big gray boxes; the resources used by the jobs are the smaller, black boxes.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/concourse/screenshots/pipeline.png&#34; alt=&#34;Download Fly&#34;  /&gt;&lt;/p&gt;
&lt;h3 id=&#34;the-pipeline&#34;&gt;The pipeline&lt;/h3&gt;
&lt;p&gt;Back at the CLI, unpause the pipeline using the &lt;code&gt;unpause-pipeline&lt;/code&gt; command, which can be abbreviated &lt;code&gt;up&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;fly -t demo unpause-pipeline -p petclinic-tests
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Watch the pipeline complete. Then click into the running job and watch it complete in more detail.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/concourse/screenshots/concourse-test.gif&#34; alt=&#34;Pipeline run&#34;  /&gt;&lt;/p&gt;
&lt;h3 id=&#34;the-pipeline-and-job-run-details&#34;&gt;The pipeline and job run details&lt;/h3&gt;
&lt;p&gt;That‚Äôs it! Your pipeline is now monitoring your test branch for changes. Whenever a change is made, this pipeline will be kicked off and the tests will be run. Feel free to keep experimenting with this pipeline configuration, and build it to suit your needs. Push more changes to the application and watch Concourse CI trigger a new test run.&lt;/p&gt;
&lt;p&gt;For more information on Concourse CI, check out the &lt;a href=&#34;https://concourse-ci.org/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;open source project&lt;/a&gt; site. For information about the commercially-supported version, check out the Concourse CI &lt;a href=&#34;https://tanzu.vmware.com/concourse&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;page&lt;/a&gt; on VMware.com.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: Continuously Develop and Monitor an Express Application on Kubernetes with Bitnami, Skaffold and Octant</title>
      
      <link>/guides/ci-cd/express-app-bitnami-skaffold-octant/</link>
      <pubDate>Thu, 30 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/guides/ci-cd/express-app-bitnami-skaffold-octant/</guid>
      <description>

        
        &lt;p&gt;As Kubernetes&#39; importance as a platform grows, developers are increasingly searching for ways to build and debug cloud-native applications on Kubernetes infrastructure from the get-go. Rather than first hacking on code in local (or virtualized) environments and then migrating it to cloud-native architecture, this new approach involves continuously developing, debugging and deploying containerized applications on a live Kubernetes cluster.&lt;/p&gt;
&lt;p&gt;If you&amp;rsquo;re a developer building cloud-native applications and targeting Kubernetes as your deployment architecture, Bitnami can help you with ready-to-use containers and Helm charts. Using these (instead of rolling your own) can save you time, help you follow best practices and allow you to focus on adding features to your application instead of dealing with Kubernetes-related complexity and configuration.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://bitnami.com/containers&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami&amp;rsquo;s containers&lt;/a&gt; for &lt;a href=&#34;https://github.com/bitnami/bitnami-docker-node&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Node.js&lt;/a&gt;, &lt;a href=&#34;https://github.com/bitnami/bitnami-docker-ruby&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Ruby&lt;/a&gt;, &lt;a href=&#34;https://github.com/bitnami/bitnami-docker-java&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Java&lt;/a&gt; and many others make it easy to containerize your applications in a consistent manner using the latest and most secure version of your programming language.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/bitnami/charts/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami&amp;rsquo;s Helm charts&lt;/a&gt; for runtime environments like &lt;a href=&#34;https://github.com/bitnami/charts/tree/master/bitnami/node&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Node.js&lt;/a&gt; and infrastructure components like &lt;a href=&#34;https://github.com/bitnami/charts/tree/master/bitnami/memcached&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Memcached&lt;/a&gt;, &lt;a href=&#34;https://github.com/bitnami/charts/tree/master/bitnami/mysql&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;MySQL&lt;/a&gt;, &lt;a href=&#34;https://github.com/bitnami/charts/tree/master/bitnami/elasticsearch&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Elasticsearch&lt;/a&gt;, &lt;a href=&#34;https://github.com/bitnami/charts/tree/master/bitnami/kafka&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Apache Kafka&lt;/a&gt; and many others let you deploy your applications on Kubernetes in a secure and reliable manner without worrying about packaging, dependencies and Kubernetes YAML files.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This guide gets you started with continuous development (CD) on Kubernetes. It uses &lt;a href=&#34;https://skaffold.dev/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Skaffold&lt;/a&gt;, in combination with Bitnami containers and Helm charts, to let you test and debug your application in a live Kubernetes environment with minimal setup effort. It also uses  &lt;a href=&#34;https://octant.dev/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Octant&lt;/a&gt; to help you monitor your cluster in real time and understand how your application behaves under different workloads.&lt;/p&gt;
&lt;h2 id=&#34;assumptions-and-prerequisites&#34;&gt;Assumptions and Prerequisites&lt;/h2&gt;
&lt;p&gt;This guide makes the following assumptions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You have a multi-node Kubernetes cluster running. &lt;a href=&#34;https://docs.bitnami.com/kubernetes/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Learn about deploying a Kubernetes cluster on different cloud platforms&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;You have the following tools installed and configured to work with your Kubernetes cluster:
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;kubectl&lt;/em&gt; CLI. &lt;a href=&#34;https://docs.bitnami.com/kubernetes/get-started-kubernetes#step-3-install-kubectl-command-line&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Learn how to install &lt;em&gt;kubectl&lt;/em&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Helm v2.x package manager. &lt;a href=&#34;https://v2.helm.sh/docs/using_helm/#installing-helm&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Learn about installing Helm v2.x&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;You have Docker installed. &lt;a href=&#34;https://docs.docker.com/install/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Learn about installing and configuring Docker for your platform&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;You have a Docker Hub account. If not, &lt;a href=&#34;https://hub.docker.com/signup&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;sign up for a free Docker Hub account&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;As of this writing, Skaffold only works with Helm v2.x.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;step-1-create-a-simple-express-application&#34;&gt;Step 1: Create a simple Express application&lt;/h2&gt;
&lt;p&gt;Your first step is to write some starter code. In this section, you will create a simple &amp;ldquo;Hello, world&amp;rdquo; application using &lt;a href=&#34;https://expressjs.com/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Express&lt;/a&gt;. Follow these steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Create a working directory for the application on your development system:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;mkdir myproject
cd myproject
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create a &lt;em&gt;package.json&lt;/em&gt; file listing the dependencies for the project:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;my-express-app&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;#34;version&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;1.0.0&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;#34;description&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Express Hello World app&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;#34;main&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;app.js&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;#34;scripts&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;nt&#34;&gt;&amp;#34;start&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;node app.js&amp;#34;&lt;/span&gt;
  &lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;#34;dependencies&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;nt&#34;&gt;&amp;#34;express&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;4.17.1&amp;#34;&lt;/span&gt;
  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create an &lt;em&gt;app.js&lt;/em&gt; file for the Express application which returns a &amp;ldquo;Hello world&amp;rdquo; message on access:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-javascript&#34; data-lang=&#34;javascript&#34;&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;use strict&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;

&lt;span class=&#34;kr&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;express&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;require&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;express&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;

&lt;span class=&#34;kr&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;PORT&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;process&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;env&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;PORT&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;||&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;

&lt;span class=&#34;kr&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;app&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;express&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;();&lt;/span&gt;

&lt;span class=&#34;nx&#34;&gt;app&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;get&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;/&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kd&#34;&gt;function&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;req&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;res&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
  &lt;span class=&#34;nx&#34;&gt;res&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;send&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Hello world\n&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;});&lt;/span&gt;

&lt;span class=&#34;nx&#34;&gt;app&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;listen&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;PORT&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;span class=&#34;nx&#34;&gt;console&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;log&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Running on http://localhost:&amp;#39;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;PORT&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you have Node.js installed in your local environment, you can test this application by running the following commands and then browsing to &lt;em&gt;http://localhost:3000&lt;/em&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;npm install
npm start
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;step-2-configure-the-application-pipeline-with-skaffold&#34;&gt;Step 2: Configure the application pipeline with Skaffold&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://skaffold.dev/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Skaffold&lt;/a&gt; is an open source tool that makes it easy to build and deploy containers on Kubernetes. In this step, you will configure Skaffold to build your Express application as a container and to deploy it on the cluster using Helm.&lt;/p&gt;
&lt;p&gt;Begin by downloading and install Skaffold using the &lt;a href=&#34;https://skaffold.dev/docs/install/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;official installation instructions for your platform&lt;/a&gt;. Once Skaffold is installed, create a file named &lt;em&gt;skaffold.yaml&lt;/em&gt; in your project directory and fill it with the content below. To ensure this works correctly with your Docker Hub account, replace the DOCKER-HUB-USERNAME placeholder with your Docker Hub username.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;skaffold/v2alpha2&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Config&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;my-express-app&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;build&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;artifacts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;DOCKER-HUB-USERNAME/my-express-app&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;deploy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;helm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;releases&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;my-express-app&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;chartPath&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;bitnami/node&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;remote&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;true&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;setValues&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;image.repository&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;DOCKER-HUB-USERNAME/my-express-app&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;service.type&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;LoadBalancer&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;getAppFromExternalRepository&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;false&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;applicationPort&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;3000&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;setValueTemplates&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;image.tag&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;{{ .DIGEST_HEX }}&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This file defines the Skaffold configuration for build and deploy stages. Let&amp;rsquo;s look at each stage in detail.&lt;/p&gt;
&lt;h3 id=&#34;build-stage&#34;&gt;Build stage&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nn&#34;&gt;...&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;build&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;artifacts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;DOCKER-HUB-USERNAME/my-express-app&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;...&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In the build stage, Skaffold will use Docker and a local &lt;em&gt;Dockerfile&lt;/em&gt; to build the application, and then push it to a registry (by default, Docker Hub). Instead of creating a &lt;em&gt;Dockerfile&lt;/em&gt; from scratch, you will streamline your work by using &lt;a href=&#34;https://github.com/bitnami/bitnami-docker-node/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami&amp;rsquo;s Node.js Docker image&lt;/a&gt;, which comes with the latest bug fixes and most secure version of Node.js. Create the &lt;em&gt;Dockerfile&lt;/em&gt; as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;FROM bitnami/node:10 as builder
ENV NODE_ENV=&amp;quot;production&amp;quot;

# Copy app&#39;s source code to the /app directory
COPY . /app

# The application&#39;s directory will be the working directory
WORKDIR /app

# Install Node.js dependencies defined in &#39;/app/package.json&#39;
RUN npm install

FROM bitnami/node:10-prod
ENV NODE_ENV=&amp;quot;production&amp;quot;
COPY --from=builder /app /app
WORKDIR /app
ENV PORT 3000
EXPOSE 3000

# Start the application
CMD [&amp;quot;npm&amp;quot;, &amp;quot;start&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Skaffold will automatically build and tag the image using this &lt;em&gt;Dockerfile&lt;/em&gt;. Once the image is built and tagged, the &lt;em&gt;image&lt;/em&gt; parameter in the build configuration tells Skaffold where to push the built image.&lt;/p&gt;
&lt;p&gt;If you are not already logged in to Docker Hub, use the command below to log in before proceeding. This is necessary so that Skaffold can push its built images to your Docker Hub account.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;docker login
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;deploy-stage&#34;&gt;Deploy stage&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nn&#34;&gt;...&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;deploy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;helm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;releases&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;my-express-app&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;chartPath&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;bitnami/node&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;remote&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;true&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;setValues&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;image.repository&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;DOCKER-HUB-USERNAME/my-express-app&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;service.type&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;LoadBalancer&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;getAppFromExternalRepository&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;false&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;applicationPort&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;3000&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;setValueTemplates&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;image.tag&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;{{ .DIGEST_HEX }}&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In the deploy stage, Skaffold will use a Helm chart to deploy the built container to the Kubernetes cluster. Instead of spending time and effort to create and test a custom Helm chart for this purpose, you will use &lt;a href=&#34;https://github.com/bitnami/charts/tree/master/bitnami/node&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami&amp;rsquo;s Node.js Helm chart&lt;/a&gt;. This is a ready-to-use, preconfigured solution that is compliant with current best practices and includes all the tools you need for a Node.js deployment on Kubernetes.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;remote&lt;/em&gt; parameter tells Skaffold that the chart is available in a remote repository (not locally) and the &lt;em&gt;chartPath&lt;/em&gt; parameter tells Skaffold where to find it. Skaffold relies on Helm&amp;rsquo;s repository list to locate the chart, so when using Bitnami&amp;rsquo;s Node.js chart as shown above, you must ensure that the Bitnami chart repository is known to Helm. If this is not the case, use the command below before proceeding to the next step:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;helm repo add bitnami https://charts.bitnami.com/bitnami
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Bitnami&amp;rsquo;s Node.js chart comes with a large number of configurable parameters, which allow you to tune the deployment to your needs. These parameters are set through the &lt;em&gt;setValues&lt;/em&gt; and &lt;em&gt;setValueTemplates&lt;/em&gt; options. Here&amp;rsquo;s a quick explanation of the ones used above:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;em&gt;image.repository&lt;/em&gt; parameter tells the chart where to find the built image - in this case, the Docker Hub repository.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;image.tag&lt;/em&gt; parameter sets the tag to identify the image by - in this case, the auto-generated tag from the build stage.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;getAppFromExternalRepository&lt;/em&gt; parameter is set to &lt;em&gt;false&lt;/em&gt; because the application is already included in the generated image and does not need to be downloaded from an external source.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;service.type&lt;/em&gt; parameter configures the deployment to be available at a public load balancer IP address so that it can be easily reviewed or tested.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;applicationPort&lt;/em&gt; parameter exposes the application on port 3000, which is the port configured for the application in the &lt;em&gt;Dockerfile&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;For Node.js applications that use MongoDB, Bitnami&amp;rsquo;s Node.js chart will automatically deploy MongoDB on your cluster. Alternatively, if you&amp;rsquo;re using a cloud-based MongoDB service, you can skip the cluster deployment of MongoDB and instead configure the chart with the necessary credentials to connect to your remote MongoDB service. &lt;a href=&#34;https://github.com/bitnami/charts/tree/master/bitnami/node#parameters&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;See the complete list of available chart parameters&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;step-3-continuously-build-and-test-the-application&#34;&gt;Step 3: Continuously build and test the application&lt;/h2&gt;
&lt;p&gt;You&amp;rsquo;re now ready to build and test the application. Use the command below to tell Skaffold to start monitoring your project directory and build and deploy your Express application on Kubernetes using Bitnami&amp;rsquo;s container image and Helm chart:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;skaffold dev
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In the build stage, Skaffold will build, tag and push your application to Docker Hub, as shown below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/express-app-bitnami-skaffold-octant/build.png&#34; alt=&#34;Build output&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Once the image is built and pushed, Skaffold will proceed to the deployment stage and deploy the application on Kubernetes with Helm:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/express-app-bitnami-skaffold-octant/deploy.png&#34; alt=&#34;Deployment output&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Once the deployment is complete, use the command shown in the output of the Helm chart to obtain the load balancer IP address for the service:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/express-app-bitnami-skaffold-octant/service.png&#34; alt=&#34;Service details&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;If you browse to the IP address, you should see the output of the Express application, as shown below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/express-app-bitnami-skaffold-octant/example-1.png&#34; alt=&#34;Example output&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;To test the CD feature, make a change to the application - for example, update the message &amp;ldquo;Hello world&amp;rdquo; in the &lt;em&gt;app.js&lt;/em&gt; file to &amp;ldquo;Aloha world&amp;rdquo;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;sed -i &amp;#39;s/Hello world/Aloha world/g&amp;#39; app.js
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The change should be detected by Skaffold, which will build, push and deploy a fresh version of the application image to the Kubernetes cluster. Once the process is complete, browse to the application URL again and you should see the modified output of the application, as shown below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/express-app-bitnami-skaffold-octant/example-2.png&#34; alt=&#34;Example output&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;At this point, you have successfully created a continuous development pipeline between your working directory and your Kubernetes cluster.&lt;/p&gt;
&lt;h2 id=&#34;step-4-monitor-and-inspect-the-running-application-with-octant&#34;&gt;Step 4: Monitor and inspect the running application with Octant&lt;/h2&gt;
&lt;p&gt;As you&amp;rsquo;re developing your application, it&amp;rsquo;s a good idea to monitor it on the cluster in real time. This gives you an idea of how it will behave under different conditions, and also lets you see the effect of changes you make to its deployment architecture.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://octant.dev&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Octant&lt;/a&gt; is an open source tool that lets you monitor your cluster and the applications running on it in real time through a simple Web interface. It also lets you debug faster by supporting granular inspection of Kubernetes objects and streaming container logs.&lt;/p&gt;
&lt;p&gt;Begin by downloading and install Octant to your system path using the &lt;a href=&#34;https://octant.dev/docs/master/getting-started&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;official installation instructions for your platform&lt;/a&gt;. Once Octant is installed, run it with the command below:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;octant
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This will start a browser on your system and direct you to the Octant dashboard. In case you&amp;rsquo;re running Octant on a different or virtual host, you can instead use the command below and then browse to http://IP-ADDRESS:7777, where IP-ADDRESS is the IP address of the Octant host:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;octant --listener-addr 0.0.0.0:7777
&lt;/code&gt;&lt;/pre&gt;&lt;blockquote&gt;
&lt;p&gt;Note that Octant does not come with authentication enabled so, if accessing it remotely from a different host, ensure that your firewall configuration only allows access from whitelisted hosts or add your own authentication layer to avoid cluster information being exposed to unauthorized users.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The Octant dashboard will display the running workloads on the cluster:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/express-app-bitnami-skaffold-octant/workloads.png&#34; alt=&#34;Octant workload display&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;You can also drill down to see individual pods, services, nodes and other cluster objects:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/express-app-bitnami-skaffold-octant/pods.png&#34; alt=&#34;Octant pod display&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Octant updates its data in real time so, when you make a change to your application and have it redeployed by Skaffold, the impact will be immediately visible in the Octant dashboard. This makes it a powerful tool to help developers understand how the application they are working on behaves on Kubernetes.&lt;/p&gt;
&lt;h2 id=&#34;useful-links&#34;&gt;Useful links&lt;/h2&gt;
&lt;p&gt;To learn more about the topics discussed in this guide, use the links below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bitnami/bitnami-docker-node&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami Node.js container&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bitnami/charts/tree/master/bitnami/node&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami Node.js Helm chart&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://skaffold.dev/docs/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Skaffold documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://octant.dev/docs/master/getting-started/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Octant documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://expressjs.com/en/starter/installing.html&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Express documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: Customize GitLab&#39;s Default Auto DevOps Pipeline with Bitnami&#39;s Helm Charts</title>
      
      <link>/guides/ci-cd/gitlab-customize-default/</link>
      <pubDate>Wed, 15 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/guides/ci-cd/gitlab-customize-default/</guid>
      <description>

        
        &lt;p&gt;&lt;a href=&#34;https://bitnami.com/stack/gitlab&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami&amp;rsquo;s GitLab CE stack&lt;/a&gt; is a popular solution for agile development teams, providing a secure and fully-functional GitLab instance for software development and deployment. GitLab CE includes a Continuous Integration and Delivery (CI/CD) system that can build, test, and deploy software updates as well as a private registry for Docker containers.&lt;/p&gt;
&lt;p&gt;Bitnami also provides &lt;a href=&#34;https://github.com/bitnami/charts&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Helm charts&lt;/a&gt; for common applications. These charts make it easy to run applications on Kubernetes and are secure, up-to-date and built in conformance with current best practices. By combining these Helm charts with GitLab&amp;rsquo;s CI/CD system, developers can quickly create custom deployment pipelines on Kubernetes infrastructure for development, testing and production scenarios.&lt;/p&gt;
&lt;p&gt;This guide shows you how to integrate Bitnami&amp;rsquo;s Helm charts with GitLab&amp;rsquo;s Auto DevOps pipeline and create custom deployments on Kubernetes. With this configuration, every change to the application code in GitLab is automatically built as a Docker container and deployed to the Kubernetes cluster using &lt;a href=&#34;https://github.com/bitnami/charts/tree/master/bitnami/node&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami&amp;rsquo;s Node.js Helm chart&lt;/a&gt;. Bitnami&amp;rsquo;s Helm charts support a number of additional parameters to ease this integration, including the ability to configure each deployment such that it is available at a public IP address for review and test.&lt;/p&gt;
&lt;h2 id=&#34;assumptions-and-prerequisites&#34;&gt;Assumptions and prerequisites&lt;/h2&gt;
&lt;p&gt;This guide assumes that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;You have deployed the &lt;a href=&#34;https://bitnami.com/stack/gitlab&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami GitLab CE stack&lt;/a&gt; on a cloud server, connected it with your Kubernetes cluster and configured a default Auto DevOps pipeline for your application. For more information on how to complete these tasks, refer to our &lt;a href=&#34;https://docs.bitnami.com/tutorials/create-ci-cd-pipeline-gitlab-kubernetes/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;detailed tutorial on creating a CI/CD pipeline with GitLab and Kubernetes&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Your application is a Node.js application and the application code is hosted in a GitLab repository. &lt;a href=&#34;https://docs.bitnami.com/tutorials/create-ci-cd-pipeline-gitlab-kubernetes/#step-6-commit-test-and-repeat&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Obtain the sample application&amp;rsquo;s code and the Dockerfile to build it&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Other programming languages or frameworks will require a different chart, but the broad steps to configure and integrate a custom Helm chart with GitLab&amp;rsquo;s Auto DevOps will remain the same.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You have &lt;a href=&#34;https://git-scm.com/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Git&lt;/a&gt; and &lt;a href=&#34;https://docs.bitnami.com/kubernetes/get-started-kubernetes#step-3-install-kubectl-command-line&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;em&gt;kubectl&lt;/em&gt;&lt;/a&gt; installed and configured to work with your GitLab CE installation and Kubernetes cluster respectively.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;step-1-replace-the-default-auto-devops-helm-chart-with-a-custom-chart&#34;&gt;Step 1: Replace the default Auto DevOps Helm chart with a custom chart&lt;/h2&gt;
&lt;p&gt;GitLab configures the Auto DevOps pipeline with a default Helm chart, but it&amp;rsquo;s possible to override this with a custom chart by either adding a chart to your code repository or, for charts hosted externally, by setting various pipeline variables that tell GitLab how to obtain the chart. In this tutorial, since the assumption is that you&amp;rsquo;re using a Node.js application, the latter approach will be followed and GitLab will be configured to use the Bitnami Node.js chart from the Bitnami chart repository.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Navigate to the project&amp;rsquo;s &amp;ldquo;Settings -&amp;gt; CI/CD&amp;rdquo; page.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the &amp;ldquo;Variables&amp;rdquo; section, add the following variables and values:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;AUTO_DEVOPS_CHART=bitnami/node
AUTO_DEVOPS_CHART_REPOSITORY=https://charts.bitnami.com/bitnami
AUTO_DEVOPS_CHART_REPOSITORY_NAME=bitnami
HELM_RELEASE_NAME=myproject
HELM_UPGRADE_EXTRA_ARGS=--set fullnameOverride=myproject --set service.type=LoadBalancer --set getAppFromExternalRepository=false --set applicationPort=5000 --set image.pullSecrets={gitlab-registry} --set image.registry=
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Click &amp;ldquo;Save variables&amp;rdquo;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/gitlab-customize-default/set-variables.png&#34; alt=&#34;GitLab Auto DevOps configuration&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Here is a quick explanation of what these variables do:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;em&gt;AUTO_DEVOPS_CHART&lt;/em&gt; variable sets the name of the custom chart to use, while the &lt;em&gt;AUTO_DEVOPS_CHART_REPOSITORY&lt;/em&gt; and &lt;em&gt;AUTO_DEVOPS_CHART_REPOSITORY_NAME&lt;/em&gt; variables define the chart repository URL and chart repository name respectively.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;HELM_RELEASE_NAME&lt;/em&gt; variable sets the Helm release name.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;HELM_UPGRADE_EXTRA_ARGS&lt;/em&gt; variable sets the list of arguments to be passed to the custom chart (this varies per chart).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For the Bitnami Node.js chart, the parameters passed via the &lt;em&gt;HELM_UPGRADE_EXTRA_ARGS&lt;/em&gt; command are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;em&gt;fullnameOverride&lt;/em&gt; parameter configures the deployment name. It must be set to the same value as the &lt;em&gt;HELM_RELEASE_NAME&lt;/em&gt; variable, or else Auto DevOps will fail to recognize the deployment and mark the pipeline as failed.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;service.type&lt;/em&gt; parameter configures the deployment to be available at a public load balancer IP address so that it can be easily reviewed or tested.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;getAppFromExternalRepository&lt;/em&gt; parameter is set to &lt;em&gt;false&lt;/em&gt; because the application is already included in the generated container image and does not need to be downloaded from an external source.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;applicationPort&lt;/em&gt; parameter exposes the application on port 5000, which is the port configured for the application in the &lt;em&gt;Dockerfile&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;image.registry&lt;/em&gt; parameter is set to an empty value, because GitLab already provides its own image registry.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;image.pullSecrets&lt;/em&gt; parameter specifies the secret holding the GitLab registry credentials. The secret is already created by Auto DevOps.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can also view the &lt;a href=&#34;https://github.com/bitnami/charts/tree/master/bitnami/node#parameters&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;complete list of parameters supported by the chart&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Using a LoadBalancer service type will typically assign a static IP address for the deployment. Depending on your cloud provider&amp;rsquo;s policies, you may incur additional charges for this static IP address.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;step-2-commit-code-and-test-the-pipeline&#34;&gt;Step 2: Commit code and test the pipeline&lt;/h2&gt;
&lt;p&gt;At this point, you are ready to commit some code to the project and have GitLab test and deploy it using the Bitnami Helm chart. This tutorial assumes that you have a simple &amp;ldquo;Hello, world&amp;rdquo; application in Node.js, together with a Dockerfile to build it. If you don&amp;rsquo;t, you can &lt;a href=&#34;https://docs.bitnami.com/tutorials/create-ci-cd-pipeline-gitlab-kubernetes/#step-6-commit-test-and-repeat&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;obtain the sample application&amp;rsquo;s code and a Dockerfile to build it&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Commit a change to the application - for example, update the message &amp;ldquo;Hello world&amp;rdquo; in the &lt;em&gt;server.js&lt;/em&gt; file to &amp;ldquo;Yahoo world&amp;rdquo; - and push the change to GitLab:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;sed -i &amp;#39;s/Hello world/Yahoo world/g&amp;#39; server.js
git add .
git commit -m &amp;#34;Modified message text&amp;#34;
git push origin master
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Pushing this commit should automatically trigger the Auto DevOps pipeline in GitLab.&lt;/p&gt;
&lt;p&gt;In the first stage, GitLab will attempt to build a container image containing the application code using the provided Dockerfile. The container will be pushed to the internal GitLab registry. Here&amp;rsquo;s an example of the output you should see in this first stage:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/gitlab-customize-default/pipeline-build.png&#34; alt=&#34;GitLab pipeline build output&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Once the container image has been built and pushed, the second stage of the pipeline will attempt to deploy it to Kubernetes for review using Bitnami&amp;rsquo;s Node.js Helm chart. If successful, the stage output will display the commands you must run to obtain the load balancer IP address which you can browse to in order to see the application in action. Here&amp;rsquo;s an example of the output you should see in this second stage:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/gitlab-customize-default/pipeline-deploy.png&#34; alt=&#34;GitLab pipeline deployment output&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;If you run the commands shown in the output, you should see the load balancer IP address, as shown below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/gitlab-customize-default/service.png&#34; alt=&#34;Service IP address&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Browse to the service IP address listed in the output and you should see the output of the Node.js application, as shown below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/gitlab-customize-default/example-1.png&#34; alt=&#34;Example output&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;At this point, you have successfully customized the default Auto DevOps pipeline provided by GitLab to use Bitnami&amp;rsquo;s Node.js Helm chart. Doing this ensures that your application deployments will always use a secure and up-to-date version of Node.js, while also giving you the flexibility to further customize the deployment as needed.&lt;/p&gt;
&lt;h2 id=&#34;useful-links&#34;&gt;Useful links&lt;/h2&gt;
&lt;p&gt;To learn more about the topics discussed in this guide, use the links below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.bitnami.com/general/apps/gitlab/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami GitLab CE stack documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bitnami/bitnami-docker-node&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami Node.js container&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bitnami/charts/tree/master/bitnami/node&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami Node.js Helm chart&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.gitlab.com&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;GitLab documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: Create a Continuous Integration Pipeline with GitLab and Kubernetes</title>
      
      <link>/guides/ci-cd/gitlab-auto-devops/</link>
      <pubDate>Wed, 18 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/guides/ci-cd/gitlab-auto-devops/</guid>
      <description>

        
        &lt;p&gt;As development velocity increases, it&amp;rsquo;s now become essential for enterprises to have a reliable and readily-available Continuous Integration/Continuous Delivery (CI/CD) pipeline integrated with cloud infrastructure. But although the requirements of such infrastructure are well understood, setting up this pipeline is still a complex task involving knowledge of cloud platforms, containerization tools like Docker, Docker Compose and others, container orchestration tools like Kubernetes and Helm, and DevOps tools and techniques.&lt;/p&gt;
&lt;p&gt;Bitnami eases the task of building an enterprise-ready CI/CD pipeline with its application stacks and container images.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://bitnami.com/stack/gitlab&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami&amp;rsquo;s GitLab CE stack&lt;/a&gt; lets you deploy a secure and fully-functional GitLab instance on the cloud in a matter of minutes and integrate it with a Kubernetes cluster.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://bitnami.com/containers&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami&amp;rsquo;s containers&lt;/a&gt; for &lt;a href=&#34;https://github.com/bitnami/bitnami-docker-node&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Node.js&lt;/a&gt;, &lt;a href=&#34;https://github.com/bitnami/bitnami-docker-ruby&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Ruby&lt;/a&gt;, &lt;a href=&#34;https://github.com/bitnami/bitnami-docker-java&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Java&lt;/a&gt;  and others makes it easy to containerize your applications in a secure and reliable manner.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Put the two together, and you have everything you need to create a modern, enterprise-grade CI/CD pipeline that leverages the scalability of Kubernetes with the flexibility of GitLab and the development agility of Bitnami containers. This guide walks you through the process.&lt;/p&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;This guide shows you how to set up a CI/CD pipeline between GitLab (deployed using the &lt;a href=&#34;https://bitnami.com/stack/gitlab&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami GitLab CE stack&lt;/a&gt;) and a Kubernetes cluster with GitLab&amp;rsquo;s Auto DevOps feature. With this configuration, every change to application code is automatically built as a Docker container (based on a &lt;a href=&#34;https://github.com/bitnami/bitnami-docker-node&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami Node.js base container&lt;/a&gt;) and deployed to the Kubernetes cluster for review and test.&lt;/p&gt;
&lt;p&gt;Communication and monitoring between the GitLab deployment and the Kubernetes cluster is achieved through the use of &lt;a href=&#34;https://helm.sh&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Helm&lt;/a&gt;, &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/ingress/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Ingress&lt;/a&gt; and &lt;a href=&#34;https://docs.gitlab.com/runner/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;GitLab Runner&lt;/a&gt;. When GitLab deploys each built container to the cluster, it also makes it available for review at an auto-generated sub-domain of your main domain name.&lt;/p&gt;
&lt;h2 id=&#34;assumptions-and-prerequisites&#34;&gt;Assumptions and prerequisites&lt;/h2&gt;
&lt;p&gt;This guide makes the following assumptions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You have deployed the Bitnami GitLab CE stack on a cloud server and have the GitLab CE administrator credentials. Learn about &lt;a href=&#34;https://docs.bitnami.com/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;deploying Bitnami applications&lt;/a&gt; and &lt;a href=&#34;https://docs.bitnami.com/general/faq/get-started/find-credentials/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;obtaining credentials&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;You have a multi-node Kubernetes cluster running. &lt;a href=&#34;https://docs.bitnami.com/kubernetes/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Learn about deploying a Kubernetes cluster on different cloud platforms&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;You have the &lt;em&gt;kubectl&lt;/em&gt; command line (kubectl CLI) installed. &lt;a href=&#34;https://docs.bitnami.com/kubernetes/get-started-kubernetes#step-3-install-kubectl-command-line&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Learn about &lt;em&gt;kubectl&lt;/em&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;You have &lt;a href=&#34;https://git-scm.com/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Git&lt;/a&gt; installed.&lt;/li&gt;
&lt;li&gt;You have a domain name and the ability to configure a wildcard DNS record for that domain name. &lt;a href=&#34;https://en.wikipedia.org/wiki/Wildcard_DNS_record&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Learn about wildcard DNS records&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;You have an SSH key pair which you can use for repository commits. To generate a new SSH key pair, use PuTTYgen (Windows) or the &lt;em&gt;ssh-keygen&lt;/em&gt; command (Linux and Mac OS X). Learn about &lt;a href=&#34;http://winscp.net/eng/docs/ui_puttygen&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;PuTTYgen&lt;/a&gt; and &lt;a href=&#34;http://www.macworld.co.uk/how-to/mac-software/how-generate-ssh-keys-3521606/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;em&gt;ssh-keygen&lt;/em&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;step-1-configure-dns-and-ssl-for-gitlab&#34;&gt;Step 1: Configure DNS and SSL for GitLab&lt;/h2&gt;
&lt;p&gt;As a first step, you must configure a domain name and SSL certificate for GitLab, such that browsing to the domain directs you to a secure page for your GitLab deployment. If you already have an SSL certificate for your domain, you can continue to use that or, if not, you can follow the approach below and generate a free Let&amp;rsquo;s Encrypt SSL certificate.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Follow the instructions to &lt;a href=&#34;https://docs.bitnami.com/general/faq/configuration/configure-custom-domain/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;configure a custom domain for GitLab&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;While logged in to the server console, &lt;a href=&#34;https://docs.bitnami.com/general/apps/gitlab/administration/generate-configure-certificate-letsencrypt/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;manually generate and install a Let&amp;rsquo;s Encrypt certificate using &lt;em&gt;lego&lt;/em&gt;&lt;/a&gt; as described in our guide.&lt;/li&gt;
&lt;li&gt;Test the configuration by browsing to &lt;em&gt;https://DOMAIN&lt;/em&gt; (replace the DOMAIN placeholder with the correct domain name) and confirming that you see a secure GitLab login page, as shown below:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/gitlab-auto-devops/gitlab-ssl.png&#34; alt=&#34;GitLab secure login&#34;  /&gt;&lt;/p&gt;
&lt;h2 id=&#34;step-2-configure-and-activate-the-gitlab-registry&#34;&gt;Step 2: Configure and activate the GitLab registry&lt;/h2&gt;
&lt;p&gt;The next step is to activate the GitLab registry, as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Log in to the server console using SSH (if you&amp;rsquo;re not already logged in).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Edit the &lt;em&gt;/etc/gitlab/gitlab.rb&lt;/em&gt; file and uncomment and update the &lt;em&gt;registry_external_url&lt;/em&gt; parameter as below, remembering to replace the DOMAIN placeholder with the GitLab domain name:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;registry_external_url &amp;#39;https://DOMAIN:5005&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the same file, uncomment and update the &lt;em&gt;external_url&lt;/em&gt; parameter as below, replacing the DOMAIN placeholder with the GitLab domain name:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;external_url &amp;#39;https://DOMAIN&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Save your changes to the file.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Configure the GitLab registry to use the SSL certificates generated in the previous step. Replace the DOMAIN placeholder with the GitLab domain name.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;cd /etc/gitlab/ssl
sudo ln -sf server.crt DOMAIN.crt
sudo ln -sf server.key DOMAIN.key
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Execute the commands below to reconfigure and restart GitLab with the changes.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;sudo gitlab-ctl reconfigure
sudo /opt/bitnami/ctlscript.sh restart
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Open port 5005 in the server firewall so that GitLab can connect to, and push built containers, to its internal registry. &lt;a href=&#34;https://docs.bitnami.com/general/faq/administration/use-firewall/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Learn about opening firewall ports for your cloud platform&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;step-3-create-a-new-gitlab-project&#34;&gt;Step 3: Create a new GitLab project&lt;/h2&gt;
&lt;p&gt;You can now log in to GitLab and prepare a new project. This project will host the code that you will eventually run through your CI/CD pipeline to build and deploy on Kubernetes.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Browse to your GitLab domain and log in using the administrator credentials.&lt;/li&gt;
&lt;li&gt;On the welcome page, select the &amp;ldquo;Create a project&amp;rdquo; option.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/gitlab-auto-devops/create-project.png&#34; alt=&#34;GitLab project creation&#34;  /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Enter a name and slug for your project. Set the visibility level to &amp;ldquo;Internal&amp;rdquo;. Click &amp;ldquo;Create project&amp;rdquo;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/gitlab-auto-devops/configure-project.png&#34; alt=&#34;GitLab project creation&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Your project is created and you should see the project page, as shown below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/gitlab-auto-devops/new-project.png&#34; alt=&#34;GitLab project page&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Click the &amp;ldquo;Clone&amp;rdquo; button and note the clone URL for the repository, which will be needed in &lt;a href=&#34;#step-6-commit-test-and-repeat&#34;&gt;Step 6&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/gitlab-auto-devops/project-clone-url.png&#34; alt=&#34;GitLab project clone URL&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Before you can commit any code to the project repository, you must add your SSH key to your profile, as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Click your user profile icon in the top right corner of the navigation bar.&lt;/li&gt;
&lt;li&gt;Select the &amp;ldquo;Settings&amp;rdquo; menu icon.&lt;/li&gt;
&lt;li&gt;On the &amp;ldquo;User Settings&amp;rdquo; page, select the &amp;ldquo;SSH Keys&amp;rdquo; menu item.&lt;/li&gt;
&lt;li&gt;Paste the public key component of your SSH key pair in the &amp;ldquo;Key&amp;rdquo; field. Add an optional label and click the &amp;ldquo;Add Key&amp;rdquo; button to save the changes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/gitlab-auto-devops/add-ssh-key.png&#34; alt=&#34;GitLab key addition&#34;  /&gt;&lt;/p&gt;
&lt;h2 id=&#34;step-4-configure-a-kubernetes-cluster-for-the-project&#34;&gt;Step 4: Configure a Kubernetes cluster for the project&lt;/h2&gt;
&lt;p&gt;GitLab comes with built-in support for Kubernetes, making it easy to build and test your projects using a Kubernetes cluster. &lt;a href=&#34;https://docs.gitlab.com/ee/user/project/clusters/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Learn more about Kubernetes support in GitLab&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;First, allow outbound requests from GitLab hooks and services, as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Navigate to the GitLab administration panel by selecting the &amp;ldquo;Admin Area&amp;rdquo; link.&lt;/li&gt;
&lt;li&gt;Navigate to the &amp;ldquo;Settings -&amp;gt; Network&amp;rdquo; page and select the &amp;ldquo;Outbound requests&amp;rdquo; section.&lt;/li&gt;
&lt;li&gt;Tick the checkboxes to allow requests to the local network from hooks and services.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/gitlab-auto-devops/allow-network-requests.png&#34; alt=&#34;GitLab network requests&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Then, configure your Kubernetes cluster in GitLab by following these steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use the &lt;em&gt;kubectl&lt;/em&gt; command-line tool to obtain the following details for your Kubernetes cluster using the &lt;a href=&#34;https://docs.gitlab.com/ee/user/project/clusters/#add-existing-kubernetes-cluster&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;instructions in the GitLab documentation&lt;/a&gt;:
&lt;ul&gt;
&lt;li&gt;Cluster API URL&lt;/li&gt;
&lt;li&gt;Cluster CA certificate&lt;/li&gt;
&lt;li&gt;Cluster service token&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;From the project page in GitLab, select the &amp;ldquo;Operations -&amp;gt; Kubernetes&amp;rdquo; menu item.&lt;/li&gt;
&lt;li&gt;On the resulting page, click the &amp;ldquo;Add Kubernetes cluster&amp;rdquo; button.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/gitlab-auto-devops/add-cluster.png&#34; alt=&#34;GitLab cluster configuration&#34;  /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Select the &amp;ldquo;Add existing cluster&amp;rdquo; tab.&lt;/li&gt;
&lt;li&gt;Enter a name for your cluster with the API URL, CA certificate and server token obtained already. Check the boxes for &amp;ldquo;RBAC-enabled&amp;rdquo; cluster and &amp;ldquo;GitLab-managed cluster&amp;rdquo;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/gitlab-auto-devops/configure-cluster.png&#34; alt=&#34;GitLab cluster configuration&#34;  /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Click the &amp;ldquo;Add Kubernetes cluster&amp;rdquo; button to save the changes.&lt;/li&gt;
&lt;li&gt;On the resulting page, find the &amp;ldquo;Applications&amp;rdquo; section and install Helm, followed by Ingress. Note the Ingress endpoint IP address generated after installing Ingress.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/gitlab-auto-devops/install-helm-ingress.png&#34; alt=&#34;Helm/Ingress installation&#34;  /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Configure a wildcard DNS record for your domain pointing to the Ingress IP address through your DNS provider&amp;rsquo;s control panel. Learn how to configure wildcard DNS records for popular DNS providers like &lt;a href=&#34;https://www.godaddy.com/help/set-up-wildcard-dns-3301&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;GoDaddy&lt;/a&gt;, &lt;a href=&#34;https://www.namecheap.com/support/knowledgebase/article.aspx/597/2237/how-can-i-set-up-a-catchall-wildcard-subdomain&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;NameCheap&lt;/a&gt; and &lt;a href=&#34;https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/DomainNameFormat.html#domain-name-format-asterisk&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;AWS Route53&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Enter the base domain name used by the wildcard DNS record in the &amp;ldquo;Base domain&amp;rdquo; field in your GitLab Kubernetes cluster configuration. For example, if you configured a wildcard DNS record for *&lt;em&gt;.example.com&lt;/em&gt;, use &lt;em&gt;example.com&lt;/em&gt; as the base domain name. This will be the base domain used for all Auto DevOps review deployments. Click &amp;ldquo;Save changes&amp;rdquo; to save the changes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/gitlab-auto-devops/configure-base-domain.png&#34; alt=&#34;GitLab base domain configuration&#34;  /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Return to the &amp;ldquo;Applications&amp;rdquo; section and install Cert-Manager. Remember to provide a valid email address so that Cert-Manager can correctly associate your certificates with your account.&lt;/li&gt;
&lt;li&gt;From the same &amp;ldquo;Applications&amp;rdquo; section, install GitLab Runner.&lt;/li&gt;
&lt;li&gt;Confirm that the runner is successfully installed and activated for the project by navigating to the project&amp;rsquo;s &amp;ldquo;Settings -&amp;gt; CI/CD&amp;rdquo; page and checking the status of the runner in the &amp;ldquo;Runners&amp;rdquo; section.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/gitlab-auto-devops/check-runner-status.png&#34; alt=&#34;GitLab runner status&#34;  /&gt;&lt;/p&gt;
&lt;h2 id=&#34;step-5-enable-auto-devops-for-the-project&#34;&gt;Step 5: Enable Auto DevOps for the project&lt;/h2&gt;
&lt;p&gt;Once the Kubernetes integration is complete and a runner is active, enable Auto DevOps for the project. Auto DevOps provides a preconfigured CI/CD pipeline which can be used to quickly get started with building, testing and deploying your project. &lt;a href=&#34;https://docs.gitlab.com/ee/topics/autodevops/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Learn more about Auto DevOps in GitLab&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To enable Auto DevOps for the project:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Navigate to the project&amp;rsquo;s &amp;ldquo;Settings -&amp;gt; CI/CD&amp;rdquo; page.&lt;/li&gt;
&lt;li&gt;In the &amp;ldquo;Auto DevOps&amp;rdquo; section, check the box for &amp;ldquo;Default to Auto DevOps pipeline&amp;rdquo; and select the &amp;ldquo;Continuous deployment to production&amp;rdquo; strategy.&lt;/li&gt;
&lt;li&gt;Click &amp;ldquo;Save changes&amp;rdquo; to enable the default pipeline.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/gitlab-auto-devops/enable-auto-devops.png&#34; alt=&#34;GitLab Auto DevOps configuration&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;The default Auto DevOps pipeline comes with various stages already configured, depending on which version of GitLab you are running. For example, there are stages to build, run tests, check code quality, scan for dependencies, review code, deploy code and test performance. This default pipeline is fully customizable and stages can be added or removed depending on your requirements, simply by adjusting pipeline variables. &lt;a href=&#34;https://docs.gitlab.com/ee/topics/autodevops/#environment-variables&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Learn about the available variables&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This tutorial will focus on creating a very simple pipeline consisting of only two stages: build and deploy. To turn off the other stages included in the default pipeline, follow these steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Navigate to the project&amp;rsquo;s &amp;ldquo;Settings -&amp;gt; CI/CD&amp;rdquo; page.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the &amp;ldquo;Variables&amp;rdquo; section, add the following three variables and values:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;TEST_DISABLED: true
CODE_QUALITY_DISABLED: true
PERFORMANCE_DISABLED: true
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Click &amp;ldquo;Save variables&amp;rdquo;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/gitlab-auto-devops/set-variables.png&#34; alt=&#34;GitLab Auto DevOps configuration&#34;  /&gt;&lt;/p&gt;
&lt;h2 id=&#34;step-6-commit-test-and-repeat&#34;&gt;Step 6: Commit, test and repeat&lt;/h2&gt;
&lt;p&gt;At this point, you are ready to commit some code to the project and have GitLab test and deploy it. This tutorial will create a simple &amp;ldquo;Hello, world&amp;rdquo; application in Node.js and then configure a Dockerfile to run it with Bitnami&amp;rsquo;s Node.js development container image.&lt;/p&gt;
&lt;p&gt;Follow these steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Create a working directory for the application on your local host:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;mkdir myproject
cd myproject
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create a &lt;em&gt;package.json&lt;/em&gt; file listing the dependencies for the project:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;simple-node-app&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;#34;version&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;1.0.0&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;#34;description&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Node.js on Docker&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;#34;main&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;server.js&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;#34;scripts&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;nt&#34;&gt;&amp;#34;start&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;node server.js&amp;#34;&lt;/span&gt;
  &lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;#34;dependencies&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;nt&#34;&gt;&amp;#34;express&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;^4.13&amp;#34;&lt;/span&gt;
  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create a &lt;em&gt;server.js&lt;/em&gt; file for the Express application which returns a &amp;ldquo;Hello world&amp;rdquo; message on access:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-javascript&#34; data-lang=&#34;javascript&#34;&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;use strict&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;

&lt;span class=&#34;kr&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;express&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;require&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;express&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;// Constants
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;kr&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;PORT&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;process&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;env&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;PORT&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;||&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;// App
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;kr&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;app&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;express&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;();&lt;/span&gt;
&lt;span class=&#34;nx&#34;&gt;app&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;get&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;/&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kd&#34;&gt;function&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;req&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;res&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
  &lt;span class=&#34;nx&#34;&gt;res&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;send&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Hello world\n&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;});&lt;/span&gt;

&lt;span class=&#34;nx&#34;&gt;app&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;listen&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;PORT&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;span class=&#34;nx&#34;&gt;console&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;log&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Running on http://localhost:&amp;#39;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;PORT&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create a &lt;em&gt;Dockerfile&lt;/em&gt; with the following content:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-dockerfile&#34; data-lang=&#34;dockerfile&#34;&gt;&lt;span class=&#34;k&#34;&gt;FROM&lt;/span&gt;&lt;span class=&#34;s&#34;&gt; bitnami/node:9 as builder&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;ENV&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;NODE_ENV&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;production&amp;#34;&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# Copy app&amp;#39;s source code to the /app directory&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;COPY&lt;/span&gt; . /app&lt;span class=&#34;err&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# The application&amp;#39;s directory will be the working directory&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;WORKDIR&lt;/span&gt;&lt;span class=&#34;s&#34;&gt; /app&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# Install Node.js dependencies defined in &amp;#39;/app/packages.json&amp;#39;&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;RUN&lt;/span&gt; npm install&lt;span class=&#34;err&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;FROM&lt;/span&gt;&lt;span class=&#34;s&#34;&gt; bitnami/node:9-prod&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;ENV&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;NODE_ENV&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;production&amp;#34;&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;COPY&lt;/span&gt; --from&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;builder /app /app&lt;span class=&#34;err&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;WORKDIR&lt;/span&gt;&lt;span class=&#34;s&#34;&gt; /app&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;ENV&lt;/span&gt; PORT &lt;span class=&#34;m&#34;&gt;5000&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;EXPOSE&lt;/span&gt;&lt;span class=&#34;s&#34;&gt; 5000&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# Start the application&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;CMD&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;npm&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;start&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This multi-stage &lt;em&gt;Dockerfile&lt;/em&gt; creates a new image using Bitnami&amp;rsquo;s Node.js container image as base. It copies the application files to the container&amp;rsquo;s &lt;em&gt;/app&lt;/em&gt; directory and then runs &lt;em&gt;npm install&lt;/em&gt; to install Express. It then creates a production-ready container image and configures the application to listen to request on port 5000.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Exposing the application on port 5000 is a requirement of GitLab&amp;rsquo;s &lt;a href=&#34;https://gitlab.com/gitlab-org/charts/auto-deploy-app&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;default Helm chart&lt;/a&gt;, which is used to deploy the application to the cluster. This can be overridden if needed using a custom Helm chart. Read more in our tutorial on &lt;a href=&#34;https://docs.bitnami.com/tutorials/customize-ci-cd-pipeline-gitlab-bitnami-charts&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;using a custom Helm chart with the Auto DevOps pipeline&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Initialize a Git repository and commit and push the application code to GitLab. Replace the NAME and EMAIL-ADDRESS placeholders with your name and email address (if not already configured) and the CLONE-URL placeholder with the repository clone URL obtained in &lt;a href=&#34;#step-3-create-a-new-gitlab-project&#34;&gt;Step 3&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;git config --global user.name &amp;#34;NAME&amp;#34;
git config --global user.name &amp;#34;EMAIL-ADDRESS&amp;#34;
git init    
git remote add origin CLONE-URL
git add .
git commit -m &amp;#34;Initial commit&amp;#34;
git push origin master
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Pushing this commit should automatically trigger the Auto DevOps pipeline in GitLab. To see the pipeline in action, navigate to the project&amp;rsquo;s &amp;ldquo;CI/CD -&amp;gt; Pipelines&amp;rdquo; page and confirm that the pipeline is running, as shown below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/gitlab-auto-devops/running-pipeline.png&#34; alt=&#34;GitLab pipeline&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;In the first stage, GitLab will attempt to build a container image containing the application code using the provided Dockerfile. The container will be pushed to the internal GitLab registry. Here&amp;rsquo;s an example of the output you should see in this first stage:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/gitlab-auto-devops/pipeline-build.png&#34; alt=&#34;GitLab pipeline build output&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Once the container image has been built and pushed, the second stage of the pipeline will attempt to deploy it to Kubernetes for review. If successful, the stage output will display a URL, which you can browse to in order to see the application in action. Here&amp;rsquo;s an example of the output you should see in this second stage:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/gitlab-auto-devops/pipeline-deploy.png&#34; alt=&#34;GitLab pipeline deployment output&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;If you browse to the application URL listed in the output, you should see the output of the Node.js app, as shown below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/gitlab-auto-devops/example-1.png&#34; alt=&#34;Example output&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;To test the CI/CD feature, make a change to the application - for example, update the message &amp;ldquo;Hello world&amp;rdquo; in the &lt;em&gt;server.js&lt;/em&gt; file to &amp;ldquo;Aloha world&amp;rdquo; - and push the change to GitLab.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;sed -i &amp;#39;s/Hello world/Aloha world/g&amp;#39; server.js
git add .
git commit -m &amp;#34;Modified message text&amp;#34;
git push origin master
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The new commit should trigger the pipeline, causing a new build and deployment to take place, and the new application will be deployed on your cluster for review. As before, check pipeline status in GitLab, wait for it to complete and then browse to the application URL listed in the output of the second stage. You should see the revised output, as shown below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/ci-cd/gitlab-auto-devops/example-2.png&#34; alt=&#34;Example output&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;At this point, you have successfully created a simple CI/CD pipeline between GitLab and a Kubernetes cluster. You can now continue to enhance it by &lt;a href=&#34;https://docs.gitlab.com/ee/topics/autodevops/#features&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;adding new stages to the Auto DevOps pipeline&lt;/a&gt;, modifying how your code is deployed with a &lt;a href=&#34;https://docs.gitlab.com/ee/topics/autodevops/#custom-helm-chart&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;custom deployment Helm chart&lt;/a&gt;, or &lt;a href=&#34;https://docs.gitlab.com/ee/user/project/pipelines/schedules.html&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;configuring pipelines to run on a schedule&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;useful-links&#34;&gt;Useful links&lt;/h2&gt;
&lt;p&gt;To learn more about the topics discussed in this guide, use the links below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.bitnami.com/general/apps/gitlab/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami GitLab CE stack documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bitnami/bitnami-docker-node#&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami Node.js container&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.bitnami.com/general/faq/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami applications FAQ&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.bitnami.com/kubernetes/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami documentation for Kubernetes deployments on different cloud platforms&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.gitlab.com&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;GitLab documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Wildcard_DNS_record&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Wildcard DNS records&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Key generation with &lt;a href=&#34;http://winscp.net/eng/docs/ui_puttygen&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;PuTTYgen&lt;/a&gt; and &lt;a href=&#34;http://www.macworld.co.uk/how-to/mac-software/how-generate-ssh-keys-3521606/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;em&gt;ssh-keygen&lt;/em&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
  </channel>
</rss>
