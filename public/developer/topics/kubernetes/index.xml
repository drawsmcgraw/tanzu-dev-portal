<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>VMware Tanzu Developer Center – Kubernetes</title>
    <link>/topics/kubernetes/</link>
    <description>Recent content in Kubernetes on VMware Tanzu Developer Center</description>
    <generator>Hugo -- gohugo.io</generator>
    
	  <atom:link href="/topics/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      
      <title>Guides: Controlling Ingress with Contour</title>
      
      <link>/guides/kubernetes/controlling-ingress-with-contour/</link>
      <pubDate>Fri, 26 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/guides/kubernetes/controlling-ingress-with-contour/</guid>
      <description>

        
        &lt;p&gt;In Kubernetes, Ingress is a set of routing rules that define how external traffic is routed to an application inside a Kubernetes cluster. An Ingress controller watches for changes to objects in the cluster and then wires together a data path for each request to be resolved. An Ingress controller processes the requests for resources, provides transport layer security (TLS) termination, and performs other functions.&lt;/p&gt;
&lt;p&gt;Ingress is an important component of Kubernetes because it cleanly separates an application from how it is accessed. A cluster administrator enables access to the application through the Ingress controller, while the application developer focuses on the application itself. Ingress, and the Ingress Controller, provide the glue that tie the two together.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://projectcontour.io/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Contour&lt;/a&gt; is an open source Kubernetes &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Ingress controller&lt;/a&gt; that acts as a control plane for the Envoy edge and service proxy (see below).​ Contour supports dynamic configuration updates and multi-team ingress delegation while maintaining a lightweight profile.&lt;/p&gt;
&lt;p&gt;Contour is built for Kubernetes to empower you to quickly deploy cloud native applications by using the flexible IngressRoute API. Contour deploys the Envoy proxy as a reverse proxy and load balancer.&lt;/p&gt;
&lt;h3 id=&#34;what-is-envoy&#34;&gt;What Is Envoy?&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.envoyproxy.io/docs/envoy/latest/intro/what_is_envoy&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Envoy&lt;/a&gt; is a Layer 7 (application layer) bus for proxy and communication in modern service-oriented architectures, such as Kubernetes clusters. Envoy strives to make the network transparent to applications while maximizing observability to ease troubleshooting.&lt;/p&gt;
&lt;h2 id=&#34;what-problems-does-contour-solve&#34;&gt;What Problems Does Contour Solve?&lt;/h2&gt;
&lt;p&gt;One of the most critical needs when running workloads at scale on Kubernetes is efficient and smooth traffic Ingress management at the application layer. Getting an application up and running is not the entire story; the app still needs a way for users to access it. Contour was designed to fill this operational gap.&lt;/p&gt;
&lt;h2 id=&#34;benefits-of-using-contour&#34;&gt;Benefits of Using Contour&lt;/h2&gt;
&lt;p&gt;Here are some of the benefits of using Contour:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Quickly deploy and integrate Envoy with a simple installation mechanism&lt;/li&gt;
&lt;li&gt;Safely support Ingress in multi-team Kubernetes clusters&lt;/li&gt;
&lt;li&gt;Cleanly integrate with the Kubernetes object model&lt;/li&gt;
&lt;li&gt;Dynamically update the Ingress configuration without dropped connections&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;keep-learning&#34;&gt;Keep Learning&lt;/h2&gt;
&lt;p&gt;Ingress and Ingress controllers remain an active topic in Kubernetes. Watch this short video for an &lt;a href=&#34;https://kube.academy/lessons/introduction-to-ingress&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Introduction to Ingress&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The Contour Ingress controller has become popular because of features such as the ability to do blue-green deployments using &lt;a href=&#34;https://tanzu.vmware.com/content/blog/deploying-new-app-versions-by-using-blue-green-deployments-with-contour-s-ingressroute&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Contour’s IngressRoute&lt;/a&gt;. This &lt;a href=&#34;https://www.youtube.com/watch?v=xUJbTnN3Dmw&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;video&lt;/a&gt; also explains blue-green deployments. This &lt;a href=&#34;/guides/kubernetes/harbor-gs/&#34;&gt;guide&lt;/a&gt; provides an example of deploying Contour in conjunction with Harbor, an open source registry for containers and Helm charts.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: Kubernetes Monitoring Overview</title>
      
      <link>/guides/kubernetes/observability-kubernetes-monitoring-overview/</link>
      <pubDate>Fri, 26 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/guides/kubernetes/observability-kubernetes-monitoring-overview/</guid>
      <description>

        
        &lt;p&gt;Observability is a key element of cloud native application architectures. Most modern applications are distributed in nature, with a collection of multiple modules that communicate with each other via APIs. Anytime a problem occurs you need to be able to see when and where failures happened. And you need to measure failures to establish a profile or baseline against which deviations from normal operation can be identified and addressed. As such, monitoring, feature-rich metrics, alerting tools, and data visualization frameworks are a key element of successful cloud native applications.&lt;/p&gt;
&lt;p&gt;This guide provides an overview of monitoring tools for Kubernetes environments.&lt;/p&gt;
&lt;h2 id=&#34;how-is-monitoring-apps-on-kubernetes-different&#34;&gt;How Is Monitoring Apps on Kubernetes Different?&lt;/h2&gt;
&lt;p&gt;Containerized systems such as Kubernetes present new monitoring challenges versus virtual-machine-based compute environments. These differences include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The ephemeral nature of containers&lt;/li&gt;
&lt;li&gt;An increased density of objects, services, and metrics within a given node&lt;/li&gt;
&lt;li&gt;A focus on services, rather than machines&lt;/li&gt;
&lt;li&gt;More diverse consumers of monitoring data&lt;/li&gt;
&lt;li&gt;Changes in the software development lifecycle&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As monolithic apps are refactored into microservices and orchestrated with Kubernetes, requirements for monitoring those apps change. To start, instrumentation to capture application data needs to be at a container level, at scale, across thousands of endpoints. Because Kubernetes workloads are ephemeral by default and can start or stop at any time, application monitoring must be dynamic and aware of Kubernetes labels and namespaces. A consistent set of rules or alerts must be applied to all pods, new and old.&lt;/p&gt;
&lt;p&gt;Observability should always be a consideration when you’re developing new apps or refactoring existing ones. Maintaining a common layer of baseline metrics that applies to all apps and infrastructure while incorporating custom metrics is extremely desirable. Adding a new metric based on user feedback should NOT trigger a major replumb of your monitoring stack.&lt;/p&gt;
&lt;h2 id=&#34;monitoring-resource-consumption-and-preventing-infiltration&#34;&gt;Monitoring Resource Consumption and Preventing Infiltration&lt;/h2&gt;
&lt;p&gt;How can you protect your Kubernetes system from hijackers and infiltrators? Here are some suggestions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Monitor cluster and network utilization&lt;/li&gt;
&lt;li&gt;Monitor for suspicious activity and analyze failed login and RBAC events&lt;/li&gt;
&lt;li&gt;Monitor configurations, such as dashboard access, for risks and vulnerabilities&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The NIST document, &lt;a href=&#34;https://nvlpubs.nist.gov/nistpubs/ir/2017/NIST.IR.8176.pdf&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Security Assurance Requirements for Linux Application Container Deployments&lt;/a&gt; sets forth security requirements and countermeasures to help meet the recommendations of the &lt;a href=&#34;https://csrc.nist.gov/publications/detail/sp/800-190/final&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;NIST Application Container Security Guide&lt;/a&gt; when containerized applications are deployed in production environments. According to NIST, you should log and monitor resource consumption of containers to ensure availability of critical resources.&lt;/p&gt;
&lt;h3 id=&#34;security-monitoring-and-auditing&#34;&gt;Security monitoring and auditing&lt;/h3&gt;
&lt;p&gt;The proper security monitoring for your cluster depends largely on the amount of time and staffing you have to respond to alerts and keep an eye on things. As a general rule, you shouldn&amp;rsquo;t spend time building security monitoring systems that you don&amp;rsquo;t have the time to maintain and tune. Start with the real-time (alert-based) and periodic (audit review) analyst or operator workflows you want to enable, and build the monitoring platform you need to enable those workflows.&lt;/p&gt;
&lt;h3 id=&#34;logging&#34;&gt;Logging&lt;/h3&gt;
&lt;p&gt;The bedrock of security monitoring is logging. You should generally capture application logs, host-level logs, Kubernetes API audit logs, and cloud-provider logs (if applicable). There are well-established patterns for implementing log aggregation on common cluster configurations.&lt;/p&gt;
&lt;p&gt;Centralized logging is an essential part of any enterprise Kubernetes deployment. Configuring and maintaining a real-time high-performance central repository for log collection can ease the day-to-day operations of tracking what went wrong and its impact. Effective central logging also helps development teams quickly observe application logs to characterize application performance. Security compliance and auditing often require a company to maintain digital trails of who did what and when. In most cases, a robust logging solution is the most efficient way to satisfy these requirements&lt;/p&gt;
&lt;p&gt;For security auditing purposes, consider streaming your logs to an external location with append-only access from within your cluster. For example, on AWS, you can create an S3 bucket in an isolated AWS account and give append-only access to your cluster log aggregator. This ensures your logs cannot be tampered with, even in the case of a total cluster compromise.&lt;/p&gt;
&lt;h5 id=&#34;log-aggregation&#34;&gt;Log Aggregation&lt;/h5&gt;
&lt;p&gt;An effective log aggregator must support the processing of events from thousands of endpoints, the ability to accommodate real-time queries, and a superior analytics engine to provide intelligent metrics to solve complex technical and business problems. You have the option to implement log aggregation using a number of popular open source or commercial logging analytics solutions, such as Elasticsearch, Fluentd, Kibana, or Splunk. Each solution has a set of strengths and weaknesses.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.fluentd.org&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Fluentd&lt;/a&gt; is an open-source data collector for unified logging. &lt;a href=&#34;https://fluentbit.io&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Fluent Bit&lt;/a&gt; is a lightweight data forwarder for Fluentd. Fluentd is used to create a unified logging layer to collect and process data. Fluent Bit is for forwarding data from the edge to Fluentd aggregators. Fluentd and Fluent Bit can collect logging data and push it to an output destination, such as &lt;a href=&#34;https://www.elastic.co&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Elasticsearch&lt;/a&gt;, which is a distributed search and analytics engine that lets data engineers query unstructured, structured, and time-series data.&lt;/p&gt;
&lt;h3 id=&#34;network-monitoring&#34;&gt;Network monitoring&lt;/h3&gt;
&lt;p&gt;Network-based security monitoring tools, such as a network intrusion detection system (IDS) and web application firewalls, may work nearly out of the box, but making them work well takes some effort. The biggest hurdle is that many tools expect IP addresses to be a useful context for events. To integrate these tools with Kubernetes, consider enriching the collected events with Kubernetes &lt;code&gt;namespace&lt;/code&gt;, &lt;code&gt;pod name&lt;/code&gt;, and &lt;code&gt;pod label&lt;/code&gt; metadata. This adds valuable context to the event that you can use for alerting or manual review and can make these traditional tools even more powerful in a Kubernetes cluster than in a traditional environment. Some monitoring tools can collect Kubernetes metadata, but you can also write custom event enrichment code to add this kind of metadata integration to those that don&amp;rsquo;t.&lt;/p&gt;
&lt;h3 id=&#34;host-event-monitoring&#34;&gt;Host event monitoring&lt;/h3&gt;
&lt;p&gt;It&amp;rsquo;s also possible to run a host-based IDS, such as file integrity monitoring and Linux system call logging (for example, auditd), directly with Kubernetes, but the results are hard to manage because the workload running on any particular node varies from hour to hour as applications deploy and Kubernetes orchestrates pods.&lt;/p&gt;
&lt;p&gt;To make sense of host-based events, you&amp;rsquo;ll again want to consider extending your existing tools to include Kubernetes pod or container metadata in the context of captured events. Systems such as &lt;a href=&#34;https://sysdig.com/opensource/falco/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Sysdig Falco&lt;/a&gt; include this context out of the box.&lt;/p&gt;
&lt;h3 id=&#34;prometheus-and-grafana&#34;&gt;Prometheus and Grafana&lt;/h3&gt;
&lt;p&gt;The open-source community is converging on &lt;a href=&#34;https://prometheus.io&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Prometheus&lt;/a&gt; as a preferred solution for Kubernetes monitoring. The ability to address evolving requirements of Kubernetes while including a rich set of language-specific client libraries gives Prometheus an advantage.&lt;/p&gt;
&lt;p&gt;Prometheus excels at monitoring multidimensional data, including time-series data, and it is hosted by the Cloud Native Computing Foundation, of which VMware is a member. &lt;a href=&#34;https://grafana.com&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Grafana&lt;/a&gt; is an open-source metrics dashboard commonly used with Prometheus to display data.&lt;/p&gt;
&lt;h3 id=&#34;wavefront&#34;&gt;Wavefront&lt;/h3&gt;
&lt;p&gt;Kubernetes can be integrated with Wavefront (VMware Tanzu Observability) to efficiently monitor containers at enterprise scale. Wavefront delivers monitoring and analytics throughout a cloud native stack for always-on metrics as a service.Wavefront gives developers and DevOps real-time visibility into the operations and performance of containerized workloads and Kubernetes clusters.&lt;/p&gt;
&lt;h2 id=&#34;keep-learning&#34;&gt;Keep Learning&lt;/h2&gt;
&lt;p&gt;KubeAcademy offers a course on &lt;a href=&#34;https://kube.academy/courses/introduction-to-observability&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Kubernetes observability&lt;/a&gt; where you can learn more about many of the topics mentioned above. For a practical guide on how to get started with Prometheus and Grafana, be sure to read &lt;a href=&#34;/guides/kubernetes/prometheus-grafana-p1/&#34;&gt;Prometheus and Grafana: Gathering Metrics from Kubernetes&lt;/a&gt;. Spring Boot users will also want to check out &lt;a href=&#34;/guides/spring/spring-prometheus/&#34;&gt;Prometheus and Grafana: Gathering Metrics from Spring Boot on Kubernetes&lt;/a&gt; to learn how to gather metrics from Spring applications. The guides &lt;a href=&#34;/guides/microservices/distributed-tracing&#34;&gt;Implementing Distributed Tracing&lt;/a&gt; and &lt;a href=&#34;/guides/spring/spring-zipkin/&#34;&gt;Getting Started with Zipkin and Spring Boot&lt;/a&gt; can help you improve observability for microservices applications.&lt;/p&gt;
&lt;p&gt;If you’re considering Wavefront, be sure and read &lt;a href=&#34;/guides/kubernetes/monitoring-at-scale-wavefront&#34;&gt;Monitoring Containers at Scale with Wavefront&lt;/a&gt; and &lt;a href=&#34;/guides/spring/spring-wavefront-gs/&#34;&gt;Wavefront for Spring Boot: Getting Started&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: What is Microservices Architecture?</title>
      
      <link>/guides/microservices/what-is-microservices-architecture/</link>
      <pubDate>Fri, 26 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/guides/microservices/what-is-microservices-architecture/</guid>
      <description>

        
        &lt;p&gt;Microservices are a modern architectural pattern for building an application. A microservices architecture breaks up the functions of an application into a set of small, discrete, decentralized, goal-oriented processes, each of which can be independently developed, tested, deployed, replaced, and scaled.&lt;/p&gt;
&lt;h2 id=&#34;the-value-of-microservices&#34;&gt;The Value of Microservices&lt;/h2&gt;
&lt;p&gt;Because microservices are small, discrete application components linked together through lightweight, well-defined APIs, they can be linked together in various ways to create modern applications with independently scalable modules. A major advantage of this application architecture is that discrete components can be updated independently, which enables developers to efficiently deliver new features and fix issues with existing ones. The business value of this approach is clear; you can deliver new digital applications and services with greater speed and efficiency.&lt;/p&gt;
&lt;h3 id=&#34;simple-microservices-example&#34;&gt;Simple Microservices Example&lt;/h3&gt;
&lt;p&gt;Consider a three-tier app that has a front end, a middle app tier, and a database. Traditionally, this app might be deployed in three different virtual machines. In a microservices architecture, the same app is broken into multiple components. For example, the front end could be broken into separate &lt;code&gt;services&lt;/code&gt; that individually handle &lt;em&gt;login, catalog, services, feedback,&lt;/em&gt; etc.&lt;/p&gt;
&lt;p&gt;The middle tier might be broken into &lt;code&gt;services&lt;/code&gt; that handle &lt;em&gt;authorization, database connections, metering,&lt;/em&gt; etc.&lt;/p&gt;
&lt;p&gt;With this modular approach, you can push an update to the &lt;em&gt;Catalog Page&lt;/em&gt;  without having to touch or update any of the login or database functionality.&lt;/p&gt;
&lt;h3 id=&#34;key-characteristics-of-microservices&#34;&gt;Key Characteristics of Microservices&lt;/h3&gt;
&lt;p&gt;Here are some of the key technical aspects of microservices:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Strong and clear interfaces&lt;/strong&gt; – Tight coupling between services should be avoided. Documented and versioned interfaces provide a certain degree of freedom for both the consumers and producers of services.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Independently deployed and managed&lt;/strong&gt; – It should be possible for a single microservice to be updated without synchronizing with all the other services. It is also desirable to be able to roll back a version of a microservice easily. This means the binaries that are deployed must be forward and backward compatible both in terms of API and any data schemas.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Built-in resilience&lt;/strong&gt; – Microservices should be built and tested to be independently resilient. If one microservice requires a response from another, it  should strive to continue working and do something reasonable in the event the other microservice is down or misbehaving. Similarly, every microservice should have defenses with respect to unanticipated load and bad inputs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Dividing a new or existing application into the right set of microservices can be a tricky thing to get right. Natural boundaries such as languages, async queues, and scaling requirements can serve as useful dividers.&lt;/p&gt;
&lt;h2 id=&#34;keep-learning&#34;&gt;Keep Learning&lt;/h2&gt;
&lt;p&gt;Microservices, coupled with containers, are becoming the architectural pattern of choice for developing new applications. The architecture breaks up the functions of an application into a set of small, discrete, decentralized, goal-oriented processes, each of which can be independently developed, tested, deployed, replaced, and scaled.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s a video by a former Netflix architect that gives you the lowdown on using a microservices architecture for building next-generation applications.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/4ClmJxVz1SM&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;The guide &lt;a href=&#34;/guides/microservices/deconstructing-the-monolith/&#34;&gt;Deconstructing the Monolith&lt;/a&gt; discusses how to break a monolithic application down into microservices. After you are comfortable with the concepts, our &lt;a href=&#34;/workshops/lab-microservice/&#34;&gt;microservices workshop&lt;/a&gt; can help you get started building microservices using &lt;a href=&#34;/topics/spring&#34;&gt;Spring&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: What is Kubernetes?</title>
      
      <link>/guides/kubernetes/what-is-kubernetes/</link>
      <pubDate>Fri, 26 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/guides/kubernetes/what-is-kubernetes/</guid>
      <description>

        
        &lt;p&gt;Containers accelerate development pipelines by removing the need to build, test and validate application code across multiple operating systems. They also help simplify application operations by being portable across multiple hosts and cloud platforms. However, an application running in a container still needs management. For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What happens if a running container has a problem or dies?&lt;/li&gt;
&lt;li&gt;How do you expose containers running on host to external/ingress traffic?&lt;/li&gt;
&lt;li&gt;How do you determine AND scale the number of containers when application workloads increase?&lt;/li&gt;
&lt;li&gt;How can you isolate two containers on the same host such that they cannot talk to each other?&lt;/li&gt;
&lt;li&gt;How do you migrate containers from one host to another for host maintenance?&lt;/li&gt;
&lt;li&gt;How can containers share common config data?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A microservices application may be spread across multiple services backed by multiple containers, increasing complexity. A platform that can orchestrate, manage and define dependencies and configs for containerized applications becomes necessary for production systems.&lt;/p&gt;
&lt;h2 id=&#34;introduction-to-kubernetes&#34;&gt;Introduction to Kubernetes&lt;/h2&gt;
&lt;p&gt;Kubernetes helps orchestrate containerized applications to run on a cluster of hosts. It&amp;rsquo;s a system that automates the deployment and management of containerized applications on a given cloud platform or on-premises infrastructure. Kubernetes manages workload distribution for containerized applications across a cluster of hosts and will dynamically roll out the container networking, routing and ingress needed for applications running in containers. It can also allocate storage and persistent volumes to running containers, provides a way to inject global config variables, implements auto-scaling, and maintains the desired state for applications.&lt;/p&gt;
&lt;p&gt;The Kubernetes API lets users define the desired end state of their applications via logical constructs like deployments, replicasets, config-maps, services etc. Kubernetes is highly extensible and portable, meaning it can run in a wide range of environments and can be used in conjunction with other technologies. There is a rapidly expanding Kubernetes ecosystem with projects that provide a wide range of different functionality.&lt;/p&gt;
&lt;p&gt;The Cloud Native Computing Foundation (CNCF) maintains an &lt;a href=&#34;https://landscape.cncf.io&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Interactive Landscape&lt;/a&gt; to keep track of everything going on. VMware Tanzu is an active sponsor and contributor for many &lt;a href=&#34;https://tanzu.vmware.com/open-source&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;open source projects&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;solving-container-challenges&#34;&gt;Solving Container Challenges&lt;/h2&gt;
&lt;p&gt;Kubernetes solves these challenges by automating the deployment and management of containerized applications. It manages everything necessary to optimize the use of computing resources and scales containers on demand.&lt;/p&gt;
&lt;p&gt;Kubernetes coordinates clusters of nodes to provide integration, orchestration, scaling, fault tolerance, and communications for running containers. It operates using the concept of pods, which are scheduling units that can include one or more containers and are distributed among nodes to provide high availability.&lt;/p&gt;
&lt;p&gt;In addition to scheduling deployment and automating the management of containerized applications, a key benefit of Kubernetes is that it maintains the desired state of an application as specified by an administrator. It does this using a declarative text file (YAML) that defines the desired state for a containerized application. If a container/pod dies it is automatically restarted, providing a built in level of resilience.&lt;/p&gt;
&lt;p&gt;Kubernetes uses various resource constructs to work with containers. These resources help define simple tasks such as how many instances of a container to run at all times, how to trigger auto-scaling, how to route ingress traffic to a set of container images, or how to define a &lt;a href=&#34;https://tanzu.vmware.com/content/blog/exploring-kube-apiserver-load-balancers-for-on-premises-kubernetes-clusters&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;load balancer&lt;/a&gt; to distribute traffic between multiple container images.&lt;/p&gt;
&lt;h2 id=&#34;keep-learning&#34;&gt;Keep Learning&lt;/h2&gt;
&lt;p&gt;If you haven’t already, check out our &lt;a href=&#34;/guides/containers/what-are-containers&#34;&gt;introduction to containers&lt;/a&gt;, and refer to the guides and resources on our &lt;a href=&#34;/topics/kubernetes/&#34;&gt;Kubernetes topic page&lt;/a&gt; to go deeper. The &lt;a href=&#34;/workshops/lab-k8s-fundamentals/&#34;&gt;Kubernetes Fundamentals workshop&lt;/a&gt; provides a quick, hands-on introduction, as well as the &lt;a href=&#34;https://kube.academy/courses/getting-started&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Getting Started with Kubernetes&lt;/a&gt; course on &lt;a href=&#34;https://kube.academy/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;KubeAcademy&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;After you feel comfortable with Kubernetes concepts, you can also learn about combining the Docker container platform with Kubernetes to develop &lt;a href=&#34;/topics/microservices&#34;&gt;microservices&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      
      <title>Blog: Using Knative Eventing for Better Observability</title>
      
      <link>/blog/using-knative-eventing-for-better-observability/</link>
      <pubDate>Mon, 22 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/using-knative-eventing-for-better-observability/</guid>
      <description>

        
        &lt;p&gt;If you’re using one of the great observability tools out there, you probably already mark your data with important events that may affect it—deployments, configuration changes, code commits, and more. But what about changes Kubernetes makes on its own, like autoscaling events?&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://knative.dev&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Knative&lt;/a&gt; is a Kubernetes-based platform used to deploy and manage serverless workloads. It has two components: serving and eventing, both of which can be deployed independently. In this post, we’re going to focus on eventing here, which can automatically mark events in your data or trigger other events based on your needs.&lt;/p&gt;
&lt;h3 id=&#34;knative-eventing&#34;&gt;Knative Eventing&lt;/h3&gt;
&lt;p&gt;The eventing component of Knative is a loosely coupled system of event producers and consumers that allows for multiple modes of usage and event transformation.&lt;/p&gt;
&lt;p&gt;Among the other components in this system are the &lt;a href=&#34;https://knative.dev/docs/eventing/broker/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;broker&lt;/a&gt;, which routes the events over &lt;a href=&#34;https://knative.dev/docs/eventing/channels/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;channels&lt;/a&gt;, and &lt;a href=&#34;https://knative.dev/docs/eventing/triggers/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;triggers&lt;/a&gt;, which subscribe specific consumers to events. For our example, we’re going to keep things very simple, with a single broker using a single&lt;a href=&#34;https://github.com/knative/eventing/blob/release-0.20/config/channels/in-memory-channel/README.md&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt; in-memory channel, &lt;/a&gt;which itself is not to be used in production.&lt;/p&gt;
&lt;h3 id=&#34;kubernetes-events&#34;&gt;Kubernetes Events&lt;/h3&gt;
&lt;p&gt;If we want Kubernetes events as a source, we can use the &lt;a href=&#34;https://knative.dev/docs/eventing/samples/kubernetes-event-source/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;API server source&lt;/a&gt; as an event producer. This will publish any changes seen by the API server to the channel we’re using, and we can consume that event with a small golang application and forward to the observability tool of our choice.&lt;/p&gt;
&lt;p&gt;In this case, we’re going to specifically watch for horizontal pod autoscaler (HPA) messages. Anytime the HPA scales our example app up or down, we’ll send a API call to mark an event.&lt;/p&gt;
&lt;h3 id=&#34;eventing-in-action&#34;&gt;Eventing in Action&lt;/h3&gt;
&lt;p&gt;We’re going to use Kind to deploy a Knative eventing setup and then deploy a sample application with an autoscaler so we can see when it scales up or down. To do this, we will need to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Have &lt;a href=&#34;https://kind.sigs.k8s.io&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Kind&lt;/a&gt; installed&lt;/li&gt;
&lt;li&gt;Clone this repo &lt;a href=&#34;https://github.com/tybritten/hpa-sender&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;https://github.com/tybritten/hpa-sender&lt;/a&gt; \&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The first thing we need to do is create the Kind cluster. If you’re not comfortable with curling to bash, you can download it first and inspect it:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;curl -sL https://raw.githubusercontent.com/csantanapr/knative-kind/master/01-kind.sh &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; bash
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Next, we need to install all the Knative eventing components, starting with the CRD and core components:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl apply -f https://github.com/knative/eventing/releases/download/v0.19.0/eventing-crds.yaml
kubectl apply -f https://github.com/knative/eventing/releases/download/v0.19.0/eventing-core.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;As mentioned earlier, we need a channel; we’re going to use the simple in-memory channel and corresponding broker:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl apply -f https://github.com/knative/eventing/releases/download/v0.19.0/in-memory-channel.yaml
kubectl apply -f https://github.com/knative/eventing/releases/download/v0.19.0/mt-channel-broker.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Lastly, we need to install the metrics server (and apply a patch) for the HPA:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.3.6/components.yaml
kubectl patch deployment metrics-server -n kube-system -p &lt;span class=&#34;s1&#34;&gt;&amp;#39;{&amp;#34;spec&amp;#34;:{&amp;#34;template&amp;#34;:{&amp;#34;spec&amp;#34;:{&amp;#34;containers&amp;#34;:[{&amp;#34;name&amp;#34;:&amp;#34;metrics-server&amp;#34;,&amp;#34;args&amp;#34;:[&amp;#34;--cert-dir=/tmp&amp;#34;, &amp;#34;--secure-port=4443&amp;#34;, &amp;#34;--kubelet-insecure-tls&amp;#34;,&amp;#34;--kubelet-preferred-address-types=InternalIP&amp;#34;]}]}}}}&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Let’s check to make sure everything is up and running:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl get all -n knative-eventing
NAME                                        READY   STATUS    RESTARTS   AGE
pod/eventing-controller-66c877b879-vx6dp    1/1     Running   &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;          7m15s
pod/eventing-webhook-644c5c7667-pr5x7       1/1     Running   &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;          7m15s
pod/imc-controller-587f98f97d-l6s9r         1/1     Running   &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;          6m56s
pod/imc-dispatcher-6db95d7857-n6249         1/1     Running   &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;          6m56s
pod/mt-broker-controller-76b65f7c96-tpktv   1/1     Running   &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;          6m51s
pod/mt-broker-filter-6bd64f8c65-wd7hm       1/1     Running   &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;          6m51s
pod/mt-broker-ingress-7d8595d747-77n77      1/1     Running   &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;          6m51s

NAME                       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;S&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;           AGE
service/broker-filter      ClusterIP   10.96.126.148   &amp;lt;none&amp;gt;        80/TCP,9092/TCP   6m51s
service/broker-ingress     ClusterIP   10.96.98.183    &amp;lt;none&amp;gt;        80/TCP,9092/TCP   6m51s
service/eventing-webhook   ClusterIP   10.96.211.233   &amp;lt;none&amp;gt;        443/TCP           7m15s
service/imc-dispatcher     ClusterIP   10.96.72.204    &amp;lt;none&amp;gt;        80/TCP            6m56s

NAME                                    READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/eventing-controller     1/1     &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;            &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;           7m15s
deployment.apps/eventing-webhook        1/1     &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;            &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;           7m15s
deployment.apps/imc-controller          1/1     &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;            &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;           6m56s
deployment.apps/imc-dispatcher          1/1     &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;            &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;           6m56s
deployment.apps/mt-broker-controller    1/1     &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;            &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;           6m51s
deployment.apps/mt-broker-filter        1/1     &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;            &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;           6m51s
deployment.apps/mt-broker-ingress       1/1     &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;            &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;           6m51s
deployment.apps/pingsource-mt-adapter   0/0     &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;            &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;           7m15s

NAME                                               DESIRED   CURRENT   READY   AGE
replicaset.apps/eventing-controller-66c877b879     &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;         &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;         &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;       7m15s
replicaset.apps/eventing-webhook-644c5c7667        &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;         &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;         &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;       7m15s
replicaset.apps/imc-controller-587f98f97d          &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;         &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;         &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;       6m56s
replicaset.apps/imc-dispatcher-6db95d7857          &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;         &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;         &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;       6m56s
replicaset.apps/mt-broker-controller-76b65f7c96    &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;         &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;         &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;       6m51s
replicaset.apps/mt-broker-filter-6bd64f8c65        &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;         &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;         &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;       6m51s
replicaset.apps/mt-broker-ingress-7d8595d747       &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;         &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;         &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;       6m51s
replicaset.apps/pingsource-mt-adapter-5d85796c74   &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;         &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;         &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;       7m15s

NAME                                                     REFERENCE                      TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
horizontalpodautoscaler.autoscaling/broker-filter-hpa    Deployment/mt-broker-filter    2%/70%    &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;         &lt;span class=&#34;m&#34;&gt;10&lt;/span&gt;        &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;          6m51s
horizontalpodautoscaler.autoscaling/broker-ingress-hpa   Deployment/mt-broker-ingress   2%/70%    &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;         &lt;span class=&#34;m&#34;&gt;10&lt;/span&gt;        &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;          6m51s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now we want to add the API event sender. First we need a broker in the default namespace where the application will be:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;eventing.knative.dev/v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;broker&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;default&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;namespace&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;knative-eventing&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We then need a service account (with a cluster role and role binding) for the API source to use:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nn&#34;&gt;---&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;ServiceAccount&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;events-sa&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;namespace&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;default&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;---&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;rbac.authorization.k8s.io/v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;ClusterRole&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;event-watcher&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;rules&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;apiGroups&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;- &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;resources&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;events&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;verbs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;get&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;watch&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;---&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;rbac.authorization.k8s.io/v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;ClusterRoleBinding&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;k8s-ra-event-watcher&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;roleRef&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;apiGroup&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;rbac.authorization.k8s.io&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;ClusterRole&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;event-watcher&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;subjects&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;ServiceAccount&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;events-sa&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;namespace&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;default&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now we need our API server source:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;sources.knative.dev/v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;ApiServerSource&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;testevents&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;namespace&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;default&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;serviceAccountName&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;events-sa&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;mode&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Resource&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;resources&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Event&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;sink&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;ref&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;eventing.knative.dev/v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Broker&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;default&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;namespace&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;knative-eventing&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can apply these all together from a file in the &lt;code&gt;kind-example&lt;/code&gt; folder in the repo:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl apply -f k8s-events.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We now have an API source and a broker reader for an event consumer:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl get all
NAME                                                                  READY   STATUS    RESTARTS   AGE
pod/apiserversource-testevents-bbeb355d-72a6-4e81-b0a8-02d9b0dtq8gn   1/1     Running   &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;          6s

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;S&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;   AGE
service/kubernetes   ClusterIP   10.96.0.1    &amp;lt;none&amp;gt;        443/TCP   42m

NAME                                                                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/apiserversource-testevents-bbeb355d-72a6-4e81-b0a8-02d9b0d31525   1/1     &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;            &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;           6s

NAME                                                                                        DESIRED   CURRENT   READY   AGE
replicaset.apps/apiserversource-testevents-bbeb355d-72a6-4e81-b0a8-02d9b0d31525-7685c896c   &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;         &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;         &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;       6s

NAME                                                              TYPE                                    SOURCE                  SCHEMA   BROKER    DESCRIPTION   READY   REASON
eventtype.eventing.knative.dev/2c7c2c1b3399bde6269acf3b7fe28c3a   dev.knative.apiserver.resource.add      https://10.96.0.1:443            default                 False   BrokerDoesNotExist
eventtype.eventing.knative.dev/5c1186d11f693b2c331a9c31246588e0   dev.knative.apiserver.resource.delete   https://10.96.0.1:443            default                 False   BrokerDoesNotExist
eventtype.eventing.knative.dev/b6426fa883a42e3e23ace1cebabfdd5e   dev.knative.apiserver.resource.update   https://10.96.0.1:443            default                 False   BrokerDoesNotExist

NAME                                             SINK                                                                                AGE   READY   REASON
apiserversource.sources.knative.dev/testevents   http://broker-ingress.knative-eventing.svc.cluster.local/knative-eventing/default   7s    True
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can now use any number of consumers, the most simple being the Knative event-display container (&lt;code&gt;gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For this example, I’ve created and published a container using the code in the repo. We’re going to deploy it along with another role that gives it access to the horizontal pod autoscalers, as well as a trigger to send the events to the container:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;ServiceAccount&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;hpa-sender&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;namespace&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;knative-eventing&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;---&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;rbac.authorization.k8s.io/v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;ClusterRole&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;hpa-sender&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;rules&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;apiGroups&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;autoscaling&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# &amp;#34;&amp;#34; indicates the core API group&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;resources&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;secrets&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;horizontalpodautoscalers&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;verbs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;get&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;watch&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;list&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;---&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;rbac.authorization.k8s.io/v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;ClusterRoleBinding&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;hpa-sender&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;roleRef&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;apiGroup&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;rbac.authorization.k8s.io&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;ClusterRole&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;hpa-sender&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;subjects&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;ServiceAccount&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;hpa-sender&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;namespace&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;knative-eventing&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;---&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;apps/v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Deployment&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;hpa-sender&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;namespace&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;knative-eventing&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;labels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;app&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;hpa-sender&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;replicas&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;selector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;matchLabels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;app&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;hpa-sender&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;template&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;labels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;app&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;hpa-sender&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;serviceAccountName&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;hpa-sender&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;containers&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;hpa-sender&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;vmtyler/hpa-sender&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;ports&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;containerPort&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;8080&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;---&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Service&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;hpa-sender&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;namespace&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;knative-eventing&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;selector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;app&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;hpa-sender&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;ports&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;protocol&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;TCP&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;port&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;80&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;targetPort&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;8080&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;---&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;eventing.knative.dev/v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Trigger&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;hpa-sender-trigger&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;namespace&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;knative-eventing&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;broker&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;default&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;subscriber&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;uri&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;http://hpa-sender.knative-eventing.svc.cluster.local&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can apply these all together from a file:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl apply -f hpa-sender-deployment.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;hpa-sender-configuration&#34;&gt;HPA-Sender Configuration&lt;/h4&gt;
&lt;p&gt;The way the hpa-sender container works is pretty simple; it:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Watches the API server events related to HPAs&lt;/li&gt;
&lt;li&gt;Retrieves the specific HPA referenced in the event from the API server&lt;/li&gt;
&lt;li&gt;Checks if the HPA has an annotation for hpa-sender with a secret location as namespace/secretname&lt;/li&gt;
&lt;li&gt;Retrieves the secret, which includes configuration for where to send the event, if it has the annotation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So to use this app, we need a secret with the configuration necessary for our event destination, and we have to annotate our HPA.&lt;/p&gt;
&lt;h4 id=&#34;sample-app-and-hpa-secret&#34;&gt;Sample App and HPA Secret&lt;/h4&gt;
&lt;p&gt;The first thing we need to do is create the secret with the necessary configuration. (There are some sample ones in the repo.) We’re going to use &lt;a href=&#34;https://tanzu.vmware.com/observability&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;VMware Tanzu Observability by Wavefront&lt;/a&gt; as our destination:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Secret&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;php-apache-hpe&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;namespace&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;default&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;type&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Opaque&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;stringData&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;url&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;https://WAVEFRONTHOSTNAME/api/v2/event&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;headers&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;    {
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;      &amp;#34;Authorization&amp;#34;: &amp;#34;Bearer &amp;lt;wavefront_api_token&amp;gt;&amp;#34;
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;    }&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;body&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;    {
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;      &amp;#34;name&amp;#34;: &amp;#34;HPA Scaling&amp;#34;,
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;      &amp;#34;annotations&amp;#34;: {
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;        &amp;#34;severity&amp;#34;: &amp;#34;info&amp;#34;,
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;        &amp;#34;type&amp;#34;: &amp;#34;scaling&amp;#34;,
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;        &amp;#34;details&amp;#34;: &amp;#34;_message_&amp;#34;
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;      },
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;      &amp;#34;tags&amp;#34; : [
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;        &amp;#34;Scale&amp;#34;
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;      ],
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;      &amp;#34;startTime&amp;#34;: 0,
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;      &amp;#34;endTime&amp;#34;: 0
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;    }&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We’ll put our WaveFront host name in there along with our API token. In the body, you’ll see a &lt;em&gt;message&lt;/em&gt;, which is what hpa-sender will replace with the actual HPA message.&lt;/p&gt;
&lt;p&gt;For our sample app, we’ll use a basic php-apache image:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;apps/v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Deployment&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;php-apache&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;namespace&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;default&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;selector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;matchLabels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;run&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;php-apache&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;replicas&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;template&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;labels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;run&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;php-apache&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;containers&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;php-apache&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;k8s.gcr.io/hpa-example&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;ports&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;containerPort&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;80&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;resources&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;limits&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;              &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;cpu&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;500m&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;requests&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;              &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;cpu&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;200m&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;---&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Service&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;php-apache&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;namespace&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;default&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;labels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;run&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;php-apache&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;ports&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;port&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;80&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;selector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;run&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;php-apache&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;---&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;autoscaling/v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;HorizontalPodAutoscaler&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;annotations&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;hpa-event&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;default/php-apache-hpe&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;php-apache&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;namespace&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;default&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;maxReplicas&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;minReplicas&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;scaleTargetRef&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;apps/v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Deployment&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;php-apache&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;targetCPUUtilizationPercentage&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;50&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;status&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;currentCPUUtilizationPercentage&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;currentReplicas&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;desiredReplicas&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You will see on the HPA that it has an annotation of &lt;code&gt;hpa-event: default/php-apache-hpe &lt;/code&gt;enabling the hpa-sender and pointing to the secret with the configuration.&lt;/p&gt;
&lt;p&gt;We can apply these all together from a single file:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl apply -f hpa-app.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To check on the app and the HPA, we can run:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ kubectl get pods
NAME                                                              READY   STATUS    RESTARTS   AGE
apiserversource-testevents-bbeb355d-72a6-4e81-b0a8-02d9b0dtq8gn   1/1     Running   &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;          74m
php-apache-d4cf67d68-crmsr

$ kubectl get hpa
NAME         REFERENCE               TARGETS         MINPODS   MAXPODS   REPLICAS   AGE
php-apache   Deployment/php-apache   &amp;lt;unknown&amp;gt;/50%   &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;         &lt;span class=&#34;m&#34;&gt;5&lt;/span&gt;         &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;          47s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now we can cause the load to go up or down by following &lt;a href=&#34;https://unofficial-kubernetes.readthedocs.io/en/latest/tasks/run-application/horizontal-pod-autoscale-walkthrough/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;these instructions&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ kubectl run -i --tty load-generator --image&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;busybox /bin/sh

Hit enter &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;command&lt;/span&gt; prompt

$ &lt;span class=&#34;k&#34;&gt;while&lt;/span&gt; true&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;do&lt;/span&gt; wget -q -O- http://php-apache.default.svc.cluster.local&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;done&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This will trigger events to our URL:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/blogs/using-knative-eventing-for-better-observability/wavefront_event_knative.png&#34; alt=&#34;WaveFront Event Screenshot&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Now we have events marking these scaling occurrences on our application’s performance charts.&lt;/p&gt;
&lt;p&gt;This is just one use case for Knative eventing with the Kubernetes API Source, but you can imagine the possibilities enabled by this very customizable, pluggable framework. We didn’t even need to deploy the Knative serving component to do it! If you want to see me go through these steps in more detail, and also walk through how the HPA-Sender works, watch this episode of Tanzu.TV:

&lt;div class=&#34;youtube-video-shortcode&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/9lBaKKe-59E&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;

      </description>
    </item>
    
    <item>
      
      <title>Blog: The Hate for YAML: The Hammer or the Nail?</title>
      
      <link>/blog/the-hate-for-yaml-the-hammer-or-the-nail/</link>
      <pubDate>Tue, 19 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/the-hate-for-yaml-the-hammer-or-the-nail/</guid>
      <description>

        
        &lt;p&gt;Those four letters that strike dread in the hearts of every Kubernetes user. That short acronym that pierces like a knife in the dark. The aura of terror that follows it, enveloping everyone and everything as its reach seems to grow to the ends of time itself.&lt;/p&gt;
&lt;p&gt;YAML.&lt;/p&gt;
&lt;p&gt;Alright, maybe that’s a bit dramatic, but there’s no doubt that YAML has developed a reputation for being a pain, namely due to the combination of semantics and empty space that gets deserialized to typed values by a library that you hope follows the same logic as others. This has fostered frustration among developers and operators no matter what the context. But is the issue as simple as “YAML is a pain”? Or is it a bit more nuanced than that?&lt;/p&gt;
&lt;p&gt;Last year, at &lt;a href=&#34;https://www.softwarecircus.io/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Software Circus: Nightmares on Cloud Street&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/jbeda&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Joe Beda&lt;/a&gt; gave a talk on this very subject titled &lt;a href=&#34;https://www.youtube.com/watch?v=8PpgqEqkQWA&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;I’m Sorry About The YAML&lt;/a&gt;. In it, he explores the factors that contribute to YAML’s reputation, or the so-called “two wolves” inside the hatred of YAML—the frustration with YAML itself and the problem that it’s being used to solve—and how they contribute to each other.&lt;/p&gt;
&lt;h2 id=&#34;the-problem-with-the-hammer&#34;&gt;The Problem with the Hammer&lt;/h2&gt;
&lt;p&gt;Beda starts by talking about YAML itself, both writing it and reading it. Of course, the first thing that comes to mind is the meaningful use of blank space. Opinions run high in this discussion, as it’s a situation with which Python developers are intimately familiar. Indeed, there’s a very real scenario where missing a couple of spaces can drastically change your data while still being valid YAML. Consider the following two examples:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;foo&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;bar&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;baz&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;foo&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;bar&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;baz&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;It’s easy to spot the difference in such a small example, but potentially even easier to overlook two missing spaces when this YAML is hundreds or thousands of lines long. To be fair, this issue isn&amp;rsquo;t completely unique to YAML. In JSON for example, you could easily place a key in a wrong node, or misspell a key in an array of objects. Both of these can of course be mitigated with tools such as schema validators.&lt;/p&gt;
&lt;p&gt;There is some behavior that isn’t as obvious, though. Take the following YAML, for example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;country_codes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;united_states&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;us&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;ireland&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;ie&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;norway&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;no&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Let’s try loading it using the standard library in Ruby, which will parse the YAML and then print it out as JSON.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-ruby&#34; data-lang=&#34;ruby&#34;&gt;&lt;span class=&#34;nb&#34;&gt;require&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;yaml&amp;#39;&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;doc&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;lt;&amp;lt;-ENDYAML
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&lt;/span&gt;&lt;span class=&#34;ss&#34;&gt;country_codes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
  &lt;span class=&#34;ss&#34;&gt;united_states&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;us&lt;/span&gt;
  &lt;span class=&#34;ss&#34;&gt;ireland&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ie&lt;/span&gt;
  &lt;span class=&#34;ss&#34;&gt;norway&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;no&lt;/span&gt;
&lt;span class=&#34;no&#34;&gt;ENDYAML&lt;/span&gt;

&lt;span class=&#34;nb&#34;&gt;puts&lt;/span&gt; &lt;span class=&#34;no&#34;&gt;YAML&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;load&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;doc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;o&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;country_codes&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&amp;gt;&lt;span class=&#34;o&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;united_states&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&amp;gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;us&amp;#34;&lt;/span&gt;, &lt;span class=&#34;s2&#34;&gt;&amp;#34;ireland&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&amp;gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;ie&amp;#34;&lt;/span&gt;, &lt;span class=&#34;s2&#34;&gt;&amp;#34;norway&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&amp;gt;false&lt;span class=&#34;o&#34;&gt;}}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This is what Beda calls the “Norway Problem.” YAML’s specification doesn’t require quoting strings, which can cause some unintended behavior from any given library in your language of choice. In the &lt;a href=&#34;https://yaml.org/type/bool.html&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;official YAML specification&lt;/a&gt;, “no” is a valid value to represent a boolean that is “false”. And it isn’t limited to just this niche case. Consider the following:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;version&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1.2.0&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;o&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;version&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&amp;gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;1.2.0&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;It makes sense that this is being parsed as a string. After all, what else could it be interpreted as? Well, what if we represent the same thing, but in a slightly different way?&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;version&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1.2&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;o&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;version&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&amp;gt;1.2&lt;span class=&#34;o&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;With no schema validation or context, the YAML specification clearly states that this should be &lt;a href=&#34;https://yaml.org/type/float.html&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;interpreted as a float&lt;/a&gt;. You and I probably even agree that both of these scenarios make sense, but this ambiguity leaves a non-zero margin of error, especially when you put it in the context of something as complex as Kubernetes.&lt;/p&gt;
&lt;h2 id=&#34;yamls-attack-surface&#34;&gt;YAML’s Attack Surface&lt;/h2&gt;
&lt;p&gt;Things can also get tricky when you start deserializing anything more complex than a 1:1 representation of data and logic.&lt;/p&gt;
&lt;p&gt;In YAML, you can reference one structure from another, which is very, very handy when you know that values from one portion of a YAML document will need to be referenced later. For example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;cp&#34;&gt;&amp;amp;speaker&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Joe Beda&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;presentation&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Nightmare on Cloud Street&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;speaker&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;cp&#34;&gt;*speaker&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This YAML defined the anchor “speaker” with the value “Joe Beda”, which is then referenced later in the line &lt;code&gt; speaker: *speaker&lt;/code&gt;. When this YAML is expanded, this becomes:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Joe Beda&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;presentation&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; 
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Nightmare on Cloud Street&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;speaker&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Joe Beda&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;But as they say, “With great power comes a great opportunity for someone with malicious intent to exploit.” Take a look at the following:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;cp&#34;&gt;&amp;amp;a&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;cp&#34;&gt;&amp;amp;b&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;cp&#34;&gt;*a,*a,*a]&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;cp&#34;&gt;&amp;amp;c&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;cp&#34;&gt;*b,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;cp&#34;&gt;*b,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;cp&#34;&gt;*b]&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This is a little weird to look at, but try to think it through. We’ve set the anchor “a” to an array containing three values. Then we give the anchor “b” a value that references the anchor “a”, three times. We then reference the “b” anchor three more times in “c” anchor. If you run this through the Ruby code from earlier to translate it to JSON, we get the following:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;=&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
 &lt;span class=&#34;nt&#34;&gt;&amp;#34;b&amp;#34;&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;=&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]],&lt;/span&gt;
 &lt;span class=&#34;nt&#34;&gt;&amp;#34;c&amp;#34;&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;=&amp;gt;&lt;/span&gt;
  &lt;span class=&#34;p&#34;&gt;[[[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]],&lt;/span&gt;
   &lt;span class=&#34;p&#34;&gt;[[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]],&lt;/span&gt;
   &lt;span class=&#34;p&#34;&gt;[[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]]]}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This is a play on a fairly old trick that utilizes recursion to expand a rather small amount of data into something that’s magnitudes larger. You can see that by the third line of our YAML, we turned an array with three items into one with one with 27. This issue was addressed in Kubernetes directly in &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/83253&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;CVE-2019-11253&lt;/a&gt; to prevent a maliciously crafted piece of YAML from crashing the kube-apiserver.&lt;/p&gt;
&lt;h2 id=&#34;the-problem-with-the-nail&#34;&gt;The Problem with the Nail&lt;/h2&gt;
&lt;p&gt;Beda did mention there was a second wolf in this conversation, and with it comes the nuance of YAML’s use in the context of Kubernetes.&lt;/p&gt;
&lt;p&gt;First and foremost, Kubernetes resources can be complex, and in turn make the YAML very verbose. Another word that could be used is “explicit.” In machine-to-machine communication, explicitness is great. It doesn’t matter if it’s 100 lines of YAML or 1,000, the difference when being parsed and passed around to different machines and APIs, all while making sure those APIs are responsible for making too many assumptions, is completely negligible. That YAML has to come from somewhere, though, and often that “somewhere” is actually “someone.” Such explicitness means an additional burden of complexity for the user.&lt;/p&gt;
&lt;p&gt;Kubernetes also does &lt;em&gt;a lot&lt;/em&gt;. In fact, it can solve so many problems that it can actually make the simpler problems harder. Beda gives a great analogy referencing a popular word processor that often received complaints for being too “bloated.” People would say, “It has all of these features and I only ever use 20 percent of them.” Beda describes the research the development team undertook to figure out how users leveraged those features: They concluded that while any individual user would only leverage a small percentage of features—say, 10 percent—different collections of users would leverage a different 10 percent, and most features actually received significant usage.&lt;/p&gt;
&lt;p&gt;The same can be said for Kubernetes. It has a long tail of features and fields in the YAML that users are writing, which some may find too verbose, explicit, or repetitive. While you may not use those features, they may be well-utilized by others.&lt;/p&gt;
&lt;p&gt;So with your YAML written, you head to the command line, run &lt;code&gt;kubectl apply&lt;/code&gt;, and don’t receive any errors. Everything must be perfect! Well, maybe not. Even with valid YAML, any of the above scenarios could lead to the Kubernetes API server initially accepting something, but then as different pods and containers are stood up and volumes are claimed, one fat-fingered configuration throws it all out the window. And that means your development loop is now significantly longer than you’d initially expected, a frustrating experience to say the least.&lt;/p&gt;
&lt;h2 id=&#34;how-do-we-improve&#34;&gt;How Do We Improve?&lt;/h2&gt;
&lt;p&gt;As Beda notes, “There are no silver bullets” to this problem. There are a lot of opinions, but at the end of the day, there’s no one-size-fits-all solution. Maybe if it were something that was focused on in the early days of Kubernetes the conversation would be different, but hindsight truly is 20/20.&lt;/p&gt;
&lt;p&gt;That said, with such a strong community, you have a seemingly limitless choice of tools and solutions. From generators to validators to templating systems, it feels like a new option is appearing every day.&lt;/p&gt;
&lt;p&gt;Beda also points out a common trap that some can fall into: mixing up config and code. That is, the difference between using an existing solution such as YAML vs. writing your own DSL. It’s really easy to say that something as complex as Kubernetes could have its own language to create and configure objects, but now you’ve layered a whole new complexity onto an existing one.&lt;/p&gt;
&lt;p&gt;He references a short, but relevant, piece by Mike Hadlow titled &lt;a href=&#34;http://mikehadlow.blogspot.com/2012/05/configuration-complexity-clock.html&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;The Configuration Complexity Clock&lt;/a&gt; in which Hadlow walks through, from start to finish, what happens in a case such as this, and expands on it. Basically, if you create your own configuration DSL, you’ve traded a common general purpose language with a widely known skill set for something that nobody knows. You’ve also traded a potential fleet of testing tools for ones you now need to write yourself. Meanwhile, if you hide concepts, you’re potentially limiting features on one hand. On the other hand, if you’re embracing them, you’re no better off than you were at the start.&lt;/p&gt;
&lt;p&gt;Beda instead advocates embracing the tools that have emerged organically from the Kubernetes community itself. He also points out the benefit of taking a Unix-like toolchain mindset of breaking down a problem into smaller chunks and feeding each one into the next. If an individual doesn’t like a piece of the chain, they can replace it with a similar solution of their own. For example, &lt;a href=&#34;https://carvel.dev/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Carvel&lt;/a&gt; aims to accomplish this approach, providing a collection of single-purpose, composable tools that you can chain together. If you&amp;rsquo;d like to learn more about Carvel, there&amp;rsquo;s a great &lt;a href=&#34;/tv/tgik/142/&#34;&gt;TGIK episode&lt;/a&gt; that covers the different tools and how they can work together.&lt;/p&gt;
&lt;p&gt;So what’s the verdict? Is YAML the worst thing in the world? Is the hate overblown? Well, it’s your opinion and yours alone, just be sure to really reflect on why, exactly, you hold that opinion. Beda’s talk made me realize just how nuanced the common jokes about the pain of YAML really are, so if you have a spare half hour, be sure to &lt;a href=&#34;https://www.youtube.com/watch?v=8PpgqEqkQWA&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;watch it&lt;/a&gt;. Even if you’re a diehard YAML advocate, he still does a great job of putting into words the things that likely haven&amp;rsquo;t even crossed many people’s minds.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/8PpgqEqkQWA&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;


      </description>
    </item>
    
    <item>
      
      <title>Guides: What Is Helm?</title>
      
      <link>/guides/kubernetes/helm-what-is/</link>
      <pubDate>Thu, 16 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/guides/kubernetes/helm-what-is/</guid>
      <description>

        
        &lt;p&gt;&lt;a href=&#34;https://helm.sh&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Helm&lt;/a&gt; is a tool to help you define, install, and upgrade applications running on Kubernetes. At its most basic, Helm is a templating engine that creates Kubernetes manifests. What makes Helm more than that is it can upgrade and scale applications as well.&lt;/p&gt;
&lt;h2 id=&#34;why-is-it-important&#34;&gt;Why Is It Important?&lt;/h2&gt;
&lt;p&gt;Helm reduces the amount of work you need to do to deploy, upgrade, and manage an application to Kubernetes. This helps limit human error and also creates a more declarative configuration to enable workflows like &lt;a href=&#34;https://www.weave.works/blog/what-is-gitops-really&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;GitOps&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This capability really stands out when you have a large, complex application; your app may contain dozens of Kubernetes objects that need to be configured and changed during upgrades.
It also applies if you&amp;rsquo;re deploying the same app multiple times. Using find-and-replace in multiple manifests is a recipe for disaster. Helm can make the process easy and repeatable.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s why an instance of a chart running on a Kubernetes cluster is called a &lt;em&gt;release&lt;/em&gt;. If you need three different installs of a web server, each one is its own release. The Helm docs includes releases as one of &lt;a href=&#34;https://helm.sh/docs/intro/using_helm/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;em&gt;three important concepts&lt;/em&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Helm installs &lt;em&gt;charts&lt;/em&gt; into Kubernetes, creating a new &lt;em&gt;release&lt;/em&gt; for each installation. And to find new charts, you can search Helm chart &lt;em&gt;repositories&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You can read more about the &lt;a href=&#34;https://helm.sh/docs/topics/architecture/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Helm architecture here.&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;how-does-helm-work&#34;&gt;How Does Helm Work?&lt;/h2&gt;
&lt;p&gt;Helm combines the templates and default values in a chart with values you&amp;rsquo;ve supplied, along with information from your cluster to deploy and update applications. You can use charts directly from repos, charts you&amp;rsquo;ve downloaded, or charts you&amp;rsquo;ve created yourself. Helm uses the &lt;a href=&#34;https://golang.org/pkg/text/template/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Go templating engine&lt;/a&gt;, so if you&amp;rsquo;re familiar with that, you&amp;rsquo;ll understand how the charts work.&lt;/p&gt;
&lt;p&gt;As of Helm 3, all of the necessary data is stored locally in your Helm client config or in the cluster where the releases are installed. In previous versions of Helm, it required a component called &lt;code&gt;tiller&lt;/code&gt; installed on the cluster. That component is no longer needed so Helm is now easier to install and use.&lt;/p&gt;
&lt;h2 id=&#34;how-can-i-use-it&#34;&gt;How Can I Use It?&lt;/h2&gt;
&lt;p&gt;If you&amp;rsquo;re ready to start using Helm, check out our guide on &lt;a href=&#34;../helm-gs&#34;&gt;Getting Started With Helm&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: Getting Started with Contour - To Ingress and Beyond</title>
      
      <link>/guides/kubernetes/service-routing-contour-to-ingress-and-beyond/</link>
      <pubDate>Wed, 24 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/guides/kubernetes/service-routing-contour-to-ingress-and-beyond/</guid>
      <description>

        
        &lt;h3 id=&#34;introduction-to-contour&#34;&gt;Introduction to Contour&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://projectcontour.io/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Contour&lt;/a&gt; is an open source Kubernetes
&lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Ingress controller&lt;/a&gt;
that acts as a control plane for the Envoy edge and service proxy (see below).​
Contour supports dynamic configuration updates and multi-team ingress delegation
while maintaining a lightweight profile.&lt;/p&gt;
&lt;p&gt;Contour is built for Kubernetes to empower you to quickly deploy cloud native
applications by using the flexible HTTPProxy API which is a lightweight system
that provides many of the advanced routing features of a Service Mesh.&lt;/p&gt;
&lt;p&gt;Contour deploys the
&lt;a href=&#34;https://www.envoyproxy.io/docs/envoy/latest/intro/what_is_envoy&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Envoy&lt;/a&gt; proxy
as a reverse proxy and load balancer. Envoy is a Layer 7 (application layer) bus
for proxy and communication in modern service-oriented architectures, such as
Kubernetes clusters. Envoy strives to make the network transparent to
applications while maximizing observability to ease troubleshooting.&lt;/p&gt;

&lt;div class=&#34;youtube-video-shortcode&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/Kz671dXioS0&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;div align=&#34;center&#34;&gt;&lt;i&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Kz671dXioS0&amp;feature=youtu.be&#34;&gt;Watch Paul livestream trying Contour 1.12.0 for the first time.&lt;/a&gt;&lt;/i&gt;&lt;/div&gt;
&lt;h3 id=&#34;before-you-begin&#34;&gt;Before You Begin&lt;/h3&gt;
&lt;p&gt;You&amp;rsquo;ll need a Kubernetes cluster. This guide uses a
&lt;a href=&#34;https://tanzu.vmware.com/kubernetes-grid&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Tanzu Kubernetes Grid&lt;/a&gt; cluster, but
any Kubernetes Cluster whether they&amp;rsquo;re running on a Public Cloud, in your
[Home] Lab, or on your desktop such as &lt;a href=&#34;https://kind.sigs.k8s.io/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;KIND&lt;/a&gt; or
&lt;a href=&#34;https://minikube.sigs.k8s.io/docs/start/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;minikube&lt;/a&gt;. You&amp;rsquo;ll also need the
Kubernetes CLI
&lt;a href=&#34;https://kubernetes.io/docs/tasks/tools/install-kubectl/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;kubectl&lt;/a&gt;.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Verify access to your Kubernetes cluster&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ kubectl version --short
Client Version: v1.20.2
Server Version: v1.19.3+vmware.1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create a scratch directory to work from&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;mkdir ~/scratch/contour-demo
&lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; ~/scratch/contour-demo
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;installing-contour-1120&#34;&gt;Installing Contour 1.12.0&lt;/h3&gt;
&lt;p&gt;Since version 1.11.0 we&amp;rsquo;ve got two primary options for installing Contour, A
singleton install from manifests, or by using the
&lt;a href=&#34;https://projectcontour.io/resources/deprecation-policy/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Operator&lt;/a&gt; (which is
currently in Alpha). Since we only plan to install Contour once on the cluster,
we can stick to the safer method of using the Contour provided manifests.&lt;/p&gt;
&lt;p&gt;You can install Contour directly from the manifests provided by the project,
however best practice would have you download them locally first for validation
and repeatability.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Download contour installation manifests&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;wget https://projectcontour.io/quickstart/v1.12.0/contour.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;View the manifests in your favorite local text editor&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;less contour.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Validate even further by doing a dry run install&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl apply -f contour.yaml --dry-run&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;client
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If that all looks good (and it should!), perform the actual install&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl apply -f contour.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;After a few moments you can confirm that its ready.&lt;/p&gt;
&lt;p&gt;You&amp;rsquo;re looking for both the &lt;strong&gt;deployment&lt;/strong&gt; and &lt;strong&gt;DaemonSet&lt;/strong&gt; to show as fully
Available, and a valid IP (or hostname) in the &lt;code&gt;EXTERNAL-IP&lt;/code&gt; field of your
envoy &lt;strong&gt;service&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ kubectl -n projectcontour get deployment,daemonset,service

  NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
  deployment.apps/contour   2/2     &lt;span class=&#34;m&#34;&gt;2&lt;/span&gt;            &lt;span class=&#34;m&#34;&gt;2&lt;/span&gt;           2m18s

  NAME                   DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
  daemonset.apps/envoy   &lt;span class=&#34;m&#34;&gt;3&lt;/span&gt;         &lt;span class=&#34;m&#34;&gt;3&lt;/span&gt;         &lt;span class=&#34;m&#34;&gt;3&lt;/span&gt;       &lt;span class=&#34;m&#34;&gt;3&lt;/span&gt;            &lt;span class=&#34;m&#34;&gt;3&lt;/span&gt;           &amp;lt;none&amp;gt;          2m17s

  NAME              TYPE           CLUSTER-IP       EXTERNAL-IP
  PORT&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;S&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;                      AGE
  service/contour   ClusterIP      100.71.191.199   &amp;lt;none&amp;gt;
  8001/TCP                     2m18s
  service/envoy     LoadBalancer   100.66.114.136   a36c85343e9284c1cb4236d844c31aab-1691151764.us-east-2.elb.amazonaws.com   80:30825/TCP,443:30515/TCP   2m18s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Save the Ingress &lt;code&gt;EXTERNAL-IP&lt;/code&gt; for later use as a &lt;a href=&#34;http://xip.io&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;xip.io&lt;/a&gt; dynamic DNS host.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;


&lt;div class=&#34;aside aside-info&#34;&gt;
    &lt;div class=&#34;aside aside-title&#34;&gt;
        &lt;i class=&#34;fas fa-exclamation-circle&#34;&gt;&lt;/i&gt;
        &lt;div&gt;Note for AWS Users&lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#34;aside aside-content&#34;&gt;
    &lt;p&gt;Since this is deployed in Amazon Web Services I had to resolve the hostname
using the &lt;code&gt;host&lt;/code&gt; command, but in other clouds you will probably get an IP
address.&lt;/p&gt;

    &lt;/div&gt;
&lt;/div&gt;

&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;nv&#34;&gt;INGRESS_HOST&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&amp;lt;external ip address from above&amp;gt;.xip.io
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;creating-an-ingress-using-contour-1120&#34;&gt;Creating an Ingress using Contour 1.12.0&lt;/h3&gt;
&lt;p&gt;Now that Contour is installed we can validate it is functioning correctly by
deploying an application, exposing it as a service, then creating an Ingress
resource. As well as creating the resources we&amp;rsquo;ll output the manifests to a file
for later re-use.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Create a namespace&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl create namespace my-ingress-app -o yaml &amp;gt; my-ingress-app-namespace.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create a deployment containing a basic nginx pod&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl -n my-ingress-app create deployment --image&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;nginx &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  nginx -o yaml &amp;gt; my-ingress-app-deployment.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create a service for the deployment&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl -n my-ingress-app expose deployment nginx --port &lt;span class=&#34;m&#34;&gt;80&lt;/span&gt; -o yaml &amp;gt; my-ingress-app-service.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Finally create an Ingress for the service&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl -n my-ingress-app create ingress nginx --class&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;default &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --rule&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;nginx.&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$INGRESS_HOST&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;/*=nginx:80&amp;#34;&lt;/span&gt; -o yaml &amp;gt; my-ingress-app-ingress.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Validate that your resources are deployed and ready&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ kubectl -n my-ingress-app get all,ingress

Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; use networking.k8s.io/v1 Ingress
NAME                         READY   STATUS    RESTARTS   AGE
pod/nginx-6799fc88d8-dphdt   1/1     Running   &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;          13m

NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;S&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;   AGE
service/nginx   ClusterIP   100.69.247.38   &amp;lt;none&amp;gt;        80/TCP    12m

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nginx   1/1     &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;            &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;           13m

NAME                               DESIRED   CURRENT   READY   AGE
replicaset.apps/nginx-6799fc88d8   &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;         &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;         &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;       13m

NAME                       CLASS     HOSTS                                                                     ADDRESS                                                                   PORTS   AGE
ingress.extensions/nginx   default   a36c85343e9284c1cb4236d844c31acb-1691151764.us-east-2.elb.amazonaws.com   a36c85343e9284c1cb4236d844c31acb-1691151764.us-east-2.elb.amazonaws.com   &lt;span class=&#34;m&#34;&gt;80&lt;/span&gt;      51s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Validate that you can access the application&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ curl -s nginx.3.13.150.109.xip.io  &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; grep h1

&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Congratulations! If you see the &lt;strong&gt;Welcome to nginx!&lt;/strong&gt; message, that means you&amp;rsquo;ve
successfully installed and tested Contour as an Ingress Controller. However its
so much more than that, so lets explore further.&lt;/p&gt;
&lt;p&gt;However let&amp;rsquo;s clean up our resources before we move on. Since all of our
resources are in a single namespace we could use
&lt;code&gt;kubectl delete namespace my-ingress-app&lt;/code&gt;, However we also saved the manifests
so we can use those like so:&lt;/p&gt;


&lt;div class=&#34;aside aside-warning&#34;&gt;
    &lt;div class=&#34;aside aside-title&#34;&gt;
        &lt;i class=&#34;fas fa-exclamation-circle&#34;&gt;&lt;/i&gt;
        &lt;div&gt;Caution&lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#34;aside aside-content&#34;&gt;
    &lt;p&gt;We created these manifests in the same directory as our contour manifests, so
we will move them into a subdirectory to ensure we only delete the app itself.
This is a lesson learned that we should have created them in a subdirectory
in the first place for organizational purposes.&lt;/p&gt;

    &lt;/div&gt;
&lt;/div&gt;

&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;mkdir my-ingress-app
mv my-ingress-app-* my-ingress-app/
kubectl delete -f my-ingress-app
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;beyond-ingress-with-contour-1120&#34;&gt;Beyond Ingress with Contour 1.12.0&lt;/h3&gt;
&lt;p&gt;As well as &lt;strong&gt;Ingress&lt;/strong&gt; Contour supports a resource type &lt;strong&gt;HTTPProxy&lt;/strong&gt; which
extends the concept of &lt;strong&gt;Ingress&lt;/strong&gt; to add many features that you would normally
have to reach for &lt;strong&gt;Istio&lt;/strong&gt; or a similar service mesh to get. We can explore
some of those features here.&lt;/p&gt;
&lt;p&gt;Having learned our lesson about sub directories above, lets create a directory
for our exploration of HTTPProxy.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;mkdir http-proxy
&lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; http-proxy
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;As we did earlier we&amp;rsquo;ll start by deploying a nginx &lt;strong&gt;Pod&lt;/strong&gt; and a &lt;strong&gt;Service&lt;/strong&gt;.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Create a namespace&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl create namespace http-proxy -o yaml &amp;gt; http-proxy-namespace.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create a Deployment containing a basic nginx pod&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl -n http-proxy create deployment --image&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;nginx &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  nginx -o yaml &amp;gt; http-proxy-nginx-deployment.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create a service for the deployment&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl -n http-proxy expose deployment nginx --port &lt;span class=&#34;m&#34;&gt;80&lt;/span&gt; -o yaml &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  &amp;gt; http-proxy-nginx-service.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now that we have the Deployment and Service created we can create the HTTPProxy
resource. Unfortunately we can&amp;rsquo;t just sling a &lt;code&gt;kubectl create httpproxy&lt;/code&gt; like we
could with the other resources so we&amp;rsquo;ll need to get creative.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Create a HTTPProxy manifest&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cat &lt;span class=&#34;s&#34;&gt;&amp;lt;&amp;lt; EOF &amp;gt; http-proxy.yaml
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;apiVersion: projectcontour.io/v1
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;kind: HTTPProxy
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;metadata:
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;  name: www
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;  namespace: http-proxy
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;spec:
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;  virtualhost:
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;    fqdn: www.$INGRESS_HOST
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;  routes:
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;    - conditions:
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;      - prefix: /
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;      services:
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;        - name: nginx
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;          port: 80
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;EOF&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Apply the HTTPProxy manifest&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl apply -n http-proxy -f http-proxy.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Wait a few moments and then attempt to access the nginx service&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;curl -s www.3.13.150.109.xip.io &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; grep h1
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;rate-limiting&#34;&gt;Rate Limiting&lt;/h4&gt;
&lt;p&gt;Now that your nginx is working via &lt;strong&gt;HTTPProxy&lt;/strong&gt; we can look at some of the more
advanced features. Let&amp;rsquo;s start with Rate limiting. Contour 1.12.0 supports doing
&lt;em&gt;local&lt;/em&gt; rate limiting, which means that each Envoy &lt;strong&gt;Pod&lt;/strong&gt; will have its own
limits, vs a &lt;em&gt;global&lt;/em&gt; rate limit which would need further coordination between
the Envoy &lt;strong&gt;Pods&lt;/strong&gt;. You can also set the Rate limit for the virtualhost, or for
a specific route.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s create a fairly aggressive rate limit so we can see the affects of it
fairly quickly. The example cluster I am using has three worker nodes, which
means three Envoy &lt;strong&gt;Pods&lt;/strong&gt; so if I set a rate limit of 2 per minute we should be
able to hit the limit after 6 requests.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Create a new &lt;strong&gt;HTTPProxy&lt;/strong&gt; resource with rate limiting enabled&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cat &lt;span class=&#34;s&#34;&gt;&amp;lt;&amp;lt; EOF &amp;gt; rate-limit.yaml
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;apiVersion: projectcontour.io/v1
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;kind: HTTPProxy
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;metadata:
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;  name: rate
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;  namespace: http-proxy
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;spec:
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;  virtualhost:
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;    fqdn: rate.$INGRESS_HOST
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;    rateLimitPolicy:
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;      local:
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;        requests: 2
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;        unit: minute
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;  routes:
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;    - conditions:
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;      - prefix: /
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;      services:
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;        - name: nginx
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;          port: 80
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;EOF&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Apply the new rate limited manifest:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl -n http-proxy apply -f rate-limit.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Wait a few moments and then fire up a while loop to connecting to the service
and watch it hit the limit after a few hits.&lt;/p&gt;
&lt;p&gt;Note: You&amp;rsquo;ll need to hit CTRL-C to break the while loop.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ &lt;span class=&#34;k&#34;&gt;while&lt;/span&gt; true&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;do&lt;/span&gt; curl -s rate.&lt;span class=&#34;nv&#34;&gt;$INGRESS_HOST&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; grep -E &lt;span class=&#34;s1&#34;&gt;&amp;#39;h1|rate&amp;#39;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;done&lt;/span&gt;

&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
local_rate_limited
local_rate_limited
local_rate_limited
^C
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;That&amp;rsquo;s it, rate limiting is enabled. This is incredibly useful if you have a
service with known limitations or you want to restrict any one user from
overwhelming the service.&lt;/p&gt;
&lt;h4 id=&#34;weighted-routing&#34;&gt;Weighted routing&lt;/h4&gt;
&lt;p&gt;The &lt;strong&gt;HTTPProxy&lt;/strong&gt; resource can also route a Virtual Host to multiple services,
this is a great feature if you want to perform Blue/Green deployments, or you
want to send a small percentage of requests to a special debug endpoint. Let&amp;rsquo;s
explore Weighted routing by adding an Apache service to receive 10% of the
requests.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Create a Deployment containing a basic httpd pod&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl -n http-proxy create deployment --image&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;httpd &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  httpd -o yaml &amp;gt; http-proxy-httpd-deployment.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create a service for the deployment&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl -n http-proxy expose deployment httpd --port &lt;span class=&#34;m&#34;&gt;80&lt;/span&gt; -o yaml &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  &amp;gt; http-proxy-httpd-service.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ensure the new &lt;strong&gt;Pod&lt;/strong&gt; is available beside the existing nginx one.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl get pods -n http-proxy
NAME                     READY   STATUS    RESTARTS   AGE
httpd-757fb56c8d-kz476   1/1     Running   &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;          23s
nginx-6799fc88d8-jxvj7   1/1     Running   &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;          163m
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create a &lt;strong&gt;HTTPProxy&lt;/strong&gt; resource to perform weighted routing across the two services&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cat &lt;span class=&#34;s&#34;&gt;&amp;lt;&amp;lt; EOF &amp;gt; weighted.yaml
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;apiVersion: projectcontour.io/v1
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;kind: HTTPProxy
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;metadata:
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;  name: weight
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;  namespace: http-proxy
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;spec:
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;  virtualhost:
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;    fqdn: weight.$INGRESS_HOST
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;  routes:
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;    - conditions:
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;      - prefix: /
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;      services:
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;        - name: httpd
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;          port: 80
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;          weight: 10
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;        - name: nginx
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;          port: 80
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;          weight: 90
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;EOF&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Apply the new resource&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl -n http-proxy apply -f weighted.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Test the weighting&lt;/p&gt;
&lt;p&gt;Note: It&amp;rsquo;s not clear in the documentation, but it appears that the weighting
is applied per Envoy &lt;strong&gt;Pod&lt;/strong&gt;, so it might not be exactly 10% for small test
runs, but would statistically work out over time.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ &lt;span class=&#34;k&#34;&gt;while&lt;/span&gt; true&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;do&lt;/span&gt; curl -s weight.&lt;span class=&#34;nv&#34;&gt;$INGRESS_HOST&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; grep h1 &lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;done&lt;/span&gt;
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&amp;lt;html&amp;gt;&amp;lt;body&amp;gt;&amp;lt;h1&amp;gt;It works!&amp;lt;/h1&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;
^C
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;That&amp;rsquo;s it! we&amp;rsquo;ve successfully done a walk through of some of the new features of
Contour 1.12.0 and tested out both Rate Limiting and Weighted Routing. Let&amp;rsquo;s
clean up.&lt;/p&gt;
&lt;h4 id=&#34;cleanup&#34;&gt;Cleanup&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Delete your http-proxy namespace and resources&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl -n http-proxy delete -f .
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Uninstall Contour&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; ..
kubectl delete -f contour.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;As you can see Contour 1.12.0 is more than just an Ingress Controller as it
brings some of the more advanced features of a service mesh but without all the
extra resources required. Next time you find yourself looking to run Istio,
remember to check in with Contour and see if it will do what you need.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: Gathering Metrics from Kubernetes with Prometheus and Grafana</title>
      
      <link>/guides/kubernetes/observability-prometheus-grafana-p1/</link>
      <pubDate>Wed, 24 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/guides/kubernetes/observability-prometheus-grafana-p1/</guid>
      <description>

        
        &lt;p&gt;You have your Kubernetes cluster up, it’s running your applications, and
everything seems to be running great. Your work is done, right?&lt;/p&gt;
&lt;p&gt;If only it was that simple. Running in production means keeping a close eye on
how things are performing, so having data that provides such insight is
important. After all, being able to recognize potentially problematic patterns
in how your applications are performing or how Kubernetes is handling specific
workloads can mean the difference between making a quick fix and getting a call
at 3 a.m. because your website is down.&lt;/p&gt;
&lt;p&gt;Two open source tools that can help with this are Prometheus and Grafana.
Prometheus excels at gathering metrics from a wide array of sources, while
Grafana is the go-to tool for visualizing complex time-series data. These two
tools working in tandem are very powerful, and are very easy to install and use!&lt;/p&gt;
&lt;p&gt;In this guide, you’ll be setting up Prometheus and Grafana on an existing Kubernetes cluster, as well as setting up a dashboard in Grafana to visualize the data gathered from that cluster. You can use any Kubernetes installation of your choosing, whether it’s hosted on a cloud provider or even something like &lt;a href=&#34;https://minikube.sigs.k8s.io/docs/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Minikube&lt;/a&gt; running on your local machine.&lt;/p&gt;
&lt;h2 id=&#34;installing-prometheus&#34;&gt;Installing Prometheus&lt;/h2&gt;
&lt;p&gt;Luckily, there’s a comprehensive
&lt;a href=&#34;https://github.com/bitnami/charts/tree/master/bitnami/kube-prometheus&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Helm chart for Prometheus&lt;/a&gt;
with an extensive list of configuration options. If you’re not familiar with
Helm, make sure to check out
&lt;a href=&#34;/guides/kubernetes/helm-what-is/&#34;&gt;Helm: What Is it?&lt;/a&gt; and
&lt;a href=&#34;/guides/kubernetes/helm-gs/&#34;&gt;Getting Started With Helm&lt;/a&gt;. For this walkthrough,
you’ll be keeping things fairly straightforward, so if you’ve used Helm before,
or are just starting to learn, you will have seen most of what’s in this guide.&lt;/p&gt;
&lt;p&gt;To install Prometheus, you first need to add the Bitnami Helm repository by
using the &lt;code&gt;helm repo add&lt;/code&gt; command followed by the &lt;code&gt;helm repo update&lt;/code&gt; command to
pull in the latest metadata:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;helm repo add bitnami https://charts.bitnami.com/bitnami
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;helm repo update
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Before you install Prometheus, check out the
&lt;a href=&#34;https://github.com/bitnami/charts/tree/master/bitnami/kube-prometheus#parameters&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;configuration options&lt;/a&gt;
you have, because there are a lot of them. In this example, it’s assumed that
you’ll be installing with the default configuration into a Kubernetes cluster
with no specific requirements. As you can see, there are options for everything
from how each component is exposed (i.e., the type of ingress used, if it’s
behind a load balancer, etc.) to how data is stored and more. If this was a
production installation, you’d want to thoroughly sort out these options, but
for the purposes of this demo, you can install Prometheus with the default
configuration using the &lt;code&gt;helm install&lt;/code&gt; command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;helm install prometheus bitnami/kube-prometheus
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;After a few moments, you’ll see a few new pods created:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;alertmanager-prometheus-prometheus-oper-alertmanager-0   2/2     Running   &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;          25m
prometheus-kube-state-metrics-68cb46fdd4-gk4jh           1/1     Running   &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;          25m
prometheus-node-exporter-rkg84                           1/1     Running   &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;          25m
prometheus-prometheus-oper-operator-745f4b599c-xjjsn     1/1     Running   &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;          25m
prometheus-prometheus-prometheus-oper-prometheus-0       3/3     Running   &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;          25m
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;There are a few ways you can access Prometheus, but it largely depends on how
your Kubernetes cluster is configured. As the Prometheus documentation points
out, traditionally you would expose the server through a reverse proxy, such as
nginx. But since the default configuration of the Prometheus Helm chart only
exposes it to other pods in the Kubernetes cluster, you can instead take
advantage of the &lt;code&gt;kubectl port-forward&lt;/code&gt; command. Open a new terminal and keep it
open after running the following:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl port-forward --namespace default svc/prometheus-kube-prometheus-prometheus 9090:9090
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Great! The above command forwards all traffic to port 9090 on your machine to
the &lt;code&gt;prometheus-server&lt;/code&gt; pod, which you can see by visiting
&lt;a href=&#34;http://localhost:9090&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;http://localhost:9090&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/prometheus-grafana/prometheus-001.png&#34; alt=&#34;Prometheus Dashboard&#34;  /&gt;&lt;/p&gt;
&lt;h2 id=&#34;installing-grafana&#34;&gt;Installing Grafana&lt;/h2&gt;
&lt;p&gt;Much like Prometheus, Grafana has a great
&lt;a href=&#34;https://hub.helm.sh/charts/bitnami/grafana&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Helm chart&lt;/a&gt; for installing it.
Again, you’ll see a plethora of configuration options to tweak the installation
to your needs, but for this demo, you can just install it with the default
configuration to see it in action. You can install the &lt;code&gt;bitnami/grafana&lt;/code&gt; Helm
chart with the following command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;helm install grafana bitnami/grafana
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;After a few moments, you’ll see a new pod created:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;grafana-66c98bcb86-xpd5t                         1/1     Running   &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;          2d23h
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;There’s one additional step you’ll need to take. The Grafana Helm chart
generates a random password for you and stores it as a secret in Kubernetes. You
can retrieve this password by running the following command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;$(&lt;/span&gt;kubectl get secret grafana-admin --namespace default -o &lt;span class=&#34;nv&#34;&gt;jsonpath&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;{.data.GF_SECURITY_ADMIN_PASSWORD}&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; base64 --decode&lt;span class=&#34;k&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Note the password, as you’ll be needing it shortly. Like the Prometheus install,
you may have different requirements or capabilities for exposing services
outside of your Kubernetes cluster, but you can again use the
&lt;code&gt;kubectl port-forward&lt;/code&gt; command. Open a new terminal and run:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;POD_NAME&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;$(&lt;/span&gt;kubectl get pods --namespace default -l &lt;span class=&#34;s2&#34;&gt;&amp;#34;app.kubernetes.io/name=grafana,app.kubernetes.io/instance=grafana&amp;#34;&lt;/span&gt; -o &lt;span class=&#34;nv&#34;&gt;jsonpath&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;{.items[0].metadata.name}&amp;#34;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;)&lt;/span&gt;
kubectl --namespace default port-forward &lt;span class=&#34;nv&#34;&gt;$POD_NAME&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;3000&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;With this command running, you can now access Grafana at
&lt;a href=&#34;http://localhost:3000&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;http://localhost:3000&lt;/a&gt;! You’ll be prompted for a
username, which by default is &lt;code&gt;admin&lt;/code&gt;, along with the password you just
retrieved. Though when you log in, you may notice that things are a bit bare.
That’s because, while Prometheus is automatically gathering metrics from your
Kubernetes cluster, Grafana doesn’t know anything about your Prometheus install.
It does, however, know how to speak to a Prometheus server, and makes it very
easy to configure it as a data source.&lt;/p&gt;
&lt;h2 id=&#34;visualizing-prometheus-data-in-grafana&#34;&gt;Visualizing Prometheus Data in Grafana&lt;/h2&gt;
&lt;p&gt;If you mouse over the cogwheel on the left-hand side of the Grafana screen,
you’ll be prompted with several configuration options. Choose “Data Sources,”
followed by “Add data source.” Here, you’ll see a long list of data sources that
Grafana knows how to talk to automatically, and luckily Prometheus is on that
list. Choose “Prometheus” and you’ll be brought to the configuration screen for
your new data source.&lt;/p&gt;
&lt;p&gt;If you didn’t change the default configuration when installing Prometheus,
you’ll only need to give it a name of your choosing, as well as the URL where
Prometheus is running. Since both are running in the same cluster, you can
connect Grafana to Prometheus using the internal DNS to Kubernetes by providing
it the service name that Prometheus is connected to:
&lt;code&gt;http://prometheus-kube-prometheus-prometheus.default.svc.cluster.local:9090&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/prometheus-grafana/prometheus-002.png&#34; alt=&#34;Adding a new Grafana data source&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;If you’re already familiar with creating dashboards and
&lt;a href=&#34;https://grafana.com/docs/grafana/latest/panels/panels-overview/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;panels&lt;/a&gt; in
Grafana, you’re all set! However, if you’re new to Grafana, don’t worry; this is
where you can once again look to the great community for information about these
tools. The Grafana website has a huge repository of dashboards that can be
easily shared and imported into your own Grafana installation. And all you need
in order to import one of these dashboards from the Grafana site is a single ID
number. Take &lt;a href=&#34;https://grafana.com/grafana/dashboards/10000&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;this dashboard&lt;/a&gt;, for
example. You can see that it has an ID of “10000.”&lt;/p&gt;
&lt;p&gt;To import this dashboard, mouse over the “Dashboards” section on the left-hand
side of the Grafana screen (the icon is four squares) and choose “Manage.” On
the top right of the dashboard management screen, click “Import” and you’ll be
prompted for the URL, ID, or JSON for the dashboard that you wish to import.
Under “Import via grafana.com,” enter “10000,” matching the ID of the dashboard
that you wish to import. Feel free to change the name or the unique identifier,
but the one thing you must provide is the data source, which is asked for on the
bottom of the configuration screen. Choose your Prometheus data source, click
“Import,” and you will be greeted by your newly created, but fully populated,
dashboard!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/prometheus-grafana/prometheus-003.png&#34; alt=&#34;Grafana dashboard showing Kubernetes metrics&#34;  /&gt;&lt;/p&gt;
&lt;h2 id=&#34;whats-next&#34;&gt;What’s Next?&lt;/h2&gt;
&lt;p&gt;The great news is that any data gathered from Prometheus can be used in Grafana.
That means any pod with the
&lt;a href=&#34;https://github.com/helm/charts/tree/master/stable/prometheus#scraping-pod-metrics-via-annotations&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;proper annotations&lt;/a&gt;
will automatically get scraped by Prometheus. Many languages and frameworks have
&lt;a href=&#34;https://prometheus.io/docs/instrumenting/clientlibs/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;libraries that support exposing metrics&lt;/a&gt;
that Prometheus can gather, including
&lt;a href=&#34;https://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-features.html#production-ready-metrics-export-prometheus&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Spring&lt;/a&gt;.
Make sure to check out the libraries available for your language of choice!
Additionally, if you&amp;rsquo;re looking to learn more about observability and how to get
better insight into your applications, make sure to check out our
&lt;a href=&#34;/patterns/observability/&#34;&gt;growing collection of guides&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: What are Kubernetes Secrets and Service Accounts?</title>
      
      <link>/guides/kubernetes/platform-security-secrets-sa-what-is/</link>
      <pubDate>Wed, 24 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/guides/kubernetes/platform-security-secrets-sa-what-is/</guid>
      <description>

        
        &lt;p&gt;In software, there’s often data that you want to keep separate from your build
process. These could be simple configuration properties, such as URLs or IP
addresses, or more sensitive data, such as usernames and passwords, OAuth tokens
or TLS certificates. In Kubernetes, these are referred to as
&lt;a href=&#34;https://kubernetes.io/docs/concepts/configuration/secret/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Secrets&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;secrets&#34;&gt;Secrets&lt;/h2&gt;
&lt;p&gt;It’s worth noting that while the name “secret” may imply “secure”, there are
some qualifiers. By default, all secrets are stored unencrypted in &lt;code&gt;etcd&lt;/code&gt;. As of
Kubernetes 1.13 though, operators are given the option of
&lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;encrypting data at rest in etcd&lt;/a&gt;.
Additionally, you can
&lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/kms-provider/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;integrate with an external Key Management Service&lt;/a&gt;,
such as &lt;a href=&#34;https://cloud.google.com/kms/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Google Cloud KMS&lt;/a&gt; or
&lt;a href=&#34;https://www.vaultproject.io/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;HashiCorp Vault&lt;/a&gt;. This guide doesn’t cover these
topics, but the above links are a great start to learn more.&lt;/p&gt;
&lt;p&gt;All examples used in this guide can be
&lt;a href=&#34;https://github.com/BrianMMcClain/k8s-secrets-and-sa&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;found on GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Before you can get started using secrets, you first need to create a secret. As
you may expect, this can be done by defining an object of kind &lt;code&gt;Secret&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Secret&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;mysecret&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;type&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Opaque&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;username&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;bXl1c2VybmFtZQo=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;c&#34;&gt;#Base64 encoded value of &amp;#34;myusername&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;password&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;bXlwYXNzd29yZAo=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;c&#34;&gt;#Base64 encoded value of &amp;#34;mypassword&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Secrets in Kubernetes are, at their most basic form, a collection of keys and
values. The above example creates a secret named &lt;code&gt;mysecret&lt;/code&gt; with two keys:
&lt;code&gt;username&lt;/code&gt; and &lt;code&gt;password&lt;/code&gt;. There’s one very important thing to note though,
which is that the values of these key/value pairs are encoded as base64.
Remember that base64 is an encoding algorithm, not an encryption algorithm. This
is done to help facilitate data that may not be entirely alpha-numeric, and
instead could include binary data, non-ASCII data, etc. You apply can this YAML
as you would if you were creating any other Kubernetes object:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl apply -f https://raw.githubusercontent.com/BrianMMcClain/k8s-secrets-and-sa/main/secret-base64.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once applied, you can see that while you can get the secret with &lt;code&gt;kubectl&lt;/code&gt;, it
avoids printing the values of each key by default:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ kubectl describe secret mysecret

Name:         mysecret
Namespace:    default
Labels:       &amp;lt;none&amp;gt;
Annotations:  
Type:         Opaque

&lt;span class=&#34;nv&#34;&gt;Data&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;====&lt;/span&gt;
password:  &lt;span class=&#34;m&#34;&gt;11&lt;/span&gt; bytes
username:  &lt;span class=&#34;m&#34;&gt;11&lt;/span&gt; bytes
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Of course, if you want to see the base64-encoded contents of the secret, you can
still fetch them with a slightly different command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ kubectl get secret mysecret -o yaml

apiVersion: v1
data:
  password: &lt;span class=&#34;nv&#34;&gt;bXlwYXNzd29yZAo&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;
  username: &lt;span class=&#34;nv&#34;&gt;bXl1c2VybmFtZQo&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;
kind: Secret
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Great! With your secret created, it’s time to start creating pods to use it!
You’re faced with another decision, however, since Kubernetes provides a couple
of methods for presenting secrets to a pod. The first example you’ll look at is
mounting them as files in a volume:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Pod&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;secret-as-file&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;containers&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;secret-as-file&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;nginx&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;volumeMounts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;mysecretvol&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;mountPath&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;/etc/mysecret&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;readOnly&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;true&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;volumes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;mysecretvol&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;secret&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;secretName&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;mysecret&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Here, a new pod named &lt;code&gt;secret-as-file&lt;/code&gt; is created from the
&lt;a href=&#34;https://hub.docker.com/_/nginx&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;NGINX Docker image&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;NOTE: The nginx container image is used here simply because it&amp;rsquo;s an easily
accessible long-running process, this would look the same for your own container
image.&lt;/p&gt;
&lt;p&gt;There are two sections to point out, the first being the &lt;code&gt;volumes&lt;/code&gt; section,
which defines a new volume. Kubernetes has
&lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;many different types of volumes to choose from&lt;/a&gt;,
but for this case you’re specifically interested in creating a
&lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/volumes/#secret&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;volume of type &lt;code&gt;secret&lt;/code&gt;&lt;/a&gt;.
These volumes are backed by &lt;code&gt;tmpfs&lt;/code&gt;, a RAM-based file system, rather than
written to a persistent disk. Secret volumes require you to define the secret to
mount (in the &lt;code&gt;secretName&lt;/code&gt; field), and for each key in your secret, it creates a
file that contains the key’s value. You can see this in action by applying this
YAML and then listing the files at the &lt;code&gt;mountPath&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl apply -f https://raw.githubusercontent.com/BrianMMcClain/k8s-secrets-and-sa/main/pod-secret-as-file.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ kubectl &lt;span class=&#34;nb&#34;&gt;exec&lt;/span&gt; secret-as-file -- ls /etc/mysecret

password
username

$ kubectl &lt;span class=&#34;nb&#34;&gt;exec&lt;/span&gt; secret-as-file -- cat /etc/mysecret/username

myusername
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;As you can see, there are two files in the volume that was created: &lt;code&gt;password&lt;/code&gt;
and &lt;code&gt;username&lt;/code&gt;. If you print out the contents of the &lt;code&gt;username&lt;/code&gt; file, you can
see the secret’s value of &lt;code&gt;myusername&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ kubectl &lt;span class=&#34;nb&#34;&gt;exec&lt;/span&gt; secret-as-file -- cat /etc/mysecret/username

myusername
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Alternatively, secrets can also be presented to your container as environment
variables. Consider the following YAML:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Pod&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;secret-as-env&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;containers&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;secret-as-env&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;nginx&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;env&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;SECRET_USERNAME&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;valueFrom&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;secretKeyRef&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;mysecret&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;key&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;username&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;SECRET_PASSWORD&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;valueFrom&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;secretKeyRef&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;mysecret&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;key&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;password&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Here, instead of defining volumes that reference your secret, two environment
variables are defined and reference the secret name and key name. Applying this
YAML allows you to retrieve these environment variables from a shell in the pod:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl apply -f https://raw.githubusercontent.com/BrianMMcClain/k8s-secrets-and-sa/main/pod-secret-as-env.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ kubectl &lt;span class=&#34;nb&#34;&gt;exec&lt;/span&gt; secret-as-env -- sh -c &lt;span class=&#34;s2&#34;&gt;&amp;#34;echo \$SECRET_USERNAME&amp;#34;&lt;/span&gt;

myusername
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;As you can see, the value of your secret is stored in the &lt;code&gt;$SECRET_USERNAME&lt;/code&gt;
environment variable as defined!&lt;/p&gt;
&lt;h2 id=&#34;service-accounts&#34;&gt;Service Accounts&lt;/h2&gt;
&lt;p&gt;When you interact directly with Kubernetes, using &lt;code&gt;kubectl&lt;/code&gt; for example, you’re
using a user account. When processes in pods need to interact with Kubernetes
though, they use a
&lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;service account&lt;/a&gt;,
which describes the set of permissions they have within Kubernetes. The good
news is that out of the box, all pods are given the &lt;code&gt;default&lt;/code&gt; service account.
Unless your Kubernetes administrator has changed the &lt;code&gt;default&lt;/code&gt; service account
though, the permissions are limited. If you run &lt;code&gt;kubectl&lt;/code&gt; in a container on
Kubernetes, it will automatically know where to find the cluster that it&amp;rsquo;s
running on. You can verify this by standing up a pod and running
&lt;code&gt;kubectl version&lt;/code&gt;, which will show information about the server it&amp;rsquo;s connected
to:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl run -it kubectl --restart&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;Never --rm --image&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;brianmmcclain/kubectl-alpine -- /bin/bash
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ kubectl version

Client Version: version.Info&lt;span class=&#34;o&#34;&gt;{&lt;/span&gt;Major:&lt;span class=&#34;s2&#34;&gt;&amp;#34;1&amp;#34;&lt;/span&gt;, Minor:&lt;span class=&#34;s2&#34;&gt;&amp;#34;18&amp;#34;&lt;/span&gt;, GitVersion:&lt;span class=&#34;s2&#34;&gt;&amp;#34;v1.18.4&amp;#34;&lt;/span&gt;, GitCommit:&lt;span class=&#34;s2&#34;&gt;&amp;#34;c96aede7b5205121079932896c4ad89bb93260af&amp;#34;&lt;/span&gt;, GitTreeState:&lt;span class=&#34;s2&#34;&gt;&amp;#34;clean&amp;#34;&lt;/span&gt;, BuildDate:&lt;span class=&#34;s2&#34;&gt;&amp;#34;2020-06-17T11:41:22Z&amp;#34;&lt;/span&gt;, GoVersion:&lt;span class=&#34;s2&#34;&gt;&amp;#34;go1.13.9&amp;#34;&lt;/span&gt;, Compiler:&lt;span class=&#34;s2&#34;&gt;&amp;#34;gc&amp;#34;&lt;/span&gt;, Platform:&lt;span class=&#34;s2&#34;&gt;&amp;#34;linux/amd64&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;}&lt;/span&gt;
Server Version: version.Info&lt;span class=&#34;o&#34;&gt;{&lt;/span&gt;Major:&lt;span class=&#34;s2&#34;&gt;&amp;#34;1&amp;#34;&lt;/span&gt;, Minor:&lt;span class=&#34;s2&#34;&gt;&amp;#34;18&amp;#34;&lt;/span&gt;, GitVersion:&lt;span class=&#34;s2&#34;&gt;&amp;#34;v1.18.2&amp;#34;&lt;/span&gt;, GitCommit:&lt;span class=&#34;s2&#34;&gt;&amp;#34;52c56ce7a8272c798dbc29846288d7cd9fbae032&amp;#34;&lt;/span&gt;, GitTreeState:&lt;span class=&#34;s2&#34;&gt;&amp;#34;clean&amp;#34;&lt;/span&gt;, BuildDate:&lt;span class=&#34;s2&#34;&gt;&amp;#34;2020-04-30T20:19:45Z&amp;#34;&lt;/span&gt;, GoVersion:&lt;span class=&#34;s2&#34;&gt;&amp;#34;go1.13.9&amp;#34;&lt;/span&gt;, Compiler:&lt;span class=&#34;s2&#34;&gt;&amp;#34;gc&amp;#34;&lt;/span&gt;, Platform:&lt;span class=&#34;s2&#34;&gt;&amp;#34;linux/amd64&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Notice that you haven’t provided any credentials or configuration file. This
information is provided by Kubernetes and the &lt;code&gt;default&lt;/code&gt; service account.
However, almost any attempt at interacting with the Kubernetes API will be
greeted with denial:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ kubectl get pods

Error from server &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;Forbidden&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;: pods is forbidden: User
&lt;span class=&#34;s2&#34;&gt;&amp;#34;system:serviceaccount:default:default&amp;#34;&lt;/span&gt; cannot list resource &lt;span class=&#34;s2&#34;&gt;&amp;#34;pods&amp;#34;&lt;/span&gt; in API group
&lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt; in the namespace &lt;span class=&#34;s2&#34;&gt;&amp;#34;default&amp;#34;&lt;/span&gt;

$ &lt;span class=&#34;nb&#34;&gt;exit&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Note: If you need to check if you have permission to run a command before
actually running it, you can use the &lt;code&gt;kubectl auth can-i&lt;/code&gt; command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ kubectl auth can-i get pods
no
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To address this, you can
&lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;create a new service account&lt;/a&gt;
with a wider set of permissions. This is demonstrated in the following YAML:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;rbac.authorization.k8s.io/v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Role&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;namespace&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;default&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;pod-read-role&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;rules&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;apiGroups&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# &amp;#34;&amp;#34; indicates the core API group&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;resources&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;pods&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;verbs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;get&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;watch&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;list&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;---&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;ServiceAccount&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;pod-read-sa&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;---&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;RoleBinding&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;rbac.authorization.k8s.io/v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;pod-read-rolebinding&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;namespace&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;default&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;subjects&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;ServiceAccount&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;pod-read-sa&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;apiGroup&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;roleRef&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Role&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;pod-read-role&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;apiGroup&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Here, three things are created: a &lt;code&gt;Role&lt;/code&gt; named “pod-read-role”, a
&lt;code&gt;ServiceAccount&lt;/code&gt;, and a &lt;code&gt;RoleBinding&lt;/code&gt; to tie them together. Specifically, the
&lt;code&gt;Role&lt;/code&gt; gives access to the “get”, “watch” and “list” actions on the resource
“pods”. Described more simply, this role allows you to read information about
pods, but not write or delete information about pods. This will allow you to do
things like &lt;code&gt;kubectl get pods&lt;/code&gt;, but not &lt;code&gt;kubectl delete pod&lt;/code&gt;. You can see this
in action by applying this YAML, creating a pod with this service account and
running the commands yourself:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl apply -f https://raw.githubusercontent.com/BrianMMcClain/k8s-secrets-and-sa/main/role-sa-pod-read.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl run -it --restart&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;Never --rm kubectl-with-sa --image&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;brianmmcclain/kubectl-alpine --serviceaccount&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;pod-read-sa -- /bin/bash
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ kubectl get pods

NAME              READY   STATUS    RESTARTS   AGE
kubectl           1/1     Running   &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;          22s
kubectl-with-sa   1/1     Running   &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;          6s
secret-as-env     1/1     Running   &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;          3h40m
secret-as-file    1/1     Running   &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;          4h10m

$ kubectl delete pod secret-as-file

Error from server &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;Forbidden&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;: pods &lt;span class=&#34;s2&#34;&gt;&amp;#34;secrets-as-file&amp;#34;&lt;/span&gt; is forbidden: User
&lt;span class=&#34;s2&#34;&gt;&amp;#34;system:serviceaccount:default:pod-read-sa&amp;#34;&lt;/span&gt; cannot delete resource &lt;span class=&#34;s2&#34;&gt;&amp;#34;pods&amp;#34;&lt;/span&gt; in API
group &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt; in the namespace &lt;span class=&#34;s2&#34;&gt;&amp;#34;default&amp;#34;&lt;/span&gt;

$ &lt;span class=&#34;nb&#34;&gt;exit&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Finally, the combination of secrets and service accounts can be leveraged to
pull container images from private registries by
&lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;using the &lt;code&gt;imagePullSecrets&lt;/code&gt; configuration property&lt;/a&gt;.
You can create a secret from the command line with the following command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl create secret docker-registry myregistrykey --docker-server&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;DUMMY_SERVER &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;        --docker-username&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;DUMMY_USERNAME --docker-password&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;DUMMY_DOCKER_PASSWORD &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;        --docker-email&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;DUMMY_DOCKER_EMAIL
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Note: The secret used here isn&amp;rsquo;t exposed to the pod in the same way that you&amp;rsquo;ve
seen earlier. The processes inside the containers of the pod don&amp;rsquo;t have access
to this information. Instead, Kubernetes knows that it needs to use these
credentials to pull the container images.&lt;/p&gt;
&lt;p&gt;Once you create the secret by filling in your registry’s server, username,
password, and email, you can create a service account, or edit an existing one,
to use this secret when pulling container images. For example, you can add this
to the &lt;code&gt;default&lt;/code&gt; service account. Make note, however, that this will overwrite
any &lt;code&gt;imagePullSecret&lt;/code&gt; previously set:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl patch serviceaccount default -p &lt;span class=&#34;s1&#34;&gt;&amp;#39;{&amp;#34;imagePullSecrets&amp;#34;: [{&amp;#34;name&amp;#34;: &amp;#34;myregistrykey&amp;#34;}]}&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Since this adds the &lt;code&gt;imagePullSecrets&lt;/code&gt; property to the default service account,
any pod that you create without specifying a different service account will have
these permissions. However, it&amp;rsquo;s worth noting that you can also
&lt;a href=&#34;https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;specify &lt;code&gt;imagePullSecrets&lt;/code&gt; on an individual pod&lt;/a&gt;
if it fits your deployment model better.&lt;/p&gt;
&lt;h2 id=&#34;cleanup&#34;&gt;Cleanup&lt;/h2&gt;
&lt;p&gt;To remove the resources that you&amp;rsquo;ve created, you can use &lt;code&gt;kubectl delete -f&lt;/code&gt;
command and provide the file names used when applying them:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl delete -f &amp;lt;insert file&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;learn-more&#34;&gt;Learn More&lt;/h2&gt;
&lt;p&gt;As with all things Kubernetes, the best place to go to keep learning is the
&lt;a href=&#34;https://kubernetes.io/docs/home/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;official documentation&lt;/a&gt;, which covers
&lt;a href=&#34;https://kubernetes.io/docs/concepts/configuration/secret/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;secrets&lt;/a&gt; and
&lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;service accounts&lt;/a&gt;
in even greater detail. You can also see where these are used in other guides,
such as &lt;a href=&#34;/guides/containers/cnb-gs-kpack/&#34;&gt;Getting Started with kpack&lt;/a&gt; and
&lt;a href=&#34;/guides/ci-cd/tekton-gs-p1/&#34;&gt;Getting Started with Tekton&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: Deploy a Custom Node.js Application using Bitnami Containers</title>
      
      <link>/guides/containers/deploy-custom-nodejs-app-bitnami-containers/</link>
      <pubDate>Fri, 05 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/guides/containers/deploy-custom-nodejs-app-bitnami-containers/</guid>
      <description>

        
        &lt;p&gt;Developers like using containers for development because they are easy to use, portable, and require less maintenance overhead compared to bare metal or virtual machines. &lt;a href=&#34;https://www.docker.com/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Docker&lt;/a&gt; is a popular choice, because it provides tools that make it simple for developers to build, run and publish applications in containers.&lt;/p&gt;
&lt;p&gt;If you&amp;rsquo;ve seen Docker in action and then wondered &amp;ldquo;can I use this with my own application&amp;rdquo;, then you&amp;rsquo;re in the right place.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://bitnami.com&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami&lt;/a&gt; makes it easy to create a Docker image of your own application using its production-ready &lt;a href=&#34;https://bitnami.com/containers&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;container images&lt;/a&gt;. Once created, you can run and test your application with Docker, or publish it online so that others can find and use it. And, because Bitnami container images are always secure, optimized and up-to-date, you can rest assured that your application always has access to the latest language features and security fixes.&lt;/p&gt;
&lt;p&gt;This guide walks you through the process of creating an optimized, secure Docker image of a custom application using a Bitnami base container, then testing it and publishing it online. It also guides you through the steps to follow when you update your application and need to rebuild and republish it for your users.&lt;/p&gt;
&lt;h2 id=&#34;assumptions-and-prerequisites&#34;&gt;Assumptions and Prerequisites&lt;/h2&gt;
&lt;p&gt;This guide focuses on creating a secure and optimized Docker image of a custom Node.js application using Bitnami&amp;rsquo;s Node.js production image. In case you don&amp;rsquo;t have a Node.js application at hand, you can use &lt;a href=&#34;https://github.com/bitnami/tutorials/tree/master/simple-node-app&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;this simple &amp;ldquo;Hello world&amp;rdquo; application&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This guide makes the following assumptions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You have a Docker environment installed and configured. &lt;a href=&#34;https://docs.docker.com/engine/installation/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Learn more about installing Docker&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;You have a Docker Hub account. &lt;a href=&#34;https://hub.docker.com/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Register for a free account&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;You have Git installed and configured.&lt;/li&gt;
&lt;li&gt;You have a basic understanding of how containers work. Learn more about containers in &lt;a href=&#34;https://www.youtube.com/watch?v=Pb1bgI59dF0&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;our YouTube video&lt;/a&gt;, on &lt;a href=&#34;https://en.wikipedia.org/wiki/Operating-system-level_virtualization&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Wikipedia&lt;/a&gt; and on &lt;a href=&#34;http://www.zdnet.com/article/containers-fundamental-to-the-evolution-of-the-cloud/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;ZDNet&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;step-1-create-or-obtain-the-application-source-code&#34;&gt;Step 1: Create or obtain the application source code&lt;/h2&gt;
&lt;p&gt;To begin the process, ensure that you have access to the application source code. If you don&amp;rsquo;t have an existing Node.js application, follow the steps below to create one:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Create a working directory for the application on your local host:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;mkdir myproject
&lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; myproject
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create a &lt;em&gt;package.json&lt;/em&gt; file listing the dependencies for the project:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;simple-node-app&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;#34;version&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;1.0.0&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;#34;description&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Node.js on Docker&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;#34;main&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;server.js&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;#34;scripts&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;nt&#34;&gt;&amp;#34;start&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;node server.js&amp;#34;&lt;/span&gt;
  &lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;#34;dependencies&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;nt&#34;&gt;&amp;#34;express&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;^4.13&amp;#34;&lt;/span&gt;
  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create a &lt;em&gt;server.js&lt;/em&gt; file for the Express application which returns a &amp;ldquo;Hello world&amp;rdquo; message on access:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-javascript&#34; data-lang=&#34;javascript&#34;&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;use strict&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;

&lt;span class=&#34;kr&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;express&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;require&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;express&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;// Constants
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;kr&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;PORT&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;process&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;env&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;PORT&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;||&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;// App
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;kr&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;app&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;express&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;();&lt;/span&gt;
&lt;span class=&#34;nx&#34;&gt;app&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;get&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;/&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kd&#34;&gt;function&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;req&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;res&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
  &lt;span class=&#34;nx&#34;&gt;res&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;send&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Hello world\n&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;});&lt;/span&gt;

&lt;span class=&#34;nx&#34;&gt;app&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;listen&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;PORT&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;span class=&#34;nx&#34;&gt;console&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;log&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Running on http://localhost:&amp;#39;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;PORT&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;step-2-create-a-dockerfile&#34;&gt;Step 2: Create a Dockerfile&lt;/h2&gt;
&lt;p&gt;A &lt;em&gt;Dockerfile&lt;/em&gt; is similar to a recipe: it contains all the ingredients needed to create a Docker image. Typically, each line represents a separate step and begins with an instruction keyword followed by a series of arguments. &lt;a href=&#34;https://docs.docker.com/engine/reference/builder&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Learn more about the &lt;em&gt;Dockerfile&lt;/em&gt; format&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In the project directory, create a file named &lt;em&gt;Dockerfile&lt;/em&gt; and fill it with the following content:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;# First build stage
FROM bitnami/node:12 as builder
ENV NODE_ENV=&amp;#34;production&amp;#34;

# Copy app&amp;#39;s source code to the /app directory
COPY . /app

# The application&amp;#39;s directory will be the working directory
WORKDIR /app

# Install Node.js dependencies defined in &amp;#39;/app/packages.json&amp;#39;
RUN npm install

# Second build stage
FROM bitnami/node:12-prod
ENV NODE_ENV=&amp;#34;production&amp;#34;

# Copy the application code
COPY --from=builder /app /app

# Create a non-root user
RUN useradd -r -u 1001 -g root nonroot
RUN chown -R nonroot /app
USER nonroot

WORKDIR /app
EXPOSE 3000

# Start the application
CMD [&amp;#34;npm&amp;#34;, &amp;#34;start&amp;#34;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This &lt;em&gt;Dockerfile&lt;/em&gt; consists of two build stages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The first stage uses the Bitnami Node.js 12.x development image to copy the application source and install the required application modules using &lt;em&gt;npm install&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;The second stage uses the Bitnami Node.js 12.x production image and creates a minimal Docker image that only consists of the application source, modules and Node.js runtime.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Bitnami&amp;rsquo;s Node.js production image is different from its Node.js development image, because the production image (tagged with the suffix &lt;em&gt;prod&lt;/em&gt;) is based on &lt;a href=&#34;https://github.com/bitnami/minideb&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;minideb&lt;/a&gt; and does not include additional development dependencies. It is therefore lighter and smaller in size than the development image and is commonly used in multi-stage builds as the final target image.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;first-build-stage&#34;&gt;First build stage&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s take a closer look at the steps in the first build stage:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;em&gt;FROM&lt;/em&gt; instruction kicks off the &lt;em&gt;Dockerfile&lt;/em&gt; and specifies the base image to use. Bitnami offers a number of container images for Docker which can be used as base images. Since the example application used in this guide is a Node.js application, &lt;a href=&#34;https://github.com/bitnami/bitnami-docker-node&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami&amp;rsquo;s Node.js development container&lt;/a&gt; is the best choice for the base image.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;NODE_ENV&lt;/em&gt; environment variable is defined so that &lt;em&gt;npm install&lt;/em&gt; only installs the application modules that are required in production environments.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;COPY&lt;/em&gt; instruction copies the source code from the current directory on the host to the &lt;em&gt;/app&lt;/em&gt; directory in the image.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;RUN&lt;/em&gt; instruction executes a shell command. It&amp;rsquo;s used to run &lt;em&gt;npm install&lt;/em&gt; to install the application dependencies.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;WORKDIR&lt;/em&gt; instructions set the working directory for the image.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;second-build-stage&#34;&gt;Second build stage&lt;/h3&gt;
&lt;p&gt;Here is what happens in the second build stage:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Since the target here is a minimal, secure image, the &lt;em&gt;FROM&lt;/em&gt; instruction specifies &lt;a href=&#34;https://github.com/bitnami/bitnami-docker-node&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami&amp;rsquo;s Node.js production container&lt;/a&gt; as the base image. Bitnami production images can be identified by the suffix &lt;em&gt;prod&lt;/em&gt; in the image tag.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;COPY&lt;/em&gt; instruction copies the source code and installed dependencies from the first stage to the &lt;em&gt;/app&lt;/em&gt; directory in the image.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;RUN&lt;/em&gt; commands create a non-root user account that the application will run under. For security reasons, it&amp;rsquo;s recommended to always run your application using a non-root user account.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;CMD&lt;/em&gt; instruction specifies the command to run when the image starts. In this case, &lt;em&gt;npm start&lt;/em&gt; will start the application.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;step-3-build-the-docker-image&#34;&gt;Step 3: Build the Docker image&lt;/h2&gt;
&lt;p&gt;Once the &lt;em&gt;Dockerfile&lt;/em&gt; is created, building a Docker image is as simple as calling the &lt;em&gt;docker build&lt;/em&gt; command. Execute the command below in the directory containing the &lt;em&gt;Dockerfile&lt;/em&gt;. Replace the DOCKER_USERNAME placeholder in the command below with your Docker account username. This Docker account username is necessary to avoid namespace errors when later pushing the image to your Docker Hub account.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;docker build -t DOCKER_USERNAME/my-node-app:0.1.0 .
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;For successful execution of this and subsequent &lt;em&gt;docker&lt;/em&gt; commands, ensure that the user account you&amp;rsquo;re using belongs to the &lt;em&gt;docker&lt;/em&gt; group.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This will create an image named &lt;em&gt;my-node-app&lt;/em&gt;, tagged as version &lt;em&gt;0.1.0&lt;/em&gt;. This tag uniquely identifies a Docker image, allowing you to deploy a specific version of the application if needed.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s an example of what you should see during the build process:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/containers/bitnami/deploy-custom-nodejs-app-bitnami-containers/deploy-custom-nodejs-app-bitnami-containers-1.png&#34; alt=&#34;Image build process&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Once the build process is complete, use the &lt;em&gt;docker images&lt;/em&gt; command to verify that the image has been added to your local repository.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;docker images | grep my-node-app
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The version tag added during the &lt;em&gt;docker build&lt;/em&gt; command also appears in the output of &lt;em&gt;docker images&lt;/em&gt;.  Here&amp;rsquo;s an example of what you should see:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/containers/bitnami/deploy-custom-nodejs-app-bitnami-containers/deploy-custom-nodejs-app-bitnami-containers-2.png&#34; alt=&#34;Image in local Docker registry&#34;  /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Tagging image releases is a recommended practice. &lt;a href=&#34;https://docs.docker.com/engine/reference/commandline/tag/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Learn more about tags&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;step-4-test-the-docker-image&#34;&gt;Step 4: Test the Docker image&lt;/h2&gt;
&lt;p&gt;Run your new Docker image in a container to test it with the &lt;em&gt;docker run&lt;/em&gt; command. Replace the DOCKER_USERNAME placeholder in the command with your Docker account username.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;docker run -it -p 3000:3000 DOCKER_USERNAME/my-node-app:0.1.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This command runs the application in a container and makes port 3000 of the container accessible by binding it to port 3000 on the Docker host. With this, a user can access the application by browsing to port 3000 of the host.&lt;/p&gt;
&lt;p&gt;Here is what you should see as the container starts:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/containers/bitnami/deploy-custom-nodejs-app-bitnami-containers/deploy-custom-nodejs-app-bitnami-containers-3.png&#34; alt=&#34;Container startup&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;To test the application, browse to &lt;em&gt;http://localhost:3000&lt;/em&gt; (if the Docker host is the same machine) or &lt;em&gt;http://SERVER-IP:3000&lt;/em&gt; (if the Docker host is a different machine) and you should see this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/containers/bitnami/deploy-custom-nodejs-app-bitnami-containers/deploy-custom-nodejs-app-bitnami-containers-4.png&#34; alt=&#34;Application output&#34;  /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If the Docker host is a different machine, ensure that the host firewall is configured to allow access on port 3000.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The &lt;em&gt;-i&lt;/em&gt; and &lt;em&gt;-t&lt;/em&gt; options to &lt;em&gt;docker run&lt;/em&gt; allocate a terminal for the container process, while the &lt;em&gt;-p&lt;/em&gt; option specifies the container-host port binding. &lt;a href=&#34;https://docs.docker.com/engine/reference/run/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Learn more about the &lt;em&gt;docker run&lt;/em&gt; command and its options&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;step-5-publish-the-docker-image&#34;&gt;Step 5: Publish the Docker image&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;This step requires a Docker Hub account. In case you don&amp;rsquo;t already have one, &lt;a href=&#34;https://hub.docker.com/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;sign up on the Docker website&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;At this point, you have built, tagged and tested a Docker image containing your application code. To share it with others, you can upload the image to a public registry. A number of such registries are available, including &lt;a href=&#34;https://cloud.google.com/container-registry/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Google Container Registry&lt;/a&gt;, &lt;a href=&#34;https://quay.io/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Quay&lt;/a&gt; and others, but this guide will use &lt;a href=&#34;https://hub.docker.com/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Docker Hub&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To upload an image to Docker Hub, follow these steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Use the &lt;em&gt;docker login&lt;/em&gt; command to log in:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;docker login
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the &lt;em&gt;docker push&lt;/em&gt; command to push the image to your Docker Hub account, as shown below. Replace the DOCKER_USERNAME placeholder in the tag name with your Docker account username.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;docker push DOCKER_USERNAME/my-node-app:0.1.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Here&amp;rsquo;s what you should see:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/containers/bitnami/deploy-custom-nodejs-app-bitnami-containers/deploy-custom-nodejs-app-bitnami-containers-5.png&#34; alt=&#34;Image publishing process&#34;  /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Once published on Docker Hub, your image is publicly available by default.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Check that the image has been successfully uploaded to Docker Hub by searching for it using the &lt;em&gt;docker search&lt;/em&gt; command, as shown below:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;docker search DOCKER_USERNAME/my-node-app:0.1.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Once published on Docker Hub, other users can download your application and try it for themselves.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You can also delete images from Docker Hub using the Docker website.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;handling-updates&#34;&gt;Handling Updates&lt;/h2&gt;
&lt;p&gt;As you continue developing your application, you will inevitably want to release fresh Docker images. This might be to include new application features or bug fixes, or to use a new version of the base container that has the latest fixes. Doing this involves regenerating the application image and republishing it to the registry.&lt;/p&gt;
&lt;p&gt;To illustrate the process, let&amp;rsquo;s perform a code change in the example application and then release a fresh Docker image with the updated code. Follow these steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Change to the directory containing the application source code.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Edit the &lt;em&gt;server.js&lt;/em&gt; file and substitute the string &amp;ldquo;Hello world&amp;rdquo; with &amp;ldquo;Hello Mom&amp;rdquo;. Save the file.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Rebuild the image, tagging it as version 0.2.0. Replace the DOCKER_USERNAME placeholder in this and subsequent commands with your Docker account username.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;docker build -t DOCKER_USERNAME/my-node-app:0.2.0  .
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Confirm that the image was successfully built and added to the local registry:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;docker images | grep my-node-app
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;/images/guides/containers/bitnami/deploy-custom-nodejs-app-bitnami-containers/deploy-custom-nodejs-app-bitnami-containers-6.png&#34; alt=&#34;Revised image in local Docker registry&#34;  /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Test the new image:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;docker run -it -p 3000:3000 DOCKER_USERNAME/my-node-app:0.2.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Here&amp;rsquo;s what you should see when you access the application through your Web browser:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/containers/bitnami/deploy-custom-nodejs-app-bitnami-containers/deploy-custom-nodejs-app-bitnami-containers-7.png&#34; alt=&#34;Application output&#34;  /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Publish the new image to Docker Hub:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt; docker push DOCKER_USERNAME/my-node-app:0.2.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Follow these steps every time you want to update and republish your Docker image. If you are deploying the new image to a Kubernetes cluster, &lt;a href=&#34;https://docs.bitnami.com/tutorials/deploy-application-kubernetes-helm/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;read about rolling updates in our Kubernetes tutorial&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;useful-links&#34;&gt;Useful links&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bitnami/bitnami-docker-node&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami Node.js container image for Docker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://bitnami.com/containers&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Other Bitnami container images for Docker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://hub.docker.com&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Docker Hub&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.docker.com/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Docker documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: How to use Harbor Registry to Eliminate Docker Hub Rate Limits</title>
      
      <link>/guides/kubernetes/harbor-as-docker-proxy/</link>
      <pubDate>Fri, 22 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/guides/kubernetes/harbor-as-docker-proxy/</guid>
      <description>

        
        
&lt;div class=&#34;youtube-video-shortcode&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/KSH2Hzk-E7U&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;div align=&#34;center&#34;&gt;&lt;i&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=KSH2Hzk-E7U&amp;feature=youtu.be&#34;&gt;Watch Paul Walk through this guide on Tanzu.TV Shortcuts.&lt;/a&gt;&lt;/i&gt;&lt;/div&gt;
&lt;p&gt;On August 24 2020 &lt;a href=&#34;https://docker.com&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Docker&lt;/a&gt; announced they would be
implementing
&lt;a href=&#34;https://www.docker.com/blog/scaling-docker-to-serve-millions-more-developers-network-egress/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Rate Limits on the Docker Hub&lt;/a&gt;
and they were
&lt;a href=&#34;https://www.docker.com/blog/what-you-need-to-know-about-upcoming-docker-hub-rate-limiting/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;implemented on November 2 2020&lt;/a&gt;
thus ending our free ride of unlimited Docker Image pulls.&lt;/p&gt;
&lt;p&gt;Unless you&amp;rsquo;re a paid customer of Docker or very lucky you&amp;rsquo;ve probably started to
see errors like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ERROR: toomanyrequests: Too Many Requests.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Or&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;You have reached your pull rate limit. You may increase
the limit by authenticating and upgrading:
https://www.docker.com/increase-rate-limits.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This can be very frustrating, especially in Kubernetes where it might not be
apparent why your new Pod is just sitting there in a &lt;code&gt;Pending&lt;/code&gt; state. Imagine
this happening right as you need to scale your Deployments to serve a sudden
increase in traffic.&lt;/p&gt;
&lt;p&gt;This would be where a troll on Reddit (You know the sort, the kind that will
&amp;ldquo;&lt;a href=&#34;https://news.ycombinator.com/item?id=6277943&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;What you guys are referring to as Linux, is in fact, GNU/Linux&amp;rdquo;&lt;/a&gt;
at you would proclaim
&amp;ldquo;&lt;a href=&#34;https://www.whoownsmyavailability.com/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;You own your availability&lt;/a&gt;&amp;rdquo;. He&amp;rsquo;s not
wrong &amp;hellip; but also not helpful.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/harbor-as-docker-proxy/tweet-who-owns-your-availability.png&#34; alt=&#34;that twitter troll&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Thankfully the team developing the &lt;a href=&#34;https://goharbor.io/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Harbor Registry&lt;/a&gt; have
been hard at work to ensure that you can access the images you need without
downloading the whole internet to your server.&lt;/p&gt;
&lt;p&gt;There are actually two features in Harbor that will let us work around the rate
limits, Registry Replication, and Registry Proxy.&lt;/p&gt;
&lt;p&gt;Registry Replication allows you to replicate images between registries, whereas
Proxy lets you keep a local copy of images on an as-requested basis.&lt;/p&gt;
&lt;p&gt;In a production scenario you would probably look to Replication so that you can
be very specific about what Images to allow, however in a Development scenario
you might use Proxy-ing as you don&amp;rsquo;t necessarily know ahead of time what Images
you might need access to. Further using Proxy-ing can be really useful for a
home lab to cut down on internet traffic as you pull images.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll explore both options below.&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;Before you get started, you&amp;rsquo;ll need Harbor (ideally version 2.1.3 or newer)
installed somewhere. If you don&amp;rsquo;t already have it installed, we&amp;rsquo;ve made it
incredibly easy for your with our &lt;a href=&#34;../harbor-gs/&#34;&gt;Getting Started with Harbor&lt;/a&gt;
Guide.&lt;/p&gt;
&lt;p&gt;Once you have a Harbor registry installed, log into it&amp;rsquo;s Web UI as an Admin user.&lt;/p&gt;


&lt;div class=&#34;aside aside-info&#34;&gt;
    &lt;div class=&#34;aside aside-title&#34;&gt;
        &lt;i class=&#34;fas fa-exclamation-circle&#34;&gt;&lt;/i&gt;
        &lt;div&gt;Confirm Versions&lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#34;aside aside-content&#34;&gt;
    &lt;p&gt;Note: Docker has been rapidly changing both the Docker Hub and the Docker CLI,
this makes it difficult for Integrations such as Harbor&amp;rsquo;s replication / proxy
features to keep pace. To ensure the best chance of functionality, ensure you&amp;rsquo;re
using the versions stated in this document.&lt;/p&gt;

    &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;set-up-a-registry-endpoint&#34;&gt;Set up a Registry Endpoint&lt;/h2&gt;
&lt;p&gt;Whether doing replication or proxy, you need to configure Dockerhub as a
replication endpoint.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Go to &lt;strong&gt;Administration&lt;/strong&gt; -&amp;gt; &lt;strong&gt;Registries&lt;/strong&gt; and click the &lt;strong&gt;+ New Endpoint&lt;/strong&gt; button.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set the &lt;strong&gt;Provider&lt;/strong&gt; and &lt;strong&gt;Name&lt;/strong&gt; both to &lt;code&gt;Docker Hub&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You can leave the rest of the settings as default, unless you want access to
private images, in which case add in your &lt;strong&gt;Access ID&lt;/strong&gt; and &lt;strong&gt;Access
Secret&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/harbor-as-docker-proxy/create-endpoint.png&#34; alt=&#34;Create Endpoint&#34;  /&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Press the &lt;strong&gt;Test Connection&lt;/strong&gt; button and an a successful test hit &lt;strong&gt;OK&lt;/strong&gt; to save.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;create-a-dockerhub-proxy&#34;&gt;Create a Dockerhub Proxy&lt;/h2&gt;
&lt;p&gt;For more information about how Proxy Projects work, see the
&lt;a href=&#34;https://goharbor.io/docs/2.1.0/administration/configure-proxy-cache/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;official documentation&lt;/a&gt;.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Go to &lt;strong&gt;Projects&lt;/strong&gt; and click the &lt;strong&gt;+ New Project&lt;/strong&gt; button.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set &lt;strong&gt;Project Name&lt;/strong&gt; to &lt;code&gt;dockerhub-proxy&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set &lt;strong&gt;Access Level&lt;/strong&gt; to &lt;code&gt;Public&lt;/code&gt; (unless you intend to make it private and require login).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Leave &lt;strong&gt;Storage Quota&lt;/strong&gt; at the default &lt;code&gt;-1 GB&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set &lt;strong&gt;Proxy Cache&lt;/strong&gt; to &lt;code&gt;Docker Hub&lt;/code&gt; (the Endpoint we created earlier).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/harbor-as-docker-proxy/create-proxy-project.png&#34; alt=&#34;Create Proxy Project&#34;  /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Test the proxy is working with &lt;code&gt;docker pull&lt;/code&gt;:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ docker pull &amp;lt;url-of-registry&amp;gt;/dockerhub-proxy/library/ubuntu:20.04
20.04: Pulling from dockerhub-proxy/library/ubuntu
83ee3a23efb7: Pull &lt;span class=&#34;nb&#34;&gt;complete&lt;/span&gt;
db98fc6f11f0: Pull &lt;span class=&#34;nb&#34;&gt;complete&lt;/span&gt;
f611acd52c6c: Pull &lt;span class=&#34;nb&#34;&gt;complete&lt;/span&gt;
Digest: sha256:703218c0465075f4425e58fac086e09e1de5c340b12976ab9eb8ad26615c3715
Status: Downloaded newer image &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; harbor.aws.paulczar.wtf/dockerhub-proxy/library/ubuntu:20.04
harbor.aws.paulczar.wtf/dockerhub-proxy/library/ubuntu:20.04
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&#34;aside aside-warning&#34;&gt;
    &lt;div class=&#34;aside aside-title&#34;&gt;
        &lt;i class=&#34;fas fa-exclamation-circle&#34;&gt;&lt;/i&gt;
        &lt;div&gt;Content-Type Error&lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#34;aside aside-content&#34;&gt;
    &lt;p&gt;If you receive error
&lt;code&gt;Error response from daemon: missing or empty Content-Type header&lt;/code&gt;, you&amp;rsquo;ll need
to upgrade Harbor to version 2.1.3 as some changes in Docker have had downstream
ripple effects. Older versions of Docker will still work.&lt;/p&gt;

    &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;configure-docker-hub-replication&#34;&gt;Configure Docker Hub Replication&lt;/h2&gt;
&lt;h3 id=&#34;create-a-project-to-replicate-to&#34;&gt;Create a Project to replicate to&lt;/h3&gt;
&lt;p&gt;With Proxy-ing enabled, let&amp;rsquo;s now turn our eyes to Replication. This is where we
can surgically select which images we want to make available.&lt;/p&gt;
&lt;p&gt;For more information about how Replication works, see the
&lt;a href=&#34;https://goharbor.io/docs/2.1.0/administration/configuring-replication/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;official documentation&lt;/a&gt;.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Go to &lt;strong&gt;Projects&lt;/strong&gt; and click the &lt;strong&gt;+ New Project&lt;/strong&gt; button.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set &lt;strong&gt;Project Name&lt;/strong&gt; to &lt;code&gt;dockerhub-replica&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Leave all other settings as their defaults.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/harbor-as-docker-proxy/create-replica-project.png&#34; alt=&#34;Create Replica Project&#34;  /&gt;&lt;/p&gt;
&lt;h3 id=&#34;create-a-replication-rule&#34;&gt;Create a Replication Rule&lt;/h3&gt;
&lt;p&gt;Next we create a Replication Rule to determine the specific Images we want to
replicate. In this case we want only the &lt;code&gt;library/python:3.8.2-slim&lt;/code&gt; image. We
restrict this as Replication can quickly hit the Docker Hub rate limits.&lt;/p&gt;
&lt;p&gt;The resource filters support basic pattern recognition, so you could use
&lt;code&gt;library/**&lt;/code&gt; if you wanted to replicate all of the official images, however this
would quickly hit the rate limits.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Go to &lt;strong&gt;Administration&lt;/strong&gt; -&amp;gt; &lt;strong&gt;Replication&lt;/strong&gt; and click the &lt;strong&gt;+ New Replication Rule&lt;/strong&gt; button.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set &lt;strong&gt;Name&lt;/strong&gt; to &lt;code&gt;dockerhub-python-slim&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set &lt;strong&gt;Replication mode&lt;/strong&gt; to &lt;code&gt;Pull-based&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set &lt;strong&gt;Source registry&lt;/strong&gt; to &lt;code&gt;Docker Hub&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set &lt;strong&gt;Source resource filter&lt;/strong&gt; -&amp;gt; &lt;strong&gt;Name&lt;/strong&gt; to &lt;code&gt;library/python&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set &lt;strong&gt;Source resource filter&lt;/strong&gt; -&amp;gt; &lt;strong&gt;Tag&lt;/strong&gt; to &lt;code&gt;3.8.2-slim&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set &lt;strong&gt;Destination namespace&lt;/strong&gt; to &lt;code&gt;dockerhub-replica/python&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Leave the rest as their defaults.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/harbor-as-docker-proxy/create-replica-python.png&#34; alt=&#34;Create Replica for Python&#34;  /&gt;&lt;/p&gt;
&lt;h3 id=&#34;test-replication&#34;&gt;Test Replication&lt;/h3&gt;
&lt;p&gt;We chose manual replication (so that we don&amp;rsquo;t overwhelm the rate limits) so we
need to actually perform the replication step, and then validate that it worked.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Go to &lt;strong&gt;Administration&lt;/strong&gt; -&amp;gt; &lt;strong&gt;Replication&lt;/strong&gt; and click the
&lt;strong&gt;dockerhub-python-slim&lt;/strong&gt; item then click the &lt;strong&gt;Replicate&lt;/strong&gt; Button.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Harbor will kick off the replication and will show the attempt below in the
&lt;strong&gt;Executions&lt;/strong&gt; section. You can click on it for more details or logs, but for
now we&amp;rsquo;re just waiting for it to &lt;strong&gt;finish&lt;/strong&gt;.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Go to &lt;strong&gt;Projects&lt;/strong&gt; and select &lt;strong&gt;dockerhub-replica&lt;/strong&gt; then click
&lt;strong&gt;Repositories&lt;/strong&gt;. You should see &lt;code&gt;dockerhub-replica/python/python&lt;/code&gt; with at
least one Artifact. *To avoid this accidental redundancy in the name we
should have set &lt;strong&gt;Destination namespace&lt;/strong&gt; to &lt;code&gt;dockerhub-replica&lt;/code&gt; rather than
&lt;code&gt;dockerhub-replica/python&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/harbor-as-docker-proxy/replica-success.png&#34; alt=&#34;Successful replication&#34;  /&gt;&lt;/p&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;That&amp;rsquo;s it! We&amp;rsquo;ve learned how to replicate Docker images from Docker Hub using
both Proxy-ing and Replication. This can be applied for Harbor to Harbor
replication as well. It&amp;rsquo;s not uncommon to have one main Harbor registry as the
source of truth and then Replication to remote sites, and Proxy-ing to edge
sites.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: Getting Started with Using Helm to Deploy Apps on Kubernetes</title>
      
      <link>/guides/kubernetes/helm-gs/</link>
      <pubDate>Thu, 16 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/guides/kubernetes/helm-gs/</guid>
      <description>

        
        &lt;p&gt;&lt;a href=&#34;https://helm.sh&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Helm&lt;/a&gt; is a tool to help you define, install, and upgrade applications running on Kubernetes. For more information, be sure to check out &lt;a href=&#34;../helm-what-is/&#34;&gt;Helm: What Is It?&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In this guide you&amp;rsquo;ll deploy a simple application using Helm to a Kubernetes cluster.&lt;/p&gt;
&lt;h2 id=&#34;before-you-begin&#34;&gt;Before You Begin&lt;/h2&gt;
&lt;p&gt;There are a few things you need to do before getting started with Helm:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Have access to a Kubernetes cluster. If you don&amp;rsquo;t, you can use local options like &lt;a href=&#34;https://hub.docker.com/search?type=edition&amp;amp;offering=community&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Docker Desktop&lt;/a&gt; or &lt;a href=&#34;https://github.com/kubernetes/minikube&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Minikube&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Check out &lt;a href=&#34;https://kube.academy/courses/getting-started&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Getting Started with Kubernetes&lt;/a&gt; on KubeAcademy, particularly if you&amp;rsquo;ve never worked with Kubernetes before.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Follow the documentation for &lt;a href=&#34;https://helm.sh/docs/intro/install/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;installing Helm&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Helm leverages your local Kubernetes context to operate, so it will have whatever permissions the account you&amp;rsquo;re using for &lt;code&gt;kubectl&lt;/code&gt; does.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;If you read about Helm and come across references to &lt;code&gt;tiller&lt;/code&gt;, previous versions (before version 3) required an extra component installed on the Kubernetes cluster.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;initial-helm-setup&#34;&gt;Initial Helm Setup&lt;/h2&gt;
&lt;p&gt;You&amp;rsquo;re going to need a chart to deploy with Helm, so the easiest thing is to connect to a chart repository with the &lt;code&gt;helm repo&lt;/code&gt; command.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ helm repo add bitnami https://charts.bitnami.com/bitnami
&amp;quot;bitnami&amp;quot; has been added to your repositories
$ helm repo update
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the &amp;quot;bitnami&amp;quot; chart repository
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now that you have a repo connected, you need to see which charts you have available to deploy.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ helm search repo bitnami
NAME                            	CHART VERSION	APP VERSION            	DESCRIPTION
bitnami/bitnami-common          	0.0.8        	0.0.8                  	Chart with custom templates used in Bitnami cha...
bitnami/airflow                 	5.0.3        	1.10.9                 	Apache Airflow is a platform to programmaticall...
bitnami/apache                  	7.3.9        	2.4.41                 	Chart for Apache HTTP Server
---- Truncated ----
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You can see a whole list of charts, but the output above shows the first three. It shows the name of the chart, the versions, and the descriptions. As you&amp;rsquo;ll see, there&amp;rsquo;s both a chart version and an app version. That&amp;rsquo;s because a chart may be updated and changed separately from the underlying application it is deploying.&lt;/p&gt;
&lt;h2 id=&#34;time-to-deploy-a-chart-create-a-release&#34;&gt;Time to Deploy a Chart (Create a Release)&lt;/h2&gt;
&lt;p&gt;Now that you have Helm configured with a repo, you can deploy a chart. In Helm lingo that&amp;rsquo;s called &lt;em&gt;creating a release&lt;/em&gt;. In this example, you&amp;rsquo;ll deploy a pretty simple one, like nginx. You can supply a name for your app like you&amp;rsquo;re going to do here (my app) or you can use the &lt;code&gt;--generate-name&lt;/code&gt; CLI option to have Helm generate one for you.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;l&#34;&gt;$ helm install my-app bitnami/nginx&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;NAME&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;my-app&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;LAST DEPLOYED&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Mon Mar 9 07:37:28 2020&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;NAMESPACE&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;default&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;STATUS&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;deployed&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;REVISION&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;TEST SUITE&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;NOTES&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;Get the NGINX URL&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;NOTE&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;It may take a few minutes for the LoadBalancer IP to be available.&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;Watch the status with&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;kubectl get svc --namespace default -w my-app-nginx&amp;#39;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;export SERVICE_IP=$(kubectl get svc --namespace default my-app-nginx --template &amp;#34;{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}&amp;#34;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;echo &amp;#34;NGINX URL: http://$SERVICE_IP/&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;After your release is successfully created, you&amp;rsquo;ll see an output like this with the name, namespace, status, etc. The &lt;code&gt;NOTES&lt;/code&gt; section has specific information about your install; that&amp;rsquo;s because it&amp;rsquo;s generated by Helm using a template, too.&lt;/p&gt;
&lt;p&gt;You can see what was deployed by using &lt;code&gt;kubectl&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ kubectl get all
NAME                                READY   STATUS    RESTARTS   AGE
pod/my-app-nginx-655b5cfc8c-mfhcb   1/1     Running   0          2m38s

NAME                   TYPE           CLUSTER-IP   EXTERNAL-IP     PORT(S)                      AGE
service/my-app-nginx   LoadBalancer   10.0.2.51    104.197.x.x   80:30291/TCP,443:31827/TCP   2m38s

NAME                           READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/my-app-nginx   1/1     1            1           2m38s

NAME                                      DESIRED   CURRENT   READY   AGE
replicaset.apps/my-app-nginx-655b5cfc8c   1         1         1       2m38s
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You can see the external IP of your application listed, but if you follow the instructions in the notes, you should see the same as well.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ export SERVICE_IP=$(kubectl get svc --namespace default my-app-nginx --template &amp;quot;{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}&amp;quot;)

$ echo &amp;quot;NGINX URL: http://$SERVICE_IP/&amp;quot;
NGINX URL: http://104.197.x.x/

$ curl $SERVICE_IP
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
&amp;lt;style&amp;gt;
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
&amp;lt;/style&amp;gt;
&amp;lt;/head&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You can see which releases are deployed using &lt;code&gt;helm list&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME  	NAMESPACE	REVISION	UPDATED                            	STATUS  	CHART      	APP VERSION
my-app	default     	1       	2020-03-09 08:07:53.54657 -0400 EDT	deployed	nginx-5.1.9	1.16.1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;It shows all the relevant information. Anytime you update a release, the revision number will increment.&lt;/p&gt;
&lt;p&gt;You can clean up by removing the app with uninstall.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ helm uninstall my-app
release &amp;quot;my-app&amp;quot; uninstalled
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;changing-the-values&#34;&gt;Changing the Values&lt;/h2&gt;
&lt;p&gt;Now you have a working nginx app, but maybe you don&amp;rsquo;t want it exposed externally via a load balancer. You can delete this app and redeploy it with &lt;code&gt;ClusterIP&lt;/code&gt; instead of &lt;code&gt;LoadBalancer&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Helm charts have a set of default values; the ones for this chart can be seen in &lt;a href=&#34;https://github.com/bitnami/charts/tree/master/bitnami/nginx&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;its GitHub repository&lt;/a&gt;. If you look there, you&amp;rsquo;ll see the value you want to change is &lt;code&gt;service.type&lt;/code&gt;. You can now install that same chart using the &lt;code&gt;--set&lt;/code&gt; flag to configure it.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ helm install my-app bitnami/nginx --set service.type=ClusterIP
NAME: my-app
LAST DEPLOYED: Mon Mar 9 08:07:53 2020
NAMESPACE: default
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
Get the NGINX URL:

  echo &amp;quot;NGINX URL: http://127.0.0.1:8080/&amp;quot;
  kubectl port-forward --namespace blog svc/my-app-nginx 8080:80
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Notice how the &lt;code&gt;NOTES&lt;/code&gt; section changed? It&amp;rsquo;s a template, too. You can see &lt;a href=&#34;https://github.com/bitnami/charts/blob/master/bitnami/nginx/templates/NOTES.txt#L25&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;here&lt;/a&gt; that changing the service type changed the output.&lt;/p&gt;
&lt;p&gt;In this instance you supplied the value via the CLI, but you could have also put it into a &lt;code&gt;values.yaml&lt;/code&gt; file and used the &lt;code&gt;--values&lt;/code&gt; CLI option. This is a common practice for when you want to supply numerous values to the chart, and/or you want to keep track of what you&amp;rsquo;re deploying by checking the file into a version control system. The easiest way to get started with your values file is to download the default one from the chart repository, &lt;a href=&#34;https://github.com/bitnami/charts/blob/master/bitnami/nginx/values.yaml&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;like this one&lt;/a&gt; for the nginx chart you deployed. Any value you aren&amp;rsquo;t changing can be deleted from the file as it will be supplied by the default values.
If you wanted to do that to get the same results as above you&amp;rsquo;d create a &lt;code&gt;my-app-values.yaml&lt;/code&gt; file with these contents:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;c&#34;&gt;## NGINX Service properties&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;##&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;service&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;c&#34;&gt;## Service type&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;c&#34;&gt;##&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;type&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;ClusterIP&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The command to create the release would then be:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ helm install my-app bitnami/nginx --values my-app-values.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;upgrading-a-release&#34;&gt;Upgrading a Release&lt;/h2&gt;
&lt;p&gt;Anytime you want to change anything about a release—be it a configuration value for the chart, an upgrade to the chart itself, or the application version—you&amp;rsquo;ll run &lt;code&gt;helm upgrade&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For your nginx chart, you can try this by changing a configuration value. Currently the default image &lt;code&gt;pullPolicy&lt;/code&gt; for this chart is &lt;code&gt;IfNotPresent&lt;/code&gt;. You can change that to &lt;code&gt;Always&lt;/code&gt; via an upgrade.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;l&#34;&gt;$ helm upgrade my-app bitnami/nginx --set service.type=ClusterIP,image.pullPolicy=Always&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Release &amp;#34;my-app&amp;#34; has been upgraded. Happy Helming!&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;NAME&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;my-app&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;LAST DEPLOYED&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Wed Mar 11 13:50:05 2020&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;NAMESPACE&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;blog&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;STATUS&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;deployed&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;REVISION&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;TEST SUITE&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;NOTES&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;Get the NGINX URL&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;echo &amp;#34;NGINX URL: http://127.0.0.1:8080/&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;kubectl port-forward --namespace blog svc/my-app-nginx 8080:80&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You can see the revision has been incremented. If you get the nginx pod you can see the change of configuration:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;l&#34;&gt;$ kubectl get pod my-app-nginx-5bd7878597-pc8jp -o yaml&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;containers&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;docker.io/bitnami/nginx:1.16.1-debian-10-r46&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;imagePullPolicy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Always&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Why did you have to supply both &lt;code&gt;service.type&lt;/code&gt; and &lt;code&gt;image.pullPolicy&lt;/code&gt;? Because if you hadn&amp;rsquo;t supplied both, the service type would have tried to revert to the default.&lt;/p&gt;
&lt;h2 id=&#34;rollback&#34;&gt;Rollback&lt;/h2&gt;
&lt;p&gt;What happens if you didn&amp;rsquo;t want that change or it didn&amp;rsquo;t work the way you expected? Remember the revision of the releases? You can rollback to a previous revision with &lt;code&gt;helm rollback&lt;/code&gt;. If you want, you can do a &lt;code&gt;--dry-run&lt;/code&gt; first to see if the rollback would even work.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ helm rollback my-app 1 --dry-run
Rollback was a success! Happy Helming!
$ helm rollback my-app 1
Rollback was a success! Happy Helming!
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If you check the pod again, you&amp;rsquo;ll see &lt;code&gt;pullPolicy&lt;/code&gt; is set back to &lt;code&gt;IfNotPreset&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;get-helming&#34;&gt;Get Helming&lt;/h2&gt;
&lt;p&gt;If you&amp;rsquo;re ready to start trying to deploy more charts, there are a whole bunch of charts available in a number of different repositories. A current list of repositories in a Helm install might look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME    	URL
stable  	https://kubernetes-charts.storage.googleapis.com
jetstack	https://charts.jetstack.io
elastic 	https://helm.elastic.co
bitnami 	https://charts.bitnami.com/bitnami
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Happy Helming!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: Monitoring Containers at Scale with Wavefront</title>
      
      <link>/guides/kubernetes/monitoring-at-scale-wavefront/</link>
      <pubDate>Fri, 26 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/guides/kubernetes/monitoring-at-scale-wavefront/</guid>
      <description>

        
        &lt;p&gt;&lt;a href=&#34;https://tanzu.vmware.com/observability&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Tanzu Observability by Wavefront&lt;/a&gt; efficiently monitors cloud native operations at scale. It is a high-performance streaming analytics platform that supports 3D observability (metrics, histograms, traces/spans) and can scale to very high data ingestion rates and query loads. You can collect data from many services and sources across your entire application stack, and can look at details for earlier data collected by Wavefront.&lt;/p&gt;
&lt;p&gt;The Wavefront platform includes dashboards that give DevOps teams real-time visibility into the operation and performance of containerized applications and Kubernetes clusters. The dashboard displays data on the performance of microservices and resource utilization to help you identify issues, troubleshoot problems, and optimize applications. The data can, for example, help you make decisions about how and when to scale a container environment. In short, Wavefront is an observability platform with automated service discovery and full-stack analytics.&lt;/p&gt;
&lt;h2 id=&#34;monitoring-kubernetes&#34;&gt;Monitoring Kubernetes&lt;/h2&gt;
&lt;p&gt;The Wavefront service can measure, correlate, and analyze data across containers and Kubernetes clusters and can display various information, including metrics, histograms, span logs, traces and distributed tracing analysis.
Because Wavefront can correlate Kubernetes performance with the performance of applications, it can help you scale faster while maintaining high quality. With Wavefront you can:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;See a real-time, full-stack picture of your Kubernetes environment&lt;/li&gt;
&lt;li&gt;Find out about incidents earlier and solve them faster by drilling down into the data&lt;/li&gt;
&lt;li&gt;Understand and assess long-term trends&lt;/li&gt;
&lt;li&gt;Improve collaboration and visibility across teams&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Wavefront can also help you evaluate and tune the performance of microservices running on Kubernetes. For example, Wavefront can help you isolate and resolve microservices rate, error, and duration problems.&lt;/p&gt;
&lt;h2 id=&#34;keep-learning&#34;&gt;Keep Learning&lt;/h2&gt;
&lt;p&gt;Wavefront helps monitor and manage container deployments. It offers so much functionality that it’s often best to begin by investigating specific areas you are interested in. The video &lt;a href=&#34;https://tanzu.vmware.com/content/vmware-tanzu-observability-by-wavefront-videos/introduction-to-wavefront&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Introduction to Wavefront&lt;/a&gt; explains more about where and how Wavefront is being used. Watch &lt;a href=&#34;https://tanzu.vmware.com/content/vmware-tanzu-observability-by-wavefront-videos/wavefront-and-kubernetes&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Wavefront and Kubernetes&lt;/a&gt; to understand Kubernetes specifics. You can check out the &lt;a href=&#34;https://tanzu.vmware.com/content/vmware-tanzu-observability-by-wavefront-videos&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;complete library of Wavefront videos&lt;/a&gt; to find topics of interest. &lt;a href=&#34;https://docs.wavefront.com/index.html&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;The Wavefront docs&lt;/a&gt; also provide an exceptionally thorough introduction to the solution, with in-depth sections on Kubernetes, dashboards, alerts, tracing and more, including lots of video content for visual learners.&lt;/p&gt;
&lt;p&gt;If you are a Spring Boot developer &lt;a href=&#34;/guides/spring/spring-wavefront-gs/&#34;&gt;Wavefront for Spring Boot: Getting Started&lt;/a&gt; is a great starting point and explains how to take advantage of our free tier offer.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: Deploy Locally a Spring Boot Application Using Bitnami Containers</title>
      
      <link>/guides/containers/deploy-locally-spring-boot-application-docker/</link>
      <pubDate>Fri, 05 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/guides/containers/deploy-locally-spring-boot-application-docker/</guid>
      <description>

        
        &lt;p&gt;&lt;a href=&#34;https://bitnami.com/stacks/containers&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami containers&lt;/a&gt; provide you with a ready-to-go environment for the development framework of your choice backed by Bitnami. By selecting a Bitnami container for local development, you can save a lot of time in coding as well as you benefit from having always the latest and more secure application image.&lt;/p&gt;
&lt;p&gt;You can use Bitnami container images for directly deploying applications or as a base for creating your own customized images. The Bitnami Tomcat container image includes all dependencies and libraries you need to deploy an application. That way, you can get productive immediately and focus only on what you love: coding.&lt;/p&gt;
&lt;p&gt;In this tutorial, you will learn how to use a &lt;a href=&#34;https://github.com/bitnami/bitnami-docker-tomcat&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami Tomcat container image&lt;/a&gt; as a framework for deploying locally a sample &lt;a href=&#34;https://spring.io/projects/spring-boot&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Spring Boot application&lt;/a&gt; that uses the &lt;a href=&#34;https://github.com/bitnami/bitnami-docker-mariadb&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami MariaDB container image&lt;/a&gt; as a database.&lt;/p&gt;
&lt;h2 id=&#34;assumptions-and-prerequisites&#34;&gt;Assumptions and Prerequisites&lt;/h2&gt;
&lt;p&gt;This guide makes the following assumptions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You have basic knowledge of &lt;a href=&#34;https://www.docker.com/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Docker&lt;/a&gt; containers.&lt;/li&gt;
&lt;li&gt;You have a Docker environment installed and configured. &lt;a href=&#34;https://docs.docker.com/install/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Learn more about installing Docker&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;You have a Docker Hub account. &lt;a href=&#34;https://hub.docker.com/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Register for a free account&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;You have Apache Maven already installed. &lt;a href=&#34;https://maven.apache.org/install.html&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Refer to the official Apache Maven Project documentation&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;You have Curl already installed. &lt;a href=&#34;https://curl.haxx.se/dlwiz/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Download and install the latest version of Curl for your operating system&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The following are the steps you will complete in this guide:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Step 1: Obtain the application source code&lt;/li&gt;
&lt;li&gt;Step 2: Create a Dockerfile&lt;/li&gt;
&lt;li&gt;Step 3: Build the Docker image&lt;/li&gt;
&lt;li&gt;Step 4: Create a &lt;em&gt;docker-compose.yml&lt;/em&gt; file to configure application services&lt;/li&gt;
&lt;li&gt;Step 5: Test the Docker image and your custom application&lt;/li&gt;
&lt;li&gt;Step 6: Publish the Docker image&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;step-1-obtain-the-application-source-code&#34;&gt;Step 1: Obtain the application source code&lt;/h2&gt;
&lt;p&gt;To begin the process, ensure that you have access to the application source code. This tutorial uses a sample &lt;a href=&#34;https://github.com/spring-guides/gs-accessing-data-mysql&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Spring Boot application&lt;/a&gt; which has been modified to replace its database with the Bitnami MariaDB container image and to be packaged as a WAR file. To get the resulting package:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Clone the sample repository as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;git clone https://github.com/bitnami/tutorials.git
cd tutorials/spring-boot-app
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This will clone the sample repository. The &lt;em&gt;gs-mysql-data-0.1.0.war&lt;/em&gt; file is located in the &lt;em&gt;spring-boot-app&lt;/em&gt; subdirectory.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Learn more about the changes done in the application source code by checking check the &lt;a href=&#34;https://github.com/bitnami/tutorials/tree/master/spring-boot-app/README.md&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;repository README file&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;step-2-create-a-dockerfile&#34;&gt;Step 2: Create a Dockerfile&lt;/h2&gt;
&lt;p&gt;A &lt;a href=&#34;https://docs.docker.com/engine/reference/builder/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;em&gt;Dockerfile&lt;/em&gt;&lt;/a&gt; is similar to a recipe: it contains all the ingredients needed to create a Docker image. Each line of the file represents a separate step and contains the instructions for the container to build the application.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Create a &lt;em&gt;Dockerfile&lt;/em&gt; in the sample &lt;em&gt;spring-boot-app&lt;/em&gt; repository to build your application with the Bitnami Tomcat container image providing the Tomcat infrastructure for the resulting application image. It should include the following content:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;FROM bitnami/tomcat:9.0
COPY gs-mysql-data-0.1.0.war /opt/bitnami/tomcat/webapps_default/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This &lt;em&gt;Dockerfile&lt;/em&gt; consists of one stage with two instructions for using the infrastructure &lt;em&gt;bitnami/tomcat:9.0&lt;/em&gt; image and to copy the application source at build time:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The FROM instruction kicks off the Dockerfile and specifies the base image to use. In this case, &lt;em&gt;bitnami/tomcat:9.0&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;The COPY instruction copies the source code from the current directory on the host to the &lt;em&gt;/webapps_default&lt;/em&gt; directory in the image. Any WAR file copied into that directory will be automatically deployed by Tomcat at bootup time.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;step-3-build-the-docker-image&#34;&gt;Step 3: Build the Docker image&lt;/h2&gt;
&lt;p&gt;Once the &lt;em&gt;Dockerfile&lt;/em&gt; is created, it is time to build the docker image only by executing the &lt;em&gt;docker build&lt;/em&gt; command. Execute the command below in the same directory where the &lt;em&gt;Dockerfile&lt;/em&gt; is located, in this case, in the &lt;em&gt;spring-boot-app&lt;/em&gt; directory of the cloned repository. Remember to replace the DOCKER_USERNAME placeholder with your Docker account username.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;docker build -t DOCKER_USERNAME/spring-java-app .
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This will create an image named &lt;em&gt;spring-java-app&lt;/em&gt;. Here is an example of the output you should see during the build process:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/containers/bitnami/deploy-locally-spring-boot-application-docker/spring-java-app.png&#34; alt=&#34;Build the Docker image&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Once the build process is complete, use the &lt;em&gt;docker image&lt;/em&gt; command to verify that the image has been added to your local repository.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;docker image ls DOCKER_USERNAME/spring-java-app
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;step-4-create-a-docker-composeyml-file-to-configure-application-services&#34;&gt;Step 4: Create a &lt;em&gt;docker-compose.yml&lt;/em&gt; file to configure application services&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.docker.com/compose/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Compose&lt;/a&gt; is a tool for defining and running applications with multiple containers in Docker. The &lt;em&gt;docker-compose.yml&lt;/em&gt; file is used to define the configuration of your application&amp;rsquo;s services. It includes the specification of the application&amp;rsquo;s service dependencies such as databases, queues, caches, etc. After defining them in the &lt;em&gt;docker-compose.yml&lt;/em&gt; file you are able to create and start one or more containers with a single command: &lt;em&gt;docker-compose up&lt;/em&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create a &lt;em&gt;docker-compose.yml&lt;/em&gt; file that contains the following content. Remember to replace the DOCKER_USERNAME placeholder with your Docker account username:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;version: &amp;#39;2&amp;#39;

services:
  mariadb:
    image: &amp;#39;bitnami/mariadb:10.3&amp;#39;
    environment:
      - ALLOW_EMPTY_PASSWORD=yes
      - MARIADB_DATABASE=db_example
      - MARIADB_USER=springuser
      - MARIADB_PASSWORD=ThePassword
    myapp:
    image: &amp;#39;DOCKER_USERNAME/spring-java-app&amp;#39;
    environment:
      - &amp;#39;SPRING_APPLICATION_JSON={&amp;#34;spring&amp;#34;: {&amp;#34;datasource&amp;#34;:{&amp;#34;url&amp;#34;: &amp;#34;jdbc:mysql://mariadb:3306/db_example&amp;#34;, &amp;#34;username&amp;#34;: &amp;#34;springuser&amp;#34;, &amp;#34;password&amp;#34;: &amp;#34;ThePassword&amp;#34;}}}&amp;#39;
    depends_on:
      - mariadb
    ports:
     - &amp;#39;8080:8080&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;The application environment variable set in this file allows you to use any MySQL database with any credentials at deployment time. This, makes your image secure for production environments.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;The ALLOW_EMPTY_PASSWORD parameter is set as &amp;ldquo;yes&amp;rdquo; in this &lt;em&gt;docker-compose.yml&lt;/em&gt; file since the application is built for development purposes. That value is highly discouraged for production. Remember to secure your deployments by setting a password in production environments.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;step-5-test-the-docker-image-and-your-custom-application&#34;&gt;Step 5: Test the Docker image and your custom application&lt;/h2&gt;
&lt;p&gt;Run your new Docker image in a container to test it with the &lt;em&gt;docker-compose up&lt;/em&gt; command.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;docker-compose up
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This will create all the containers and volumes both for your application and the database. Now, it is time to test if the application works fine by calling the API endpoint using &lt;em&gt;Curl&lt;/em&gt;. Follow these instructions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Open a new terminal in your local system and execute the following command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;curl &amp;#39;localhost:8080/gs-mysql-data-0.1.0/demo/all&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The output should be an empty array &amp;ldquo;[]&amp;rdquo; that means that there is no data present in the database.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Let&amp;rsquo;s insert some data in the database by executing:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;curl &amp;#39;localhost:8080/gs-mysql-data-0.1.0/demo/add?name=First&amp;amp;email=someemail@someemailprovider.com&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Query the application again to check if the data is present in the database:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;curl &amp;#39;localhost:8080/gs-mysql-data-0.1.0/demo/all&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now, you should get an output similar to this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;[{&amp;#34;id&amp;#34;:1, &amp;#34;name&amp;#34;:&amp;#34;First&amp;#34;, &amp;#34;email&amp;#34;:&amp;#34;someemail@someemailprovider.com&amp;#34;}]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Congratulations! You have your Spring Boot application running locally and ready to use!&lt;/p&gt;
&lt;h2 id=&#34;step-6-publish-the-docker-image&#34;&gt;Step 6: Publish the Docker image&lt;/h2&gt;
&lt;p&gt;Now that your Docker image is built and contains your application code, you can upload it into a public registry. This tutorial uses &lt;a href=&#34;https://hub.docker.com/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Docker Hub&lt;/a&gt;, but you can select one of your own choice such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.google.com/container-registry/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Google Container Registry&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://aws.amazon.com/ecr/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Amazon EC2 Container Registry&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://azure.microsoft.com/en-us/services/container-registry/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Azure container Registry&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://quay.io/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Quay&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To upload the image to Docker Hub, follow the steps below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Log in to Docker Hub:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;docker login
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Push the image to your Docker Hub account. Replace the DOCKER_USERNAME placeholder with the username of your Docker Hub account and &lt;em&gt;my-custom-app:latest&lt;/em&gt; with the name and the version of your Docker image:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;docker push DOCKER_USERNAME/my-custom-app:latest
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Confirm that you see the image in your Docker Hub repositories dashboard.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;useful-links&#34;&gt;Useful links&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://spring.io/projects/spring-boot&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Spring Boot official site&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bitnami/tutorials&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami tutorials repository&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bitnami/bitnami-docker-tomcat&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami Tomcat container image&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.docker.com/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Docker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://hub.docker.com/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Docker Hub&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://bitnami.com/containers&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami containers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.bitnami.com/tutorials/deploy-java-application-kubernetes-helm&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Deploy a Java application on Kubernetes with Helm&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: Creating Your First Helm Chart</title>
      
      <link>/guides/kubernetes/create-helm-chart/</link>
      <pubDate>Fri, 05 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/guides/kubernetes/create-helm-chart/</guid>
      <description>

        
        &lt;p&gt;So, you&amp;rsquo;ve got your &lt;a href=&#34;https://docs.bitnami.com/kubernetes/get-started-kubernetes/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Kubernetes cluster up and running&lt;/a&gt; and &lt;a href=&#34;https://docs.bitnami.com/kubernetes/get-started-kubernetes/#step-4-install-helm&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;set up Helm v3.x&lt;/a&gt;, but how do you run your applications on it? This guide walks you through the process of creating your first ever chart, explaining what goes inside these packages and the tools you use to develop them. By the end of it you should have an understanding of the advantages of using Helm to deliver your own applications to your cluster.&lt;/p&gt;
&lt;p&gt;For a typical cloud-native application with a 3-tier architecture, the diagram below illustrates how it might be described in terms of &lt;a href=&#34;http://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Kubernetes objects&lt;/a&gt;. In this example, each tier consists of a &lt;a href=&#34;http://kubernetes.io/docs/user-guide/deployments/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Deployment&lt;/a&gt; and &lt;a href=&#34;http://kubernetes.io/docs/user-guide/services/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Service&lt;/a&gt; object, and may additionally define &lt;a href=&#34;http://kubernetes.io/docs/user-guide/configmap/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;ConfigMap&lt;/a&gt; or &lt;a href=&#34;http://kubernetes.io/docs/user-guide/secrets/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Secret&lt;/a&gt; objects. Each of these objects are typically defined in separate YAML files, and are fed into the &lt;em&gt;kubectl&lt;/em&gt; command line tool.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/create-helm-chart/diagrams/three-tier-kubernetes-architecture.png&#34; alt=&#34;3-tier application architecture on Kubernetes&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;A Helm chart encapsulates each of these YAML definitions, provides a mechanism for configuration at deploy-time and allows you to define metadata and documentation that might be useful when sharing the package. Helm can be useful in different scenarios:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Find and use popular software packaged as Kubernetes charts&lt;/li&gt;
&lt;li&gt;Share your own applications as Kubernetes charts&lt;/li&gt;
&lt;li&gt;Create reproducible builds of your Kubernetes applications&lt;/li&gt;
&lt;li&gt;Intelligently manage your Kubernetes object definitions&lt;/li&gt;
&lt;li&gt;Manage releases of Helm packages&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s explore the second and third scenarios by creating our first chart.&lt;/p&gt;
&lt;h2 id=&#34;step-1-generate-your-first-chart&#34;&gt;Step 1: Generate your first chart&lt;/h2&gt;
&lt;p&gt;The best way to get started with a new chart is to use the &lt;em&gt;helm create&lt;/em&gt; command to scaffold out an example we can build on. Use this command to create a new chart named &lt;em&gt;mychart&lt;/em&gt; in a new directory:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;helm create mychart
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Helm will create a new directory in your project called &lt;em&gt;mychart&lt;/em&gt; with the structure shown below. Let&amp;rsquo;s navigate our new chart (pun intended) to find out how it works.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mychart
|-- Chart.yaml
|-- charts
|-- templates
|   |-- NOTES.txt
|   |-- _helpers.tpl
|   |-- deployment.yaml
|   |-- ingress.yaml
|   `-- service.yaml
`-- values.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;templates&#34;&gt;Templates&lt;/h3&gt;
&lt;p&gt;The most important piece of the puzzle is the &lt;em&gt;templates/&lt;/em&gt; directory. This is where Helm finds the YAML definitions for your Services, Deployments and other Kubernetes objects. If you already have definitions for your application, all you need to do is replace the generated YAML files for your own. What you end up with is a working chart that can be deployed using the &lt;em&gt;helm install&lt;/em&gt; command.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s worth noting however, that the directory is named &lt;em&gt;templates&lt;/em&gt;, and Helm runs each file in this directory through a &lt;a href=&#34;https://golang.org/pkg/text/template/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Go template&lt;/a&gt; rendering engine. Helm extends the template language, adding a number of utility functions for writing charts. Open the &lt;em&gt;service.yaml&lt;/em&gt; file to see what this looks like:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
name: {{ template &amp;quot;fullname&amp;quot; . }}
labels:
    chart: &amp;quot;{{ .Chart.Name }}-{{ .Chart.Version | replace &amp;quot;+&amp;quot; &amp;quot;_&amp;quot; }}&amp;quot;
spec:
type: {{ .Values.service.type }}
ports:
- port: {{ .Values.service.externalPort }}
    targetPort: {{ .Values.service.internalPort }}
    protocol: TCP
    name: {{ .Values.service.name }}
selector:
    app: {{ template &amp;quot;fullname&amp;quot; . }}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This is a basic Service definition using templating. When deploying the chart, Helm will generate a definition that will look a lot more like a valid Service. We can do a dry-run of a &lt;em&gt;helm install&lt;/em&gt; and enable debug to inspect the generated definitions:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;helm install --dry-run --debug ./mychart
...
# Source: mychart/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
name: pouring-puma-mychart
labels:
    chart: &amp;quot;mychart-0.1.0&amp;quot;
spec:
type: ClusterIP
ports:
- port: 80
    targetPort: 80
    protocol: TCP
    name: nginx
selector:
    app: pouring-puma-mychart
...
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;values&#34;&gt;Values&lt;/h4&gt;
&lt;p&gt;The template in &lt;em&gt;service.yaml&lt;/em&gt; makes use of the Helm-specific objects &lt;em&gt;.Chart&lt;/em&gt; and &lt;em&gt;.Values.&lt;/em&gt;. The former provides metadata about the chart to your definitions such as the name, or version. The latter &lt;em&gt;.Values&lt;/em&gt; object is a key element of Helm charts, used to expose configuration that can be set at the time of deployment. The defaults for this object are defined in the &lt;em&gt;values.yaml&lt;/em&gt; file. Try changing the default value for &lt;em&gt;service.internalPort&lt;/em&gt; and execute another dry-run, you should find that the &lt;em&gt;targetPort&lt;/em&gt; in the Service and the &lt;em&gt;containerPort&lt;/em&gt; in the Deployment changes. The &lt;em&gt;service.internalPort&lt;/em&gt; value is used here to ensure that the Service and Deployment objects work together correctly. The use of templating can greatly reduce boilerplate and simplify your definitions.&lt;/p&gt;
&lt;p&gt;If a user of your chart wanted to change the default configuration, they could provide overrides directly on the command-line:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;helm install --dry-run --debug ./mychart --set service.internalPort&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;8080&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;For more advanced configuration, a user can specify a YAML file containing overrides with the &lt;em&gt;--values&lt;/em&gt; option.&lt;/p&gt;
&lt;h4 id=&#34;helpers-and-other-functions&#34;&gt;Helpers and other functions&lt;/h4&gt;
&lt;p&gt;The &lt;em&gt;service.yaml&lt;/em&gt; template also makes use of partials defined in &lt;em&gt;_helpers.tpl&lt;/em&gt;, as well as functions like &lt;em&gt;replace&lt;/em&gt;. The &lt;a href=&#34;https://helm.sh/docs/chart_template_guide/getting_started/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Helm documentation&lt;/a&gt; has a deeper walkthrough of the templating language, explaining how functions, partials and flow control can be used when developing your chart.&lt;/p&gt;
&lt;h3 id=&#34;documentation&#34;&gt;Documentation&lt;/h3&gt;
&lt;p&gt;Another useful file in the &lt;em&gt;templates/&lt;/em&gt; directory is the &lt;em&gt;NOTES.txt&lt;/em&gt; file. This is a templated, plaintext file that gets printed out after the chart is successfully deployed. As we&amp;rsquo;ll see when we deploy our first chart, this is a useful place to briefly describe the next steps for using a chart. Since &lt;em&gt;NOTES.txt&lt;/em&gt; is run through the template engine, you can use templating to print out working commands for obtaining an IP address, or getting a password from a Secret object.&lt;/p&gt;
&lt;h3 id=&#34;metadata&#34;&gt;Metadata&lt;/h3&gt;
&lt;p&gt;As mentioned earlier, a Helm chart consists of metadata that is used to help describe what the application is, define constraints on the minimum required Kubernetes and/or Helm version and manage the version of your chart. All of this metadata lives in the &lt;em&gt;Chart.yaml&lt;/em&gt; file. The &lt;a href=&#34;https://helm.sh/docs/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Helm documentation&lt;/a&gt; describes the different fields for this file.&lt;/p&gt;
&lt;h2 id=&#34;step-2-deploy-your-first-chart&#34;&gt;Step 2: Deploy your first chart&lt;/h2&gt;
&lt;p&gt;The chart you generated in the previous step is set up to run an NGINX server exposed via a Kubernetes Service. By default, the chart will create a &lt;em&gt;ClusterIP&lt;/em&gt; type Service, so NGINX will only be exposed internally in the cluster. To access it externally, we&amp;rsquo;ll use the &lt;em&gt;NodePort&lt;/em&gt; type instead. We can also set the name of the Helm release so we can easily refer back to it. Let&amp;rsquo;s go ahead and deploy our NGINX chart using the &lt;em&gt;helm install&lt;/em&gt; command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;helm install example ./mychart --set service.type&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;NodePort
NAME:   example
LAST DEPLOYED: Tue May  &lt;span class=&#34;m&#34;&gt;2&lt;/span&gt; 20:03:27 &lt;span class=&#34;m&#34;&gt;2017&lt;/span&gt;
NAMESPACE: default
STATUS: DEPLOYED

RESOURCES:
&lt;span class=&#34;o&#34;&gt;==&lt;/span&gt;&amp;gt; v1/Service
NAME             CLUSTER-IP  EXTERNAL-IP  PORT&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;S&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;       AGE
example-mychart  10.0.0.24   &amp;lt;nodes&amp;gt;      80:30630/TCP  &lt;span class=&#34;nv&#34;&gt;0s&lt;/span&gt;

&lt;span class=&#34;o&#34;&gt;==&lt;/span&gt;&amp;gt; v1beta1/Deployment
NAME             DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
example-mychart  &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;        &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;        &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;           &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;          0s

NOTES:
1. Get the application URL by running these commands:
&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;NODE_PORT&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;$(&lt;/span&gt;kubectl get --namespace default -o &lt;span class=&#34;nv&#34;&gt;jsonpath&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;{.spec.ports[0].nodePort}&amp;#34;&lt;/span&gt; services example-mychart&lt;span class=&#34;k&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;NODE_IP&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;$(&lt;/span&gt;kubectl get nodes --namespace default -o &lt;span class=&#34;nv&#34;&gt;jsonpath&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;{.items[0].status.addresses[0].address}&amp;#34;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; http://&lt;span class=&#34;nv&#34;&gt;$NODE_IP&lt;/span&gt;:&lt;span class=&#34;nv&#34;&gt;$NODE_PORT&lt;/span&gt;/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The output of &lt;em&gt;helm install&lt;/em&gt; displays a handy summary of the state of the release, what objects were created, and the rendered &lt;em&gt;NOTES.txt&lt;/em&gt; file to explain what to do next. Run the commands in the output to get a URL to access the NGINX service and pull it up in your browser.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/create-helm-chart/nginx-server.png&#34; alt=&#34;nginx server default page&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;If all went well, you should see the NGINX welcome page as shown above. Congratulations! You&amp;rsquo;ve just deployed your very first service packaged as a Helm chart!&lt;/p&gt;
&lt;h2 id=&#34;step-3-modify-chart-to-deploy-a-custom-service&#34;&gt;Step 3: Modify chart to deploy a custom service&lt;/h2&gt;
&lt;p&gt;The generated chart creates a Deployment object designed to run an image provided by the default values. This means all we need to do to run a different service is to change the referenced image in &lt;em&gt;values.yaml&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;We are going to update the chart to run a &lt;a href=&#34;https://github.com/prydonius/todomvc/tree/master/examples/react&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;todo list application&lt;/a&gt; available on &lt;a href=&#34;https://hub.docker.com/r/prydonius/todo/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Docker Hub&lt;/a&gt;. In &lt;em&gt;values.yaml&lt;/em&gt;, update the image keys to reference the todo list image:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;image:
repository: prydonius/todo
tag: 1.0.0
pullPolicy: IfNotPresent
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;As you develop your chart, it&amp;rsquo;s a good idea to run it through the linter to ensure you&amp;rsquo;re following best practices and that your templates are well-formed. Run the &lt;em&gt;helm lint&lt;/em&gt; command to see the linter in action:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;helm lint ./mychart
&lt;span class=&#34;o&#34;&gt;==&lt;/span&gt;&amp;gt; Linting ./mychart
&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;INFO&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; Chart.yaml: icon is recommended

&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; chart&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;s&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt; linted, no failures
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The linter didn&amp;rsquo;t complain about any major issues with the chart, so we&amp;rsquo;re good to go. However, as an example, here is what the linter might output if you managed to get something wrong:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;malformed&amp;#34;&lt;/span&gt; &amp;gt; mychart/values.yaml
helm lint ./mychart
&lt;span class=&#34;o&#34;&gt;==&lt;/span&gt;&amp;gt; Linting mychart
&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;INFO&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; Chart.yaml: icon is recommended
&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;ERROR&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; values.yaml: unable to parse YAML
    error converting YAML to JSON: yaml: line 34: could not find expected &lt;span class=&#34;s1&#34;&gt;&amp;#39;:&amp;#39;&lt;/span&gt;

Error: &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; chart&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;s&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt; linted, &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; chart&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;s&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt; failed
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This time, the linter tells us that it was unable to parse my &lt;em&gt;values.yaml&lt;/em&gt; file correctly. With the line number hint, we can easily find the fix the bug we introduced.&lt;/p&gt;
&lt;p&gt;Now that the chart is once again valid, run &lt;em&gt;helm install&lt;/em&gt; again to deploy the todo list application:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;helm install example2 ./mychart --set service.type&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;NodePort
NAME:   example2
LAST DEPLOYED: Wed May  &lt;span class=&#34;m&#34;&gt;3&lt;/span&gt; 12:10:03 &lt;span class=&#34;m&#34;&gt;2017&lt;/span&gt;
NAMESPACE: default
STATUS: DEPLOYED

RESOURCES:
&lt;span class=&#34;o&#34;&gt;==&lt;/span&gt;&amp;gt; v1/Service
NAME              CLUSTER-IP  EXTERNAL-IP  PORT&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;S&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;       AGE
example2-mychart  10.0.0.78   &amp;lt;nodes&amp;gt;      80:31381/TCP  &lt;span class=&#34;nv&#34;&gt;0s&lt;/span&gt;

&lt;span class=&#34;o&#34;&gt;==&lt;/span&gt;&amp;gt; apps/v1/Deployment
NAME              DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
example2-mychart  &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;        &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;        &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;           &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;          0s


NOTES:
1. Get the application URL by running these commands:
&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;NODE_PORT&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;$(&lt;/span&gt;kubectl get --namespace default -o &lt;span class=&#34;nv&#34;&gt;jsonpath&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;{.spec.ports[0].nodePort}&amp;#34;&lt;/span&gt; services example2-mychart&lt;span class=&#34;k&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;NODE_IP&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;$(&lt;/span&gt;kubectl get nodes --namespace default -o &lt;span class=&#34;nv&#34;&gt;jsonpath&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;{.items[0].status.addresses[0].address}&amp;#34;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; http://&lt;span class=&#34;nv&#34;&gt;$NODE_IP&lt;/span&gt;:&lt;span class=&#34;nv&#34;&gt;$NODE_PORT&lt;/span&gt;/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once again, we can run the commands in the NOTES to get a URL to access our application.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/create-helm-chart/todo-list-app.png&#34; alt=&#34;Todo List Application&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;If you have already built containers for your applications, you can run them with your chart by updating the default values or the &lt;em&gt;Deployment&lt;/em&gt; template. Check out the Bitnami Docs for an &lt;a href=&#34;https://docs.bitnami.com/tutorials/deploy-custom-nodejs-app-bitnami-containers/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;introduction to containerizing your applications&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;step-4-package-it-all-up-to-share&#34;&gt;Step 4: Package it all up to share&lt;/h2&gt;
&lt;p&gt;So far in this tutorial, we&amp;rsquo;ve been using the &lt;em&gt;helm install&lt;/em&gt; command to install a local, unpacked chart. However, if you are looking to share your charts with your team or the community, your consumers will typically install the charts from a tar package. We can use &lt;em&gt;helm package&lt;/em&gt; to create the tar package:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;helm package ./mychart
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Helm will create a &lt;em&gt;mychart-0.1.0.tgz&lt;/em&gt; package in our working directory, using the name and version from the metadata defined in the &lt;em&gt;Chart.yaml&lt;/em&gt; file. A user can install from this package instead of a local directory by passing the package as the parameter to &lt;em&gt;helm install&lt;/em&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;helm install example3 mychart-0.1.0.tgz --set service.type&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;NodePort
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;repositories&#34;&gt;Repositories&lt;/h3&gt;
&lt;p&gt;In order to make it much easier to share packages, Helm has built-in support for installing packages from an HTTP server. Helm reads a repository index hosted on the server which describes what chart packages are available and where they are located.&lt;/p&gt;
&lt;p&gt;We can use the &lt;em&gt;helm serve&lt;/em&gt; command to run a local repository to serve our chart.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;helm serve
Regenerating index. This may take a moment.
Now serving you on 127.0.0.1:8879
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now, in a separate terminal window, you should be able to see your chart in the local repository and install it from there:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;helm search &lt;span class=&#34;nb&#34;&gt;local&lt;/span&gt;
NAME         	VERSION	DESCRIPTION
local/mychart	0.1.0  	A Helm chart &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; Kubernetes

helm install example4 local/mychart --set service.type&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;NodePort
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To set up a remote repository you can follow the guide in the &lt;a href=&#34;https://github.com/kubernetes/helm/blob/master/docs/chart_repository.md&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Helm documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;dependencies&#34;&gt;Dependencies&lt;/h2&gt;
&lt;p&gt;As the applications your packaging as charts increase in complexity, you might find you need to pull in a dependency such as a database. Helm allows you to specify sub-charts that will be created as part of the same release. To define a dependency, create a &lt;em&gt;requirements.yaml&lt;/em&gt; file in the chart root directory:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cat &amp;gt; ./mychart/requirements.yaml &lt;span class=&#34;s&#34;&gt;&amp;lt;&amp;lt;EOF
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;dependencies:
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;- name: mariadb
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;version: 0.6.0
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;repository: https://charts.helm.sh/stable
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;EOF&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Much like a runtime language dependency file (such as Python&amp;rsquo;s &lt;em&gt;requirements.txt&lt;/em&gt;), the &lt;em&gt;requirements.yaml&lt;/em&gt; file allows you to manage your chart&amp;rsquo;s dependencies and their versions. When updating dependencies, a lockfile is generated so that subsequent fetching of dependencies use a known, working version. Run the following command to pull in the MariaDB dependency we defined:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;helm dep update ./mychart
Hang tight &lt;span class=&#34;k&#34;&gt;while&lt;/span&gt; we grab the latest from your chart repositories...
...Unable to get an update from the &lt;span class=&#34;s2&#34;&gt;&amp;#34;local&amp;#34;&lt;/span&gt; chart repository &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;http://127.0.0.1:8879/charts&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;:
    Get http://127.0.0.1:8879/charts/index.yaml: dial tcp 127.0.0.1:8879: getsockopt: connection refused
...Successfully got an update from the &lt;span class=&#34;s2&#34;&gt;&amp;#34;bitnami&amp;#34;&lt;/span&gt; chart repository
...Successfully got an update from the &lt;span class=&#34;s2&#34;&gt;&amp;#34;incubator&amp;#34;&lt;/span&gt; chart repository
Update Complete. *Happy Helming!*
Saving &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; charts
Downloading mariadb from repo
$ ls ./mychart/charts
mariadb-0.6.0.tgz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Helm has found a matching version in the &lt;em&gt;bitnami&lt;/em&gt; repository and has fetched it into my chart&amp;rsquo;s sub-chart directory. Now when we go and install the chart, we&amp;rsquo;ll see that MariaDB&amp;rsquo;s objects are created too:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;helm install example5 ./mychart --set service.type&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;NodePort
NAME:   example5
LAST DEPLOYED: Wed May  &lt;span class=&#34;m&#34;&gt;3&lt;/span&gt; 16:28:18 &lt;span class=&#34;m&#34;&gt;2017&lt;/span&gt;
NAMESPACE: default
STATUS: DEPLOYED

RESOURCES:
&lt;span class=&#34;o&#34;&gt;==&lt;/span&gt;&amp;gt; v1/Secret
NAME              TYPE    DATA  AGE
example5-mariadb  Opaque  &lt;span class=&#34;m&#34;&gt;2&lt;/span&gt;     &lt;span class=&#34;nv&#34;&gt;1s&lt;/span&gt;

&lt;span class=&#34;o&#34;&gt;==&lt;/span&gt;&amp;gt; v1/ConfigMap
NAME              DATA  AGE
example5-mariadb  &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;     &lt;span class=&#34;nv&#34;&gt;1s&lt;/span&gt;

&lt;span class=&#34;o&#34;&gt;==&lt;/span&gt;&amp;gt; v1/PersistentVolumeClaim
NAME              STATUS  VOLUME                                    CAPACITY  ACCESSMODES  AGE
example5-mariadb  Bound   pvc-229f9ed6-3015-11e7-945a-66fc987ccf32  8Gi       RWO          &lt;span class=&#34;nv&#34;&gt;1s&lt;/span&gt;

&lt;span class=&#34;o&#34;&gt;==&lt;/span&gt;&amp;gt; v1/Service
NAME              CLUSTER-IP  EXTERNAL-IP  PORT&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;S&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;       AGE
example5-mychart  10.0.0.144  &amp;lt;nodes&amp;gt;      80:30896/TCP  1s
example5-mariadb  10.0.0.108  &amp;lt;none&amp;gt;       3306/TCP      &lt;span class=&#34;nv&#34;&gt;1s&lt;/span&gt;

&lt;span class=&#34;o&#34;&gt;==&lt;/span&gt;&amp;gt; apps/v1/Deployment
NAME              DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
example5-mariadb  &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;        &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;        &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;           &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;          1s
example5-mychart  &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;        &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;        &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;           &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;          1s


NOTES:
1. Get the application URL by running these commands:
&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;NODE_PORT&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;$(&lt;/span&gt;kubectl get --namespace default -o &lt;span class=&#34;nv&#34;&gt;jsonpath&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;{.spec.ports[0].nodePort}&amp;#34;&lt;/span&gt; services example5-mychart&lt;span class=&#34;k&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;NODE_IP&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;$(&lt;/span&gt;kubectl get nodes --namespace default -o &lt;span class=&#34;nv&#34;&gt;jsonpath&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;{.items[0].status.addresses[0].address}&amp;#34;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; http://&lt;span class=&#34;nv&#34;&gt;$NODE_IP&lt;/span&gt;:&lt;span class=&#34;nv&#34;&gt;$NODE_PORT&lt;/span&gt;/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;contribute-to-the-bitnami-repository&#34;&gt;Contribute to the Bitnami repository!&lt;/h2&gt;
&lt;p&gt;As a chart author, you can help to build out Bitnami&amp;rsquo;s chart repository by improving existing charts or submitting new ones. Checkout &lt;a href=&#34;https://kubeapps.com&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;https://kubeapps.com&lt;/a&gt; to see what&amp;rsquo;s currently available and head to &lt;a href=&#34;https://github.com/bitnami/charts&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;https://github.com/bitnami/charts&lt;/a&gt; to get involved.&lt;/p&gt;
&lt;h2 id=&#34;useful-links&#34;&gt;Useful links&lt;/h2&gt;
&lt;p&gt;We&amp;rsquo;ve walked through some of the ways Helm supercharges the delivery of applications on Kubernetes. From an empty directory, you were able to get a working Helm chart out of a single command, deploy it to your cluster and access an NGINX server. Then, by simply changing a few lines and re-deploying, you had a much more useful todo list application running on your cluster! Beyond templating, linting, sharing and managing dependencies, here are some other useful tools available to chart authors:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/helm/blob/master/docs/charts_hooks.md&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Define hooks to run &lt;em&gt;Jobs&lt;/em&gt; before or after installing and upgrading releases&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/helm/blob/master/docs/provenance.md&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Sign chart packages to help users verify its integrity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/helm/blob/master/docs/chart_tests.md&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Write integration/validation tests for your charts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/helm/blob/master/docs/charts_tips_and_tricks.md&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Employ a handful of tricks in your chart templates&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: What is Helmfile?</title>
      
      <link>/guides/kubernetes/helmfile-what-is/</link>
      <pubDate>Thu, 30 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/guides/kubernetes/helmfile-what-is/</guid>
      <description>

        
        &lt;p&gt;&lt;a href=&#34;https://github.com/roboll/helmfile&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Helmfile&lt;/a&gt; adds additional functionality to &lt;a href=&#34;https://helm.sh&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Helm&lt;/a&gt; by wrapping it in a declarative spec that allows you to compose several charts together to create a comprehensive deployment artifact for anything from a single application to your entire infrastructure stack.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: If you&amp;rsquo;re not familiar with Helm, start with our &lt;a href=&#34;../helm-what-is&#34;&gt;Getting Started with Helm&lt;/a&gt; guide.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In addition to the Templating and Packaging Helm gives you for your Kubernetes manifests, Helmfile provides a way to apply GitOps style CI/CD methodologies over your Helm charts by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Separating out your Environment specific information from your Chart&lt;/li&gt;
&lt;li&gt;Performing a diff of your existing deployment and only applying the changes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Helmfile uses the same templating system as Helm and in a way lets you template your templates (&lt;em&gt;&lt;insert yo dawg meme here&gt;&lt;/em&gt;). This can be a bit difficult to wrap your mind around at first, but adds a ton of powerful features as it allows you to put basic programming logic like &lt;em&gt;if/then/else&lt;/em&gt; into just about any component including your actual Helm Chart Values.&lt;/p&gt;
&lt;h2 id=&#34;why-is-it-important&#34;&gt;Why Is It Important?&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://helm.sh&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Helm&lt;/a&gt; is a great tool for templating and sharing Kubernetes manifests for your applications. However it can become quite cumbersome to install larger multi-tier applications or groups of applications across multiple Kubernetes clusters.&lt;/p&gt;
&lt;p&gt;Helmfile addresses this issue and more by providing a fairly simple but very powerful declarative specification for deploying Helm charts across many environments.&lt;/p&gt;
&lt;p&gt;First and foremost Helm is a &lt;strong&gt;declarative&lt;/strong&gt; specification. Like Kubernetes manifests you can store them in version control, and perform declarative style actions. Much like Kubernetes has &lt;code&gt;kubectl apply&lt;/code&gt; for Kubernetes manifests, Helmfile has &lt;code&gt;helmfile apply&lt;/code&gt; for Helm charts.&lt;/p&gt;
&lt;p&gt;Helmfile is very &lt;strong&gt;modular&lt;/strong&gt;, you can have one large &lt;code&gt;helmfile.yaml&lt;/code&gt; that does everything or you can break it down to suit your way of working. This modularity allows you to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Give each Helm chart its own &lt;code&gt;helmfile.yaml&lt;/code&gt; and include them recursively in a centralized &lt;code&gt;helmfile.yaml&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Separate out &lt;a href=&#34;https://github.com/roboll/helmfile/blob/master/docs/writing-helmfile.md#layering-state-files&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;environment specific&lt;/a&gt; values from general values. Often you&amp;rsquo;ll find while a Helm chart can take 50 different values, only a few actually differ between your environments.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;As well as providing a set of values, either Environment specific or otherwise, you can also read Environment Variables, Execute scripts and read their output.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Store &lt;a href=&#34;https://github.com/roboll/helmfile/pull/648&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;remote state&lt;/a&gt; in git/s3/fileshare/etc in much the same way as Terraform does.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Helmfile is &lt;strong&gt;versatile&lt;/strong&gt; enough to allow you to also include raw Kubernetes manifests, &lt;a href=&#34;https://github.com/kubernetes-sigs/kustomize&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Kustomizations&lt;/a&gt;, or even execute scripts via hooks, turning all of these into &lt;a href=&#34;https://github.com/roboll/helmfile/pull/673&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Helm releases&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Need to modify the resources generated by a specific Helm chart? Helmfile allows you to &lt;a href=&#34;https://github.com/roboll/helmfile/pull/673&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;JSON/Strategic-Merge&lt;/a&gt; &lt;strong&gt;patch&lt;/strong&gt; resources before actually installing them.&lt;/p&gt;
&lt;h2 id=&#34;how-does-it-work&#34;&gt;How Does It Work?&lt;/h2&gt;
&lt;p&gt;Helmfile works by reading in your Helmfile manifest (usually &lt;code&gt;helmfile.yaml&lt;/code&gt;) which declares the Helm Charts you want to install and the values you wish to install them with, these are compared against the actual state of what is running in your cluster and any differences are then acted upon by calling out to Helm itself.&lt;/p&gt;
&lt;p&gt;A basic &lt;code&gt;helmfile.yaml&lt;/code&gt; to install nginx would look something like this:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;./apps/nginx/helmfile.yaml&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;repositories&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;stable&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;url&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;https://kubernetes-charts.storage.googleapis.com&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;releases&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;my-nginx-server&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;namespace&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;default&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;labels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;app&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;nginx&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;chart&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;stable/nginx&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;version&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;~1.24.1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;values&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;./nginx/vault.yaml.gotmpl&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;my.registry.com/nginx&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Values are a list that can be passed in as a file or a list of key/values. These are the helm style values that will be rendered into your chart. Helmfile will treat any file with the &lt;code&gt;.gotmpl&lt;/code&gt; extension as a template and will render it &lt;strong&gt;before&lt;/strong&gt; passing it onto Helm.&lt;/p&gt;
&lt;p&gt;If you wanted to load the above into a parent &lt;code&gt;helmfile.yaml&lt;/code&gt; you could do the following:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;./helmfile.yaml&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;helmfiles&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;apps/*/helmfile.yaml&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You can even make all of your included &lt;code&gt;helmfile.yaml&lt;/code&gt; files templates and render stuff right into the helmfiles. It really is templates all the way down.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;./helmfile.yaml&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;helmfiles:
  - apps/*/helmfile.yaml.gotmpl
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Thankfully the Helmfile GitHub repository has some really good &lt;a href=&#34;https://github.com/roboll/helmfile#configuration&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;documentation&lt;/a&gt; and &lt;a href=&#34;https://github.com/roboll/helmfile/blob/master/docs/writing-helmfile.md&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;best practices&lt;/a&gt; showing different ways to construct your &lt;code&gt;helmfile.yaml&lt;/code&gt; files.&lt;/p&gt;
&lt;h2 id=&#34;how-can-i-use-it&#34;&gt;How Can I Use It?&lt;/h2&gt;
&lt;p&gt;It&amp;rsquo;s pretty easy to get started with Helmfile. The documentation in the repository is quite good.&lt;/p&gt;
&lt;p&gt;For an interesting perspective showing how to completely decouple your Code and Environment data have a look at Paul Czarkowski&amp;rsquo;s &lt;a href=&#34;https://github.com/paulczar/helmfile-starter-kit&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Helmfile Starter Kit&lt;/a&gt; and the &lt;a href=&#34;https://github.com/paulczar/platform-operations-on-kubernetes&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Platform Operations on Kubernetes&lt;/a&gt; project built on top of it. The latter of which is used to deploy a whole kitchen sink worth of platform tooling across dozens of Kubernetes clusters.&lt;/p&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;/css/faq.css&#34;&gt;
&lt;div class=&#34;faqs&#34; id=&#34;faqs&#34;&gt;
    &lt;div class=&#34;flex-container jc-between&#34;&gt;&lt;/div&gt;
        &lt;h2 class=&#34;h2 mb-md&#34;&gt;Frequently Asked Questions&lt;/h2&gt;
        &lt;div class=&#34;faq&#34;&gt;
            
  &lt;div class=&#34;faq-item&#34; id=&#34;faq-item&#34;&gt;
    &lt;div class=&#34;flex jc-between ai-center&#34;&gt;
        &lt;h4 class=&#34;faq-question&#34;&gt;What is a Helmfile?&lt;/h4&gt;
        &lt;i class=&#34;fa fa-angle-down&#34; id=&#34;arrow&#34;&gt;&lt;/i&gt;
    &lt;/div&gt;
    &lt;div class=&#34;faq-answer&#34;&gt;
        &lt;div&gt;Helmfile is a declarative specification wrapping for deploying distributions of Helm charts. They add additional functionality to Helm by allowing you to compose several charts together to create a comprehensive deployment artifact.&lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;
  &lt;div class=&#34;faq-item&#34; id=&#34;faq-item&#34;&gt;
    &lt;div class=&#34;flex jc-between ai-center&#34;&gt;
        &lt;h4 class=&#34;faq-question&#34;&gt;What is a Helm chart?&lt;/h4&gt;
        &lt;i class=&#34;fa fa-angle-down&#34; id=&#34;arrow&#34;&gt;&lt;/i&gt;
    &lt;/div&gt;
    &lt;div class=&#34;faq-answer&#34;&gt;
        &lt;div&gt;Helm charts are Kubernetes manifests or a collection of files that correspond to a directly related set of Kubernetes resources.&lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;
  &lt;div class=&#34;faq-item&#34; id=&#34;faq-item&#34;&gt;
    &lt;div class=&#34;flex jc-between ai-center&#34;&gt;
        &lt;h4 class=&#34;faq-question&#34;&gt;How do Helmfiles work?&lt;/h4&gt;
        &lt;i class=&#34;fa fa-angle-down&#34; id=&#34;arrow&#34;&gt;&lt;/i&gt;
    &lt;/div&gt;
    &lt;div class=&#34;faq-answer&#34;&gt;
        &lt;div&gt;Helmfiles work by reading your Helmfile manifests and comparing them against the actual state of what is running in your cluster. Any differences are then acted upon by calling out to Helm itself.&lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;
  &lt;div class=&#34;faq-item&#34; id=&#34;faq-item&#34;&gt;
    &lt;div class=&#34;flex jc-between ai-center&#34;&gt;
        &lt;h4 class=&#34;faq-question&#34;&gt;What is the difference between Helm and Helmfile?&lt;/h4&gt;
        &lt;i class=&#34;fa fa-angle-down&#34; id=&#34;arrow&#34;&gt;&lt;/i&gt;
    &lt;/div&gt;
    &lt;div class=&#34;faq-answer&#34;&gt;
        &lt;div&gt;Helm is a tool for templating and sharing Kubernetes manifests for your applications, while a Helmfile is a declarative specification for deploying Helm charts that adds functionality to Helm.&lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;
  &lt;div class=&#34;faq-item&#34; id=&#34;faq-item&#34;&gt;
    &lt;div class=&#34;flex jc-between ai-center&#34;&gt;
        &lt;h4 class=&#34;faq-question&#34;&gt;How do you modify the resources generated by a specific Helm chart?&lt;/h4&gt;
        &lt;i class=&#34;fa fa-angle-down&#34; id=&#34;arrow&#34;&gt;&lt;/i&gt;
    &lt;/div&gt;
    &lt;div class=&#34;faq-answer&#34;&gt;
        &lt;div&gt;Resources generated by a specific Helm chart can be modified before installation by allowing you to JSON/Strategic-Merge patch resources.&lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;
  &lt;div class=&#34;faq-item&#34; id=&#34;faq-item&#34;&gt;
    &lt;div class=&#34;flex jc-between ai-center&#34;&gt;
        &lt;h4 class=&#34;faq-question&#34;&gt;What are the benefits of Helmfiles?&lt;/h4&gt;
        &lt;i class=&#34;fa fa-angle-down&#34; id=&#34;arrow&#34;&gt;&lt;/i&gt;
    &lt;/div&gt;
    &lt;div class=&#34;faq-answer&#34;&gt;
        &lt;div&gt;Helmfiles are beneficial because they provide powerful declarative specification for deploying Helm charts across many environments.&lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;

        &lt;/div&gt;
    &lt;/div&gt;
    
&lt;/div&gt;

&lt;script type=&#34;text/javascript&#34;&gt;
    $(&#34;.faq-item&#34;).each( function() {
        $(this).click(function () {
            $(this).find(&#34;#arrow&#34;).toggleClass(&#34;flip&#34;); 
            $(this).find(&#34;.faq-answer&#34;).slideToggle(200); 
        });
    });
&lt;/script&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: Getting Started with Cloud Foundry for Kubernetes</title>
      
      <link>/guides/kubernetes/cf4k8s-gs/</link>
      <pubDate>Wed, 29 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/guides/kubernetes/cf4k8s-gs/</guid>
      <description>

        
        &lt;blockquote&gt;
&lt;p&gt;Updated October 2020: CF CLI version 7+ and 6 CPU availability now required, removed metrics server install, new values added to the install yaml eliminate steps from before, and new Kubernetes rendering file. Overall this simplifies installation from previous iterations.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/cloudfoundry/cf-for-k8s.git&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;CF-for-k8s&lt;/a&gt; brings Cloud Foundry to Kubernetes.&lt;/p&gt;
&lt;p&gt;Cloud Foundry is an open-source, multi-cloud application platform as a service governed by the Cloud Foundry Foundation, a 501 organization.&lt;/p&gt;
&lt;p&gt;Using Cloud Foundry developers only have to focus on writing and delivering code as CF takes care of the rest. Developers enter &lt;code&gt;cf push&lt;/code&gt; into the command line and their app will be deployed immediately receiving an endpoint. The CF platform will take care of containerizing the source code into a working app with the required dependencies, can be configured to bind to a database, connect to a market place and much more.&lt;/p&gt;
&lt;p&gt;The cf-for-k8s platform adds a higher level of abstraction to Kubernetes by removing the sharp learning curve required for teams, developers don&amp;rsquo;t have to know Kubernetes they only have to &lt;code&gt;cf push&lt;/code&gt;. Kubernetes adds new possibilities to Cloud Foundry opening up the massive Kubernetes ecosystem.&lt;/p&gt;
&lt;p&gt;In this guide you&amp;rsquo;ll deploy Cloud Foundry on Kubernetes locally.&lt;/p&gt;
&lt;h2 id=&#34;before-you-begin&#34;&gt;Before you begin&lt;/h2&gt;
&lt;h3 id=&#34;machine-requirements&#34;&gt;Machine Requirements&lt;/h3&gt;
&lt;p&gt;Currently cf-for-k8s supports Kubernetes 1.15.x or 1.16.x, the config yaml file we are using to make our kind cluster will make a cluster with the following requirements, see that your computer can handle them:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;have a minimum of 1 node&lt;/li&gt;
&lt;li&gt;have a minimum of 6 CPU, 8GB memory per node&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;tools-required&#34;&gt;Tools required&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;You will need a few tools before beginning and once set up installation usually takes 10 minutes or less.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;CF CLI version requirement changed to version 7+&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.cloudfoundry.org/cf-cli/install-go-cli.html&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Cloud Foundry CLI&lt;/a&gt; (version 7+) to talk to Cloud Foundry&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;on mac
&lt;pre&gt;&lt;code&gt;brew install cloudfoundry/tap/cf-cli
# verify install 
cf version
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You will need &lt;code&gt;kubectl&lt;/code&gt; to interact with your cluster &lt;a href=&#34;https://kubernetes.io/docs/tasks/tools/install-kubectl/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;kubectl install instructions&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;on mac
&lt;pre&gt;&lt;code&gt;brew install kubectl
# verify install 
kubectl version --client
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;KinD (Kubernetes in Docker) to instantiate your local cluster &lt;a href=&#34;https://kind.sigs.k8s.io/docs/user/quick-start/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Kind install instructions&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;on mac
&lt;pre&gt;&lt;code&gt;brew install kind
# verify install 
kind version
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://bosh.io/docs/cli-v2-install/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bosh CLI&lt;/a&gt; the &lt;code&gt;./hack/generate-values.sh&lt;/code&gt; script will use the Bosh CLI to generate certificates, keys, and passwords in the file &lt;code&gt;./cf-install-values.yml&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;on mac
&lt;pre&gt;&lt;code&gt;brew install cloudfoundry/tap/bosh-cli
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://k14s.io/#install&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;kapp&lt;/a&gt; (v0.21.0+) will aid you to deploy cf-for-k8s to your cluster&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;on mac
&lt;pre&gt;&lt;code&gt;brew tap k14s/tap
brew install ytt kapp
kapp --version
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://k14s.io/#install&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;ytt&lt;/a&gt; (v0.26.0+) will help create templates to deploy cf-for-k8s&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;on mac you should have this installed from the above command, to verify:
&lt;pre&gt;&lt;code&gt;ytt version
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://hub.docker.com/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;DockerHub&lt;/a&gt; is the image registry used in this guide please make an account if you don&amp;rsquo;t have one they are free and quickly made.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;clone-the-cf-for-k8s-repo&#34;&gt;Clone the CF for K8s repo&lt;/h2&gt;
&lt;p&gt;Clone the repo to preferred location and cd into it.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/cloudfoundry/cf-for-k8s.git &amp;amp;&amp;amp; cd cf-for-k8s
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;setup-your-local-k8s-cluster-with-kind&#34;&gt;Setup your local k8s cluster with KinD&lt;/h2&gt;
&lt;p&gt;Create your cluster using the config yaml from the cf-for-k8s repo obtained above.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kind create cluster --config=./deploy/kind/cluster.yml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Point your kubeconfig to your new cluster.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl cluster-info --context kind-kind
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;generate-the-yaml-used-to-deploy-cf-for-k8s&#34;&gt;Generate the yaml used to deploy CF for k8s&lt;/h2&gt;
&lt;p&gt;In this script you use &lt;code&gt;vcap.me&lt;/code&gt; as your CF domain with the flag &lt;code&gt;-d&lt;/code&gt;, this way you can avoid configuring DNS for a domain.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;./hack/generate-values.sh&lt;/code&gt; script will generate certificates, keys, passwords, and configuration needed to deploy into `./cf-install-values.yml&#39;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;./hack/generate-values.sh -d vcap.me &amp;gt; ./cf-install-values.yml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Append the app_registry credentials to your DockerHub registry to the bottom of the &lt;code&gt;./cf-install-values.yml&lt;/code&gt; replacing with your information. You can copy/paste  or use the following command.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The repeated username is a requirement for DockerHub, this setting changes with some container registries. Also, don&amp;rsquo;t forget to add the quotes.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;To use another container registry follow the &lt;a href=&#34;https://github.com/cloudfoundry/cf-for-k8s/blob/master/docs/deploy.md&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;instructions under step 3&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;gt;&amp;gt; cf-install-values.yml &amp;lt;&amp;lt; EOL
app_registry:
  hostname: https://index.docker.io/v1/
  repository_prefix: &amp;quot;&amp;lt;DockerHub-username&amp;gt;&amp;quot;
  username: &amp;quot;&amp;lt;DockerHub-username&amp;gt;&amp;quot;
  password: &amp;quot;&amp;lt;DockerHub-password&amp;gt;&amp;quot;
EOL
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;There are a few more lines to add to your cf-install-values.yml, like adding a metrics server because KinD doesn&amp;rsquo;t come with one.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;gt;&amp;gt; cf-install-values.yml &amp;lt;&amp;lt; EOL
add_metrics_server_components: true
enable_automount_service_account_token: true
metrics_server_prefer_internal_kubelet_address: true
remove_resource_requirements: true
use_first_party_jwt_tokens: true

load_balancer:
  enable: false
EOL
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now, use cf-install-values.yml to render the final Kubernetes template to raw Kubernetes configuration.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ytt -f config -f ./cf-install-values.yml &amp;gt; ./cf-for-k8s-rendered.yml
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;deploy-cf-for-k8s&#34;&gt;Deploy CF for k8s&lt;/h2&gt;
&lt;p&gt;You are ready to deploy cf-for-k8s using the &lt;code&gt;./cf-for-k8s-rendered.yml&lt;/code&gt; file created above. Once you deploy it should take around 10 minutes to finish.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The deployment has a timer and will exit with a timeout error if it takes too long. Assuming all previous steps were followed correctly enter the deployment command again to finish if it exits early.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;kapp deploy -a cf -f ./cf-for-k8s-rendered.yml -y
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;validate-the-deployment&#34;&gt;Validate the deployment&lt;/h2&gt;
&lt;p&gt;Target your CF CLI to point to the new CF instance.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cf api --skip-ssl-validation https://api.vcap.me
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Set the CF_ADMIN_PASSWORD environment variable to the CF administrative password, stored in the cf_admin_password key in the configuration-values/deployment-values.yml file:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;CF_ADMIN_PASSWORD=&amp;quot;$(bosh interpolate ./cf-install-values.yml --path /cf_admin_password)&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Log into the installation as the admin user.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cf auth admin &amp;quot;$CF_ADMIN_PASSWORD&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Enable Docker&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cf enable-feature-flag diego_docker
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;A powerful feature provided by CF is multi-tenancy, where you can create a space for a team, an app or whatever your workflow requires.&lt;/p&gt;
&lt;p&gt;Create and target an organization and space.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cf create-org test-org
cf create-space -o test-org test-space
cf target -o test-org -s test-space
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;deploy-an-application-with-cf-push&#34;&gt;Deploy an application with &lt;code&gt;cf push&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;At last you can push the included sample &lt;code&gt;test-node-app&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cf push test-node-app -p ./tests/smoke/assets/test-node-app
&lt;/code&gt;&lt;/pre&gt;&lt;blockquote&gt;
&lt;p&gt;Or you can push any app you wish just cd into the directory and push the app with the following command.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cf push APP-NAME
&lt;/code&gt;&lt;/pre&gt;&lt;/blockquote&gt;
&lt;p&gt;Once your app stages you can find it in Cloud Foundry with this command.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cf apps
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The output in the terminal should look something as follows.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Getting apps in org test-org / space test-space as admin...
OK

name            requested state   instances   memory   disk   urls
test-node-app   started           1/1         1G       1G     test-node-app.vcap.me
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To see the pods that have applications on your Cloud Foundry instance look in the &lt;code&gt;cf-workloads&lt;/code&gt; namespace.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl get pods -n cf-workloads
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You can now play with cf for k8s and deploy other apps and observe how it affects the Kubernetes infrastructure. Try other cf commands like &lt;code&gt;cf delete test-node-app&lt;/code&gt; and see what changes, enjoy you new cf for k8s instance.&lt;/p&gt;
&lt;h2 id=&#34;delete-the-cf-for-k8s-deployment&#34;&gt;Delete the cf-for-k8s deployment&lt;/h2&gt;
&lt;p&gt;You can delete the cf-for-k8s deployment from your cluster by running the following command.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kapp delete -a cf
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;delete-your-kind-cluster&#34;&gt;Delete your Kind cluster&lt;/h2&gt;
&lt;p&gt;To delete your KinD cluster.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kind delete cluster
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;next-steps&#34;&gt;Next Steps&lt;/h2&gt;
&lt;p&gt;Learn more about Cloud Foundry with the links below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/cloudfoundry/cf-for-k8s.git&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;cf-for-k8s GitHub&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cloudfoundry.org/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;cloudfoundry.org&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://katacoda.com/cloudfoundry-tutorials/scenarios/trycf&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Online CF Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: Move a Custom Spring Boot Application to Production Using Bitnami Helm Charts</title>
      
      <link>/guides/containers/deploy-spring-boot-application-production-helm/</link>
      <pubDate>Mon, 13 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/guides/containers/deploy-spring-boot-application-production-helm/</guid>
      <description>

        
        &lt;p&gt;Bitnami provides ready-to-run &lt;a href=&#34;https://github.com/bitnami/charts&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Helm charts&lt;/a&gt; that can be directly deployed on &lt;a href=&#34;https://kubernetes.io/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Kubernetes&lt;/a&gt; and also infrastructure charts that can help you deploy your custom applications. That is the case of the &lt;a href=&#34;https://github.com/bitnami/charts/tree/master/bitnami/tomcat&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami Tomcat Helm chart&lt;/a&gt; that with some tweaks can be used to run Java applications in production easily.&lt;/p&gt;
&lt;p&gt;This tutorial walks you through the process of deploying a Spring Boot container image on Kubernetes using the Bitnami Apache Tomcat Helm chart. It uses the resulting image created in the &lt;a href=&#34;../deploy-locally-spring-boot-application-docker&#34;&gt;Deploy locally a Spring Boot application using Bitnami containers&lt;/a&gt; guide as an example. While the Bitnami Tomcat Helm chart will be modified to get the application container from the DockerHub registry, create a secret to secure the application pod and connect it to a MariaDB pod.&lt;/p&gt;
&lt;h2 id=&#34;assumptions-and-prerequisites&#34;&gt;Assumptions and prerequisites&lt;/h2&gt;
&lt;p&gt;This guide makes the following assumptions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You have basic knowledge of &lt;a href=&#34;https://www.docker.com/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Docker&lt;/a&gt; containers.&lt;/li&gt;
&lt;li&gt;You have a Docker environment installed and configured. &lt;a href=&#34;https://docs.docker.com/install/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Learn more about installing Docker&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;You have a Docker Hub account. &lt;a href=&#34;https://hub.docker.com/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Register for a free account&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;You have a &lt;a href=&#34;https://docs.bitnami.com/tutorials/deploy-locally-spring-boot-application-docker/#step-6-publish-the-docker-image&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Spring Boot container published&lt;/a&gt; in a container registry (this tutorial assumes that you are using &lt;a href=&#34;https://hub.docker.com/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Docker Hub&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;You have a &lt;a href=&#34;https://docs.bitnami.com/kubernetes/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Kubernetes cluster running&lt;/a&gt; in the platform of your choice. This tutorial uses &lt;a href=&#34;https://docs.bitnami.com/kubernetes/get-started-kubernetes/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Minikube&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;You have the &lt;a href=&#34;https://docs.bitnami.com/kubernetes/get-started-kubernetes/#step-3-install-kubectl-command-line&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;em&gt;kubectl&lt;/em&gt; command line (&lt;em&gt;kubectl&lt;/em&gt; CLI)&lt;/a&gt; installed.&lt;/li&gt;
&lt;li&gt;You have &lt;a href=&#34;https://docs.bitnami.com/kubernetes/get-started-kubernetes/#step-4-install-helm&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Helm v3.x&lt;/a&gt; installed.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The following are the steps you will complete in this guide:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Step 1: Create the Helm chart&lt;/li&gt;
&lt;li&gt;Step 2: Adapt the Helm chart to include the source code and database&lt;/li&gt;
&lt;li&gt;Step 3: Create a secret to secure the deployment&lt;/li&gt;
&lt;li&gt;Step 4: Deploy the example application in Kubernetes&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Learn how to create a Spring Boot Docker container image in the &lt;a href=&#34;../deploy-locally-spring-boot-application-docker&#34;&gt;Deploy locally a Spring Boot application using Bitnami containers&lt;/a&gt; guide.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;step-1-create-the-helm-chart&#34;&gt;Step 1: Create the Helm chart&lt;/h2&gt;
&lt;p&gt;Begin by creating the Helm chart for our application. In this case, the Bitnami Tomcat Helm chart will serve you as a starting point which you can modify to build your custom chart. To do so, execute the following command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;helm fetch bitnami/tomcat --untar
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This will create a folder in your local system that contains all the files required for deploying Tomcat in a Kubernetes cluster.&lt;/p&gt;
&lt;h2 id=&#34;step-2-adapt-the-helm-chart-to-include-the-source-code-and-database&#34;&gt;Step 2: Adapt the Helm chart to include the source code and database&lt;/h2&gt;
&lt;p&gt;The first step consists of adapting the current Bitnami Tomcat Helm chart to include the sample Spring Boot container image and MariaDB as a database. Then, connect both pods when deploying the resulting chart. Follow the instructions below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Change to the &lt;em&gt;tomcat&lt;/em&gt; directory and create a file named &lt;em&gt;requirements.yaml&lt;/em&gt; with the content below to include MariaDB as a dependency:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;dependencies:
- name: mariadb
  version: 5.x.x
  repository: https://charts.helm.sh/stable/
  condition: mariadb.enabled
  tags:
    - spring-java-app-database
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Edit the &lt;em&gt;values.yaml&lt;/em&gt; file and replace the default values with the following to include your image. Remember to replace the DOCKER_USERNAME placeholder with your Docker account username.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;[...]
image:
   registry: docker.io
   repository: DOCKER_USERNAME/spring-java-app
   tag: latest
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add the following lines at the end of the &lt;em&gt;values.yaml&lt;/em&gt; file to specify the database:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;[...]
mariadb:
   Whether to deploy a mariadb server to satisfy the applications database requirements. To use an external database set this to false and configure the externalDatabase parameters
  enabled: true
   Disable MariaDB replication
  replication:
    enabled: false
   Create a database and a database user
   ref: https://github.com/bitnami/bitnami-docker-mariadb/blob/master/README.mdcreating-a-database-user-on-first-run

  db:
    name: db_example
    user: springuser
   If the password is not specified, mariadb will generates a random password

  password: ThePassword
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Edit the &lt;em&gt;templates/_helpers.tpl&lt;/em&gt; and add the lines below to generate the name of the MariaDB service so the application will be able to connect to it:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;{{/*
Create a default fully qualified app name.
We truncate at 63 chars because some Kubernetes name fields are limited to this (by the DNS naming spec).
*/}}
{{- define &amp;#34;mariadb.fullname&amp;#34; -}}
{{- printf &amp;#34;%s-%s&amp;#34; .Release.Name &amp;#34;mariadb&amp;#34; | trunc 63 | trimSuffix &amp;#34;-&amp;#34; -}}
{{- end -}}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;step-3-create-a-secret-to-secure-the-deployment&#34;&gt;Step 3: Create a secret to secure the deployment&lt;/h2&gt;
&lt;p&gt;The next step is to create a secret for the Spring Boot application that secures the connection between the application and the database. Follow these instructions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In the &lt;em&gt;templates&lt;/em&gt; directory, create a file named &lt;em&gt;spring-secret.yaml&lt;/em&gt; that includes the following content:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;apiVersion: v1
kind: Secret
metadata:
  name: {{ template &amp;#34;tomcat.fullname&amp;#34; . }}-spring
  labels:
    app: {{ template &amp;#34;tomcat.fullname&amp;#34; . }}
    chart: &amp;#34;{{ .Chart.Name }}-{{ .Chart.Version }}&amp;#34;
    release: &amp;#34;{{ .Release.Name }}&amp;#34;
    heritage: &amp;#34;{{ .Release.Service }}&amp;#34;
type: Opaque
data:
  spring-db: {{ printf &amp;#34;{\&amp;#34;spring\&amp;#34;: {\&amp;#34;datasource\&amp;#34;:{\&amp;#34;url\&amp;#34;: \&amp;#34;jdbc:mysql://%s:3306/%s\&amp;#34;, \&amp;#34;username\&amp;#34;: \&amp;#34;%s\&amp;#34;, \&amp;#34;password\&amp;#34;: \&amp;#34;%s\&amp;#34;}}}&amp;#34; (include &amp;#34;mariadb.fullname&amp;#34; .) .Values.mariadb.db.name .Values.mariadb.db.user .Values.mariadb.db.password | b64enc }}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Edit the &lt;em&gt;templates/deployment.yaml&lt;/em&gt; file to add the lines below. These refer to the secret created in the step above:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;[...]
- name: TOMCAT_PASSWORD
  valueFrom:
    secretKeyRef:
      name: {{ template &amp;#34;tomcat.fullname&amp;#34; . }}
      key: tomcat-password
- name: SPRING_APPLICATION_JSON
  valueFrom:
    secretKeyRef:
      name: {{ template &amp;#34;tomcat.fullname&amp;#34; . }}-spring
      key: spring-db
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;step-4-deploy-the-example-application-in-kubernetes&#34;&gt;Step 4: Deploy the example application in Kubernetes&lt;/h2&gt;
&lt;p&gt;Before deploying the resulting Helm chart, make sure that you can connect to your Kubernetes cluster by running this command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;kubectl cluster-info
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Execute the command below to install missing dependencies. In this case, it will install the database that we have indicated in the &lt;em&gt;requirements.yaml&lt;/em&gt; file:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;helm dependency update .
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Deploy the chart by executing the &lt;em&gt;helm install&lt;/em&gt; command. It is recommended to install it by passing a name using the &lt;em&gt;/&amp;ndash;n&lt;/em&gt; flag.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;helm install spring-java .
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Check that all pods are ready by executing the &lt;em&gt;kubectl get pods&lt;/em&gt; command. Take into account that the database pod takes more time to be deployed than the Tomcat pod, is possible that the &lt;em&gt;kubectl logs&lt;/em&gt; command show errors during that time.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;kubectl get pods -w
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You should see an output similar to this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/containers/bitnami/deploy-spring-boot-application-production-helm/get-pods.png&#34; alt=&#34;Pod status&#34;  /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To test that the Spring Boot application has been successfully deployed it is necessary to make it accessible from your local system. To do so, port forward the Tomcat pod as shown below. Replace &lt;em&gt;svc/spring-java-tomcat&lt;/em&gt; with the name of the service that appears in your deployment:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;kubectl port-forward svc/spring-java-tomcat 8080:80
Forwarding from 127.0.0.1:8080 -&amp;gt; 8080
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To test if the application works fine, open a new terminal and insert some data in the database by executing:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;curl &amp;#39;localhost:8080/gs-mysql-data-0.1.0/demo/add?name=First&amp;amp;email=someemail@someemailprovider.com&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Query the application again to check if the data is present in the database:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;curl &amp;#39;localhost:8080/gs-mysql-data-0.1.0/demo/all&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You should get an output similar to this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;[{&amp;#34;id&amp;#34;:1, &amp;#34;name&amp;#34;:&amp;#34;First&amp;#34;, &amp;#34;email&amp;#34;:&amp;#34;someemail@someemailprovider.com&amp;#34;}]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Congratulations! You have your Spring Boot application running in a Kubernetes production cluster and ready to use!&lt;/p&gt;
&lt;h2 id=&#34;useful-links&#34;&gt;Useful links&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://bitnami.com/kubernetes&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami Kubernetes projects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../deploy-locally-spring-boot-application-docker&#34;&gt;Deploy locally a Spring Boot application using Bitnami containers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.bitnami.com//kubernetes/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Get started with Kubernetes guides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.bitnami.com/tutorials/deploy-application-kubernetes-helm/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Deploy, Scale And Upgrade An Application On Kubernetes With Helm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bitnami/charts/tree/master/bitnami/tomcat&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami Tomcat Helm chart&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bitnami/charts&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami Helm charts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://hub.docker.com/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Docker Hub&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://spring.io/projects/spring-boot&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Spring Boot official site&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bitnami/tutorials&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami tutorials repository&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: Best Practices for Creating Production-Ready Helm Charts</title>
      
      <link>/guides/kubernetes/production-ready-helm/</link>
      <pubDate>Fri, 05 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/guides/kubernetes/production-ready-helm/</guid>
      <description>

        
        &lt;p&gt;Three years have passed since &lt;a href=&#34;https://github.com/helm/helm/releases?after=v1.1&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;the first release of Helm&lt;/a&gt;, and it has indeed made a name for itself. Both avowed fans and fervent haters agree that the Kubernetes &amp;ldquo;apt-get equivalent&amp;rdquo; is the standard way of deploying to production (at least for now, let&amp;rsquo;s see what Operators end up bringing to the table). During this time, Bitnami has contributed to the project in many ways. You can find us in PRs in Helm&amp;rsquo;s code, in solutions like &lt;a href=&#34;https://hub.kubeapps.com/charts/bitnami&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Kubeapps&lt;/a&gt;, and especially in what we are mostly known for: &lt;a href=&#34;https://bitnami.com/stacks/helm&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;our huge application library&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As maintainers of a collection of more than &lt;a href=&#34;https://github.com/bitnami/charts/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;45 Helm charts&lt;/a&gt;, we know that creating a maintainable, secure and production-ready chart is far from trivial. In this sense, this blog post shows essential features that any chart developer should know.&lt;/p&gt;
&lt;h2 id=&#34;use-non-root-containers&#34;&gt;Use non-root containers&lt;/h2&gt;
&lt;p&gt;Ensuring that a container is able to perform only a very limited set of operations is vital for production deployments. This is possible thanks to the &lt;strong&gt;use of non-root containers, which are executed by a user different from &lt;em&gt;root&lt;/em&gt;.&lt;/strong&gt; Although creating a non-root container is a bit more complex than a root container (especially regarding filesystem permissions), it is absolutely worth it. Also, in environments like OpenShift, &lt;a href=&#34;https://engineering.bitnami.com/articles/running-non-root-containers-on-openshift.html&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;using non-root containers is mandatory&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In order to make your Helm chart work with non-root containers, add the &lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/security-context/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;em&gt;securityContext&lt;/em&gt;&lt;/a&gt; section to your &lt;em&gt;yaml&lt;/em&gt; files.&lt;/p&gt;
&lt;p&gt;This is what we do, for instance, in the Bitnami Elasticsearch Helm chart. This chart deploys several Elasticsearch &lt;em&gt;StatefulSets&lt;/em&gt; and &lt;em&gt;Deployments&lt;/em&gt; (data, ingestion, coordinating and master nodes), all of them with non-root containers. If we check the master node &lt;em&gt;StatefulSet&lt;/em&gt;, we see the following:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;{{- &lt;span class=&#34;l&#34;&gt;if .Values.securityContext.enabled }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;securityContext&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;fsGroup&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;{{&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;.Values.securityContext.fsGroup }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;{{- &lt;span class=&#34;l&#34;&gt;end }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The snippet above changes the permissions of the mounted volumes, so the container user can access them for read/write operations. In addition to this, inside the container definition, we see another &lt;em&gt;securityContext&lt;/em&gt; block:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;{{- &lt;span class=&#34;l&#34;&gt;if .Values.securityContext.enabled }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;securityContext&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;runAsUser&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;{{&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;.Values.securityContext.runAsUser }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;{{- &lt;span class=&#34;l&#34;&gt;end }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In this part we specify the user running the container. In the &lt;em&gt;values.yaml&lt;/em&gt; file, we set the default values for these parameters:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;c&#34;&gt;## Pod Security Context&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;##&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;securityContext&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;enabled&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;true&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;fsGroup&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1001&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;runAsUser&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1001&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;With these changes, the chart will work as non-root in platforms like GKE, Minikube or OpenShift.&lt;/p&gt;
&lt;h2 id=&#34;do-not-persist-the-configuration&#34;&gt;Do not persist the configuration&lt;/h2&gt;
&lt;p&gt;Adding persistence is an essential part of deploying stateful applications. In our experience, deciding what or what not to persist can be tricky. After several iterations in our charts, we found that &lt;strong&gt;persisting the application configuration is not a recommended practice&lt;/strong&gt;. One advantage of Kubernetes is that you can change the deployment parameters very easily by just doing &lt;code&gt;kubectl edit deployment&lt;/code&gt; or &lt;code&gt;helm upgrade&lt;/code&gt;. If the configuration is persisted, none of the changes would be applied. So, when developing a production-ready Helm chart, make sure that the configuration can be easily changed with &lt;code&gt;kubectl&lt;/code&gt; or &lt;code&gt;helm upgrade&lt;/code&gt;. One common practice is to create a &lt;em&gt;ConfigMap&lt;/em&gt; with the configuration and have it mounted in the container. Let&amp;rsquo;s use the &lt;a href=&#34;https://github.com/bitnami/charts/tree/master/bitnami/rabbitmq&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami RabbitMQ chart&lt;/a&gt; as an example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;ConfigMap&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;{{&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;template &amp;#34;rabbitmq.fullname&amp;#34; . }}-config&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;labels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;app&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;{{&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;template &amp;#34;rabbitmq.name&amp;#34; . }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;chart&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;{{&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;template &amp;#34;rabbitmq.chart&amp;#34; .  }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;release&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;{{ .Release.Name }}&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;heritage&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;{{ .Release.Service }}&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;enabled_plugins&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;|-&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;{{&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;template &amp;#34;rabbitmq.plugins&amp;#34; . }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;rabbitmq.conf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;|-&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;    ##username and password
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;    default_user={{.Values.rabbitmq.username}}
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;    default_pass=CHANGEME&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;{{&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;.Values.rabbitmq.configuration | indent 4 }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;{{&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;.Values.rabbitmq.extraConfiguration | indent 4 }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Note that there is a section in the &lt;em&gt;values.yaml&lt;/em&gt; file that allows you to include any custom configuration:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;c&#34;&gt;## Configuration file content: required cluster configuration&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;c&#34;&gt;## Do not override unless you know what you are doing. To add more configuration, use `extraConfiguration` instead&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;configuration&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;|-&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;    ## Clustering
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;    cluster_formation.peer_discovery_backend  = rabbit_peer_discovery_k8s
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;    cluster_formation.k8s.host = kubernetes.default.svc.cluster.local
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;    cluster_formation.node_cleanup.interval = 10
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;    cluster_formation.node_cleanup.only_log_warning = true
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;    cluster_partition_handling = autoheal
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;    # queue master locator
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;    queue_master_locator=min-masters
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;    # enable guest user
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;    loopback_users.guest = false&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;c&#34;&gt;## Configuration file content: extra configuration&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;c&#34;&gt;## Use this instead of  `configuration` to add more configuration&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;extraConfiguration&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;|-&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;    #disk_free_limit.absolute = 50MB
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;    #management.load_definitions = /app/load_definition.json&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This &lt;em&gt;ConfigMap&lt;/em&gt; then gets mounted in the container filesystem, as shown in this extract of the &lt;em&gt;StatefulSet&lt;/em&gt; spec:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;volumes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;config-volume&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;configMap&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;{{&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;template &amp;#34;rabbitmq.fullname&amp;#34; . }}-config&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If the application needs to write in the configuration file, then you&amp;rsquo;ll need to create a copy inside the container, as &lt;em&gt;ConfigMaps&lt;/em&gt; are mounted as read-only. This is done in the same spec:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;containers&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;rabbitmq&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;{{&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;template &amp;#34;rabbitmq.image&amp;#34; . }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;imagePullPolicy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;{{&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;.Values.image.pullPolicy | quote }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;command&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# ...&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;c&#34;&gt;#copy the mounted configuration to both places&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;cp  /opt/bitnami/rabbitmq/conf/* /opt/bitnami/rabbitmq/etc/rabbitmq&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# ...&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This will make your chart not only easy to upgrade, but also more adaptable to user needs, as they can provide their custom configuration file.&lt;/p&gt;
&lt;h2 id=&#34;integrate-charts-with-logging-and-monitoring-tools&#34;&gt;Integrate charts with logging and monitoring tools&lt;/h2&gt;
&lt;p&gt;If we are talking about production environments, we are talking about observability. It is essential having our deployments properly monitored so we can early detect potential issues. It also essential to have application usage, cost and resource consumption metrics. In order to gather this information, you would commonly deploy logging stacks like EFK (&lt;a href=&#34;https://www.elastic.co/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;ElasticSearch&lt;/a&gt;, &lt;a href=&#34;https://www.fluentd.org/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Fluentd&lt;/a&gt;, and &lt;a href=&#34;https://www.elastic.co/products/kibana&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Kibana&lt;/a&gt; and monitoring tools like &lt;a href=&#34;https://prometheus.io/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Prometheus&lt;/a&gt;. Bitnami offers the &lt;a href=&#34;https://kubeprod.io/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami Kubernetes Production Runtime (BKPR)&lt;/a&gt; that easily installs these tools (along with others) so your cluster is ready to handle production workloads.&lt;/p&gt;
&lt;p&gt;When writing your chart, make sure that your deployment is able to work with the above tools seamlessly. To do so, ensure the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All the containers log to stdout/stderr (so the EFK stack can easily ingest all the logging information)&lt;/li&gt;
&lt;li&gt;Prometheus exporters are included (either using sidecar containers or having a separate deployment)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All Bitnami charts work with BKPR (which includes EFK and Prometheus) out of the box. Let&amp;rsquo;s take a look at the &lt;a href=&#34;https://github.com/bitnami/charts/tree/master/bitnami/postgresql&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami PostgreSQL chart&lt;/a&gt; and &lt;a href=&#34;https://github.com/bitnami/bitnami-docker-postgresql&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami PostgreSQL container&lt;/a&gt; to see how we did it.&lt;/p&gt;
&lt;p&gt;To begin with, the process inside the container runs at the foreground, so all the logging information is written to stdout/stderr, as shown below:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;info &lt;span class=&#34;s2&#34;&gt;&amp;#34;** Starting PostgreSQL **&amp;#34;&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; am_i_root&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;then&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;exec&lt;/span&gt; gosu &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$POSTGRESQL_DAEMON_USER&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;${&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;cmd&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;${&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;flags&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[@]&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;exec&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;${&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;cmd&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;${&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;flags&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[@]&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;fi&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;With this, we ensured that it works with EFK. Then, in the chart we added a sidecar container for the Prometheus metrics:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;{{- &lt;span class=&#34;l&#34;&gt;if .Values.metrics.enabled }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;metrics&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;{{&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;template &amp;#34;postgresql.metrics.image&amp;#34; . }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;imagePullPolicy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;{{&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;.Values.metrics.image.pullPolicy | quote }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;         &lt;/span&gt;{{- &lt;span class=&#34;l&#34;&gt;if .Values.metrics.securityContext.enabled }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;securityContext&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;runAsUser&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;{{&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;.Values.metrics.securityContext.runAsUser }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;{{- &lt;span class=&#34;l&#34;&gt;end }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;env&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;{{- &lt;span class=&#34;l&#34;&gt;$database := required &amp;#34;In order to enable metrics you need to specify a database (.Values.postgresqlDatabase or .Values.global.postgresql.postgresqlDatabase)&amp;#34; (include &amp;#34;postgresql.database&amp;#34; .) }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;DATA_SOURCE_URI&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;              &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;value&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;{{&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;printf &amp;#34;127.0.0.1:%d/%s?sslmode=disable&amp;#34; (int (include &amp;#34;postgresql.port&amp;#34; .)) $database | quote }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;{{- &lt;span class=&#34;l&#34;&gt;if .Values.usePasswordFile }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;DATA_SOURCE_PASS_FILE&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;              &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;value&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;/opt/bitnami/postgresql/secrets/postgresql-password&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;{{- &lt;span class=&#34;l&#34;&gt;else }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;DATA_SOURCE_PASS&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;              &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;valueFrom&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;                &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;secretKeyRef&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;                  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;{{&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;template &amp;#34;postgresql.secretName&amp;#34; . }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;                  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;key&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;postgresql-password&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;{{- &lt;span class=&#34;l&#34;&gt;end }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;DATA_SOURCE_USER&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;              &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;value&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;{{&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;template &amp;#34;postgresql.username&amp;#34; . }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;{{- &lt;span class=&#34;l&#34;&gt;if .Values.livenessProbe.enabled }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;livenessProbe&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;httpGet&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;              &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;              &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;port&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;http-metrics&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;initialDelaySeconds&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;{{&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;.Values.metrics.livenessProbe.initialDelaySeconds }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;periodSeconds&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;{{&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;.Values.metrics.livenessProbe.periodSeconds }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;timeoutSeconds&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;{{&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;.Values.metrics.livenessProbe.timeoutSeconds }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;successThreshold&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;{{&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;.Values.metrics.livenessProbe.successThreshold }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;failureThreshold&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;{{&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;.Values.metrics.livenessProbe.failureThreshold }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;{{- &lt;span class=&#34;l&#34;&gt;end }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;{{- &lt;span class=&#34;l&#34;&gt;if .Values.readinessProbe.enabled }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;readinessProbe&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;httpGet&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;              &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;              &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;port&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;http-metrics&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;initialDelaySeconds&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;{{&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;.Values.metrics.readinessProbe.initialDelaySeconds }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;periodSeconds&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;{{&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;.Values.metrics.readinessProbe.periodSeconds }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;timeoutSeconds&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;{{&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;.Values.metrics.readinessProbe.timeoutSeconds }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;successThreshold&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;{{&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;.Values.metrics.readinessProbe.successThreshold }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;failureThreshold&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;{{&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;.Values.metrics.readinessProbe.failureThreshold }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;{{- &lt;span class=&#34;l&#34;&gt;end }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;volumeMounts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;{{- &lt;span class=&#34;l&#34;&gt;if .Values.usePasswordFile }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;postgresql-password&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;              &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;mountPath&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;/opt/bitnami/postgresql/secrets/&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;{{- &lt;span class=&#34;l&#34;&gt;end }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;{{- &lt;span class=&#34;l&#34;&gt;if .Values.metrics.customMetrics }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;custom-metrics&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;              &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;mountPath&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;/conf&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;              &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;readOnly&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;true&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;--extend.query-path&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;/conf/custom-metrics.yaml&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;{{- &lt;span class=&#34;l&#34;&gt;end }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;ports&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;http-metrics&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;              &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;containerPort&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;9187&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;{{- &lt;span class=&#34;l&#34;&gt;if .Values.metrics.resources }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;resources&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;{{- &lt;span class=&#34;l&#34;&gt;toYaml .Values.metrics.resources | nindent 12 }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;{{- &lt;span class=&#34;l&#34;&gt;end }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;{{- &lt;span class=&#34;l&#34;&gt;end }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We also made sure that the pods or services contain the proper annotations that Prometheus uses to detect exporters. In this case, we defined them in the chart&amp;rsquo;s &lt;em&gt;values.yaml&lt;/em&gt; file, as shown below:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;metrics&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;enabled&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;false&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;service&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;type&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;ClusterIP&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;annotations&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;prometheus.io/scrape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;prometheus.io/port&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;9187&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;c&#34;&gt;#...  &lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In the case of the PostgreSQL chart, these annotations go to a metrics service, separate from the PostgreSQL service, which is defined as below:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;{{- &lt;span class=&#34;l&#34;&gt;if .Values.metrics.enabled }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Service&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;{{&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;template &amp;#34;postgresql.fullname&amp;#34; . }}-metrics&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;labels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;app&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;{{&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;template &amp;#34;postgresql.name&amp;#34; . }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;chart&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;{{&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;template &amp;#34;postgresql.chart&amp;#34; . }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;release&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;{{&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;.Release.Name | quote }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;heritage&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;{{&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;.Release.Service | quote }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;annotations&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;{{&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;toYaml .Values.metrics.service.annotations | indent 4 }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;type&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;{{&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;.Values.metrics.service.type }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;{{- &lt;span class=&#34;l&#34;&gt;if and (eq .Values.metrics.service.type &amp;#34;LoadBalancer&amp;#34;) .Values.metrics.service.loadBalancerIP }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;loadBalancerIP&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;{{&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;.Values.metrics.service.loadBalancerIP }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;{{- &lt;span class=&#34;l&#34;&gt;end }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;ports&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;http-metrics&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;port&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;9187&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;targetPort&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;http-metrics&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;selector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;app&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;{{&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;template &amp;#34;postgresql.name&amp;#34; . }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;release&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;{{&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;.Release.Name }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;role&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;master&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;{{- &lt;span class=&#34;l&#34;&gt;end }}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;With these modifications, your chart will seamlessly integrate with your monitoring platform. All the obtained metrics will be crucial for maintaining the deployment in good shape.&lt;/p&gt;
&lt;h2 id=&#34;production-workloads-in-kubernetes-are-possible&#34;&gt;Production workloads in Kubernetes are possible&lt;/h2&gt;
&lt;p&gt;Now you know some essential guidelines for creating secured (with non-root containers), adaptable (with proper configuration management), and observable (with proper monitoring) charts. With these features, you have covered the basics to ensure that your application can be deployed to production. However, this is just another step in your journey to mastering Helm. You should also take into account other features like upgradability, usability, stability and testing.&lt;/p&gt;
&lt;h2 id=&#34;useful-links&#34;&gt;Useful Links&lt;/h2&gt;
&lt;p&gt;To learn more, check the following links:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://helm.sh/docs/chart_best_practices/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Official Helm chart good practice guidelines&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://codefresh.io/docs/docs/new-helm/helm-best-practices/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Helm best practices by CodeFresh&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubeprod.io/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami Kubernetes Production Runtime&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bitnami/charts/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami Helm charts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://hub.kubeapps.com/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Kubeapps Hub&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: Getting Started with Kubeapps</title>
      
      <link>/guides/kubernetes/kubeapps-gs/</link>
      <pubDate>Thu, 23 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/guides/kubernetes/kubeapps-gs/</guid>
      <description>

        
        &lt;p&gt;This guide will walk you through the process of deploying Kubeapps for your cluster and installing an example application.&lt;/p&gt;


&lt;div class=&#34;aside aside-info&#34;&gt;
    &lt;div class=&#34;aside aside-title&#34;&gt;
        &lt;i class=&#34;fas fa-exclamation-circle&#34;&gt;&lt;/i&gt;
        &lt;div&gt;Note&lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#34;aside aside-content&#34;&gt;
    &lt;p&gt;This getting started guide is a copy of
&lt;a href=&#34;https://github.com/kubeapps/kubeapps/blob/master/docs/user/getting-started.md&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Get Started with Kubeapps&lt;/a&gt;
in the &lt;a href=&#34;https://github.com/kubeapps/kubeapps&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;kubeapps/kubeapps repository&lt;/a&gt;
inclusive of commit
&lt;a href=&#34;https://github.com/kubeapps/kubeapps/commit/1e49264088094dfd327d2a24b62cda470cc547d0&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;1e49264&lt;/a&gt;
on April 20, 2020.&lt;/p&gt;

    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;This guide is also available in video form:&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/9HsWsoDd1fM&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;Kubeapps assumes a working Kubernetes cluster (v1.8+),
&lt;a href=&#34;https://helm.sh/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;code&gt;Helm&lt;/code&gt;&lt;/a&gt; (2.14.0+) installed in your cluster and
&lt;a href=&#34;https://kubernetes.io/docs/tasks/tools/install-kubectl/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;code&gt;kubectl&lt;/code&gt;&lt;/a&gt; installed
and configured to talk to your Kubernetes cluster. Kubeapps has been tested with
Azure Kubernetes Service (AKS), Google Kubernetes Engine (GKE), &lt;code&gt;minikube&lt;/code&gt; and
Docker for Desktop Kubernetes. Kubeapps works on RBAC-enabled clusters and this
configuration is encouraged for a more secure install.&lt;/p&gt;


&lt;div class=&#34;aside aside-info&#34;&gt;
    &lt;div class=&#34;aside aside-title&#34;&gt;
        &lt;i class=&#34;fas fa-exclamation-circle&#34;&gt;&lt;/i&gt;
        &lt;div&gt;GKE Permissions&lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#34;aside aside-content&#34;&gt;
    &lt;p&gt;On GKE, you must either be an &amp;ldquo;Owner&amp;rdquo; or have the &amp;ldquo;Container Engine Admin&amp;rdquo; role
in order to install Kubeapps.&lt;/p&gt;

    &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;step-1-install-kubeapps&#34;&gt;Step 1: Install Kubeapps&lt;/h2&gt;
&lt;p&gt;Use the Helm chart to install the latest version of Kubeapps:&lt;/p&gt;
&lt;p&gt;For Helm 2:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;helm repo add bitnami https://charts.bitnami.com/bitnami
helm install --name kubeapps --namespace kubeapps bitnami/kubeapps
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If you are using Helm 3, you need to set an extra flag to enable it:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;helm repo add bitnami https://charts.bitnami.com/bitnami
kubectl create namespace kubeapps
helm install kubeapps --namespace kubeapps bitnami/kubeapps --set &lt;span class=&#34;nv&#34;&gt;useHelm3&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;true&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;For detailed information on installing, configuring and upgrading Kubeapps,
checkout the
&lt;a href=&#34;https://github.com/kubeapps/kubeapps/blob/master/chart/kubeapps/README.md&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;chart README&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The above commands will deploy Kubeapps into the &lt;code&gt;kubeapps&lt;/code&gt; namespace in your
cluster. It may take a few minutes to execute. Once it has been deployed and the
Kubeapps pods are running, continue to step 2.&lt;/p&gt;
&lt;h2 id=&#34;step-2-create-a-kubernetes-api-token&#34;&gt;Step 2: Create a Kubernetes API token&lt;/h2&gt;
&lt;p&gt;Access to the Dashboard requires a Kubernetes API token to authenticate with the
Kubernetes API server.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl create serviceaccount kubeapps-operator
kubectl create clusterrolebinding kubeapps-operator --clusterrole&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;cluster-admin --serviceaccount&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;default:kubeapps-operator
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; It&amp;rsquo;s not recommended to create &lt;code&gt;cluster-admin&lt;/code&gt; users for Kubeapps
production usage. Please refer to the
&lt;a href=&#34;https://github.com/kubeapps/kubeapps/blob/master/docs/user/access-control.md&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Access Control&lt;/a&gt;
documentation to configure fine-grained access control for users.&lt;/p&gt;
&lt;p&gt;To retrieve the token,&lt;/p&gt;
&lt;h3 id=&#34;on-linuxmacos&#34;&gt;On Linux/macOS:&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl get secret &lt;span class=&#34;k&#34;&gt;$(&lt;/span&gt;kubectl get serviceaccount kubeapps-operator -o &lt;span class=&#34;nv&#34;&gt;jsonpath&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;{range .secrets[*]}{.name}{&amp;#34;\n&amp;#34;}{end}&amp;#39;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; grep kubeapps-operator-token&lt;span class=&#34;k&#34;&gt;)&lt;/span&gt; -o &lt;span class=&#34;nv&#34;&gt;jsonpath&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;{.data.token}&amp;#39;&lt;/span&gt; -o go-template&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;{{.data.token | base64decode}}&amp;#39;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;on-windows&#34;&gt;On Windows:&lt;/h3&gt;
&lt;p&gt;Create a file called &lt;code&gt;GetDashToken.cmd&lt;/code&gt; with the following lines in it:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bat&#34; data-lang=&#34;bat&#34;&gt;&lt;span class=&#34;p&#34;&gt;@&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;ECHO&lt;/span&gt; OFF
&lt;span class=&#34;c1&#34;&gt;REM Get the Service Account&lt;/span&gt;
kubectl get serviceaccount kubeapps-operator -o jsonpath={.secrets[].name} &lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt; s.txt
&lt;span class=&#34;k&#34;&gt;SET&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;/p&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;ks&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&amp;lt;&lt;/span&gt;s.txt
&lt;span class=&#34;k&#34;&gt;DEL&lt;/span&gt; s.txt

&lt;span class=&#34;c1&#34;&gt;REM Get the Base64 encoded token&lt;/span&gt;
kubectl get secret &lt;span class=&#34;nv&#34;&gt;%ks%&lt;/span&gt; -o jsonpath={.data.token} &lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt; b64.txt

&lt;span class=&#34;c1&#34;&gt;REM Decode The Token&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;DEL&lt;/span&gt; token.txt
certutil -decode b64.txt token.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Open a command prompt and run the &lt;code&gt;GetDashToken.cmd&lt;/code&gt; Your token can be found in
the &lt;code&gt;token.txt&lt;/code&gt; file.&lt;/p&gt;
&lt;h2 id=&#34;step-3-start-the-kubeapps-dashboard&#34;&gt;Step 3: Start the Kubeapps Dashboard&lt;/h2&gt;
&lt;p&gt;Once Kubeapps is installed, securely access the Kubeapps Dashboard from your
system by running:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl port-forward -n kubeapps svc/kubeapps 8080:80
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This will start an HTTP proxy for secure access to the Kubeapps Dashboard. Visit
http://127.0.0.1:8080/ in your preferred web browser to open the Dashboard.
Here&amp;rsquo;s what you should see:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/kubeapps/screenshots/dashboard-login.png&#34; alt=&#34;Dashboard login page&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Paste the token generated in the previous step to authenticate and access the
Kubeapps dashboard for Kubernetes.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/kubeapps/screenshots/dashboard-home.png&#34; alt=&#34;Dashboard main page&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Note:&lt;/strong&gt;&lt;/em&gt; If you are setting up Kubeapps for other people to access, you will
want to use a different service type or setup Ingress rather than using the
above &lt;code&gt;kubectl port-forward&lt;/code&gt;. For detailed information on installing,
configuring and upgrading Kubeapps, checkout the
&lt;a href=&#34;https://github.com/kubeapps/kubeapps/blob/master/chart/kubeapps/README.md&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;chart README&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;step-4-deploy-wordpress&#34;&gt;Step 4: Deploy WordPress&lt;/h2&gt;
&lt;p&gt;Once you have the Kubeapps Dashboard up and running, you can start deploying
applications into your cluster.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Use the &amp;ldquo;Deploy App&amp;rdquo; or click on the &amp;ldquo;Catalog&amp;rdquo; page in the Dashboard to select
an application from the list of charts in any of the configured Helm chart
repositories. This example assumes you want to deploy WordPress.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/kubeapps/screenshots/wordpress-search.png&#34; alt=&#34;WordPress chart&#34;  /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Click the &amp;ldquo;Deploy&amp;rdquo; button.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/kubeapps/screenshots/wordpress-chart.png&#34; alt=&#34;WordPress chart&#34;  /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You will be prompted for the release name and values for the application. The
form is populated by the values (YAML), which you can see in the adjacent tab.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/kubeapps/screenshots/wordpress-installation.png&#34; alt=&#34;WordPress installation&#34;  /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Click the &amp;ldquo;Submit&amp;rdquo; button. The application will be deployed. You will be able
to track the new Helm deployment directly from the browser. The status will be
shown at the top and you can also look at the individual resources lower in
the page. It will also show the number of ready pods. If you run your cursor
over the status, you can see the workloads and number of ready and total pods
within them.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/kubeapps/screenshots/wordpress-deployment.png&#34; alt=&#34;WordPress deployment&#34;  /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To access your new WordPress site, you can run the commands in the &amp;ldquo;Notes&amp;rdquo;
section to get the URLs or simply click a URL (HTTP and HTTPS) shown.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Note:&lt;/strong&gt;&lt;/em&gt; Depending on your cloud provider of choice, it may take some time
for an access URL to be available for the application and the Service will stay
in a &amp;ldquo;Pending&amp;rdquo; state until a URL is assigned. If using Minikube, you will need
to run &lt;code&gt;minikube tunnel&lt;/code&gt; in your terminal in order for an IP address to be
assigned to your application.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/kubeapps/screenshots/wordpress-url.png&#34; alt=&#34;WordPress deployment notes&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;To get the credentials for logging into your WordPress account, refer to the
&amp;ldquo;Notes&amp;rdquo; section. You can also get the WordPress password by scrolling down to
&amp;ldquo;Secrets&amp;rdquo; and clicking the eye next to &lt;code&gt;wordpress-password&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/kubeapps/screenshots/wordpress-credentials.png&#34; alt=&#34;WordPress deployment notes&#34;  /&gt;&lt;/p&gt;
&lt;h2 id=&#34;optional-step-5-uninstalldelete-wordpress&#34;&gt;[Optional] Step 5: Uninstall/Delete WordPress&lt;/h2&gt;
&lt;p&gt;If you want to uninstall/delete your WordPress application, you can do so by
clicking the &amp;ldquo;Delete&amp;rdquo; button. You can choose to click the checkbox for &amp;ldquo;Purge
Release&amp;rdquo; (default action with the Helm 3 CLI). If you do not click it, the Helm
chart history will remain (default action with Helm 2). This is fine, so long as
you don&amp;rsquo;t attempt to install another chart with the same name in the same
namespace.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/kubeapps/screenshots/wordpress-uninstall.png&#34; alt=&#34;WordPress uninstall&#34;  /&gt;&lt;/p&gt;
&lt;h2 id=&#34;next-steps&#34;&gt;Next Steps&lt;/h2&gt;
&lt;p&gt;Learn more about Kubeapps with the links below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubeapps/kubeapps/blob/master/chart/kubeapps/README.md&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Detailed installation instructions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubeapps/kubeapps/blob/master/docs/user/operators.md&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Deploying Operators&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubeapps/kubeapps/blob/master/docs/user/dashboard.md&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Kubeapps Dashboard documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubeapps/kubeapps/blob/master/docs/architecture/overview.md&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Kubeapps components&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubeapps/kubeapps/wiki/Roadmap&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Roadmap&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: Installing Harbor on Kubernetes with Project Contour, Cert Manager, and Let’s Encrypt</title>
      
      <link>/guides/kubernetes/harbor-gs/</link>
      <pubDate>Mon, 02 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/guides/kubernetes/harbor-gs/</guid>
      <description>

        
        &lt;p&gt;Running a private container image registry has been a staple in many enterprise
environments for years. These registries allow full control over access,
updates, and the software platform itself. And while your organization may have
an official image registry, having your own can also be a benefit!&lt;/p&gt;
&lt;p&gt;Maybe it&amp;rsquo;s just for learning. Or maybe you like the idea of self-hosting your
own services. Or maybe, like so many companies that have also made the decision
to self-host, you are concerned about security and access. Whatever the reason,
you have made the decision to deploy your own. But this process includes a lot
more than just an initial install.&lt;/p&gt;
&lt;p&gt;A container image registry needs to be accessible to many online services to be
useful, not the least of which is your desktop. It’s what makes pulling and
pushing images possible. And you probably want it to be accessible from outside
your own network, too, so that you can collaborate and share your projects.
These days, to be secure, this requires TLS encryption to enable HTTPS traffic.&lt;/p&gt;
&lt;p&gt;In this guide, you will deploy Harbor to Kubernetes as the actual image registry
application. You will also use Project Contour to manage ingress to your
Kubernetes cluster.&lt;/p&gt;

&lt;div class=&#34;youtube-video-shortcode&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/SXSqrgYKO4s&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h2 id=&#34;harbor-and-contour&#34;&gt;Harbor and Contour&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://goharbor.io&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Harbor&lt;/a&gt; is a powerful registry for containers and Helm
charts. It is fully open source and backed by the
&lt;a href=&#34;https://landscape.cncf.io/selected=harbor&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Cloud Native Computing Foundation (CNCF)&lt;/a&gt;.
But getting it up and running, with automated TLS certificate renewal in
particular, can be a challenge—especially with the multiple services Harbor uses
that require east-west and north-south network communication.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://projectcontour.io&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Contour&lt;/a&gt;, also a CNCF project, is an ingress
controller built for Kubernetes. It functions as a control plane for Envoy while
also offering advanced routing functionality beyond the default ingress
controller provided by Kubernetes.&lt;/p&gt;
&lt;p&gt;You will deploy Harbor and Contour, and use
&lt;a href=&#34;https://cert-manager.io/docs/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;cert-manager&lt;/a&gt; and
&lt;a href=&#34;https://letsencrypt.org&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Let&amp;rsquo;s Encrypt&lt;/a&gt; to automate TLS certificate generation
and renewal for your Harbor installation. This will allow you to keep your
Harbor installation up and running and utilizing HTTPS without manually
generating and applying new certificates.&lt;/p&gt;
&lt;p&gt;Also, using the patterns in this guide, you should be able to deploy other
services to Kubernetes and secure their ingress as well. And once you understand
the basics of certificate management, ingress, and routing services, you’ll be
able to keep on deploying other services to Kubernetes and enabling secure
access over the internet!&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;Before you get started, you’ll need to do the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Install Helm 3&lt;/strong&gt;: Helm is used in this guide to install Contour and Harbor.
A guide for installing Helm can be found
&lt;a href=&#34;https://helm.sh/docs/intro/install/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Create a Kubernetes cluster&lt;/strong&gt;: This guide was built using Google Kubernetes
Engine (GKE) with Kubernetes version 1.17. Any Kubernetes cluster with access
to the internet should work fine, but your results may vary depending on the
version you use. Initial testing with 1.16 resulted in errors during the
Harbor install. For a guide on creating a GKE cluster, see
&lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/quickstart&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;this page&lt;/a&gt; from
Google. Ensure your
&lt;a href=&#34;https://kubernetes.io/docs/reference/kubectl/cheatsheet/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;&lt;code&gt;kubectl&lt;/code&gt; context&lt;/a&gt;
is using this cluster.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Install watch&lt;/strong&gt;: watch is a small command-line utility that continually
shows the output of a command being run. This allows you to monitor
&lt;code&gt;kubectl get pods&lt;/code&gt;, for example, without explicitly re-running the command
multiple times. Instructions for installing on macOS are
&lt;a href=&#34;https://osxdaily.com/2010/08/22/install-watch-command-on-os-x/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;About 30 minutes&lt;/strong&gt;: It could take more time than that; it will depend on how
long the Let’s Encrypt servers take to issue certificates. But in most of my
testing for writing this post, it took about 30 minutes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Optional - buy a domain name&lt;/em&gt;: You can use &lt;code&gt;.xip.io&lt;/code&gt; (a
&lt;a href=&#34;http://xip.io&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;service&lt;/a&gt; that provides dynamic DNS based on IP address)
addresses to avoid needing to buy a domain. Otherwise you will need a domain
name that you control in order to configure DNS. This guide uses a
Google-managed domain and DNS zone, but instructions can be modified for other
providers. Note: if you do decide to use &lt;code&gt;.xip.io&lt;/code&gt;, you may experience issues
using the &lt;code&gt;letsencrypt-prod&lt;/code&gt; ClusterIssuer later in the demo due to rate
limiting. Often you can just wait a few hours and it&amp;rsquo;ll eventually work. For
best results, we recommend using your own domain.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;prepare-the-environment&#34;&gt;Prepare the Environment&lt;/h2&gt;
&lt;p&gt;Create a Kubernetes cluster.  In GKE this can be as simple as running:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;gcloud container clusters create jan8 --num-nodes &lt;span class=&#34;m&#34;&gt;3&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Being organized is key to any successful project. So before you dig in, create a
new project directory and &lt;code&gt;cd&lt;/code&gt; into it.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;mkdir harbor-install &lt;span class=&#34;o&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;$_&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;install-project-contour&#34;&gt;Install Project Contour&lt;/h2&gt;
&lt;p&gt;While you are going to use Helm to install Project Contour, Helm will not create
the namespace for you. So the first step in this installation is to create that
namespace.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ kubectl create namespace projectcontour
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If you do not have the Bitnami repo referenced in Helm yet (you can check by
running &lt;code&gt;helm repo list&lt;/code&gt;), you will need to install it.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;helm repo add bitnami https://charts.bitnami.com/bitnami
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You will also need to update your Helm repositories.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;helm repo update
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Next, run &lt;code&gt;helm install&lt;/code&gt; to finish the installation. Here you will install
Bitnami&amp;rsquo;s image for Contour. This chart includes defaults that will work out of
the box for this guide. And since it comes from Bitnami, you can trust that the
image has been thoroughly tested and scanned.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;helm install ingress bitnami/contour -n projectcontour --version 3.3.1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now simply wait for the Pods to become &lt;code&gt;READY&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;watch kubectl get pods -n projectcontour
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Then define some environment variables for your proposed Harbor domain and your
email address. It is recommended that you use a subdomain under the domain
procured in the prerequisites section (e.g., harbor.example.com). This way you
can use other subdomain URLs to access other services you may deploy in the
future. The email address will be used for your Let&amp;rsquo;s Encrypt certificate so the
cert authority can notify you about expirations.&lt;/p&gt;
&lt;p&gt;Defining these variables will make the rest of the commands in this guide more
simple to run. The email address and the domain do not have to use the same
domain (a Gmail address is fine).&lt;/p&gt;
&lt;p&gt;If you do not have a custom domain run:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;nv&#34;&gt;IP&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;$(&lt;/span&gt;kubectl describe svc ingress-contour-envoy --namespace projectcontour &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; grep Ingress &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; awk &lt;span class=&#34;s1&#34;&gt;&amp;#39;{print $3}&amp;#39;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;DOMAIN&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$IP&lt;/span&gt;.xip.io
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Otherwise run:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;DOMAIN&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;your.domain.com
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Set your email address for cert-manager:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;EMAIL_ADDRESS&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;username@example.com
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;set-up-dns&#34;&gt;Set Up DNS&lt;/h2&gt;


&lt;div class=&#34;aside aside-warning&#34;&gt;
    &lt;div class=&#34;aside aside-title&#34;&gt;
        &lt;i class=&#34;fas fa-exclamation-circle&#34;&gt;&lt;/i&gt;
        &lt;div&gt;You may skip this section&lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#34;aside aside-content&#34;&gt;
    &lt;p&gt;This section is not applicable for those using &lt;code&gt;xip.io&lt;/code&gt;.
If that&amp;rsquo;s you, please skip this entire section.&lt;/p&gt;

    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;In this section, things can vary a bit. Because this guide was written using
Google Cloud Platform (GCP) tooling, the instructions below will reflect that.
But should you be using another provider, I will try to provide a generalized
description of what is being done so you can look up those specific steps. I
have also numbered these steps for clarity.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;In order to set up DNS records, you need the IP address of the Envoy ingress
router installed by Project Contour. Use the following command to describe
the service. If the &lt;code&gt;EXTERNAL-IP&lt;/code&gt; is still in a &lt;code&gt;&amp;lt;pending&amp;gt;&lt;/code&gt; state, just wait
until one is assigned. Record this value for a future step.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;watch kubectl get service ingress-contour-envoy -n projectcontour -o wide
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Now set up a DNS zone within your cloud provider.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;For GCP, this can be found in the GCP web UI, under Networking Services,
Cloud DNS. Click &lt;code&gt;Create Zone&lt;/code&gt; and follow the instructions to give the zone
a descriptive name, as well as provide the DNS name. The DNS name will be
whatever domain you have registered (e.g., &lt;code&gt;example.com&lt;/code&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For AWS, this is done via a service called
&lt;a href=&#34;https://aws.amazon.com/route53/faqs/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Route 53&lt;/a&gt;, and for Azure,
&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/dns/dns-getstarted-portal_&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;this is done&lt;/a&gt;
by creating a resource in the Azure Portal.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Once completed, the important part is that you now have a list of name
servers. For example, one or more of the format
&lt;code&gt;ns-cloud-x1.googledomains.com.&lt;/code&gt; Record these for a future step.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Next, add an A record to your DNS zone for a wildcard (&lt;code&gt;*&lt;/code&gt;) subdomain. An A
record is an &amp;ldquo;Address&amp;rdquo; record, one of the most fundamental types of DNS
records; it maps the more user-friendly URL (harbor.example.com) to an IP
address. Setting this up as a wildcard will allow you to configure any
traffic coming into any subdomain to first be resolved by Project Contour and
Envoy, then be routed to its final destination based on the specific
subdomain requested.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;For GCP, click into the DNS zone you just created and select &lt;code&gt;Add Record Set&lt;/code&gt;.
From here, add a &lt;code&gt;*&lt;/code&gt; as the DNS name field so the full DNS name reads
&lt;code&gt;*.example.com&lt;/code&gt;. In the IPv4 address field, enter the &lt;code&gt;EXTERNAL_IP&lt;/code&gt; of the
Envoy service recorded earlier in Step 1. Then click create.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For AWS and Azure, this is done via the respective services listed in the previous step.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Once completed, your DNS zone should have an A record set up for any subdomain
(&lt;code&gt;*&lt;/code&gt;) to be mapped to the IP address of the Envoy service running in
Kubernetes.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;For the last of the somewhat confusing UI-based steps, you need to add the
list of name servers from Step 2 to your personal domain. This will allow any
HTTP/HTTPS requests for your domain to reference the records you set up
previously in your DNS zone.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;For Google-managed domains, in the Google Domains UI, click &lt;code&gt;manage&lt;/code&gt; next to
the domain you want to modify. Then click on &lt;code&gt;DNS&lt;/code&gt;. In the Name Servers
section at the top, click &lt;code&gt;edit&lt;/code&gt;. Now, one at a time, simply paste in the list
of name servers recorded in Step 2. There should be four name server
addresses. Then click &lt;code&gt;Save&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For other domain name registrars, the process is similar. Contact your
registrar if you require further assistance with this process.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That&amp;rsquo;s it for configuring DNS. Now you need to wait for all the new records to
propagate. Run the following command and wait for the output to show that your
domain is now referencing the IP address of the Envoy service. This could take a
few minutes.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;watch host &lt;span class=&#34;nv&#34;&gt;$DOMAIN&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;install-cert-manager&#34;&gt;Install cert-manager&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://cert-manager.io&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Cert-manager&lt;/a&gt; will automate certificate renewal for
your services behind Project Contour. When a certificate is set to expire,
cert-manager will automatically request a new one from the certificate issuer
(you will set this up in the next section). This is especially important when
using a project like Let&amp;rsquo;s Encrypt, whose certificates are only valid for 90
days.&lt;/p&gt;
&lt;p&gt;To install cert-manager, this guide uses Helm, for uniformity. As with
installing Project Contour, Helm will not create a namespace, so the first step
is to create one.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl create namespace cert-manager
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If you do not have the Jetstack repo referenced in Helm yet (you can check by
running &lt;code&gt;helm repo list&lt;/code&gt;), you will need to install that reference.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;helm repo add jetstack https://charts.jetstack.io
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once again, update your Helm repositories.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;helm repo update
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Next, run &lt;code&gt;helm install&lt;/code&gt; to install cert-manager.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;helm install cert-manager jetstack/cert-manager --namespace cert-manager &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;   --version v1.0.2 --set &lt;span class=&#34;nv&#34;&gt;installCRDs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;true&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Finally, wait for the Pods to become &lt;code&gt;READY&lt;/code&gt; before moving on to the next step.
This should only take a minute or so.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;watch kubectl get pods -n cert-manager
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;set-up-lets-encrypt-staging-certificates&#34;&gt;Set up Let&amp;rsquo;s Encrypt Staging Certificates&lt;/h2&gt;
&lt;p&gt;When initially setting up your Harbor service, or any service that will be using
Let&amp;rsquo;s Encrypt, it is important to start by using certificates from their staging
server as opposed to the production server. Staging certificates allow you to
test your service without the risk of running up against any API rate limiting,
which Let’s Encrypt imposes on its production environment. This is not a
requirement, however; production certificates can be used initially. But using
the staging ones is a good habit to get into, and will highlight how these
certificates are applied.&lt;/p&gt;
&lt;p&gt;Create the deployment YAML for the staging certificate by pasting the block
below into a new file. Once applied, this will set up the staging cert
configuration along with your email address, for certificate expirations
notifications. You won&amp;rsquo;t need to worry about these emails, however, as
cert-manager will take care of the renewal for you.&lt;/p&gt;
&lt;p&gt;Notice that this is of &lt;code&gt;kind: ClusterIssuer&lt;/code&gt;. That means this certificate issuer
is scoped to all namespaces in this Kubernetes cluster. For more granular
controls in production, you may decide to simply change this to &lt;code&gt;kind: Issuer&lt;/code&gt;,
which will be scoped to a specific namespace and only allow services in that
namespace to request certificates from that issuer. But be aware that this will
necessitate changing other configuration options throughout this guide. We are
using &lt;code&gt;ClusterIssuer&lt;/code&gt; because it is secure enough for our use case, and
presumably you are not using a multi-tenant Kubernetes cluster when following
this guide.&lt;/p&gt;
&lt;p&gt;Finally, this sets up a “challenge record&amp;quot; in the &lt;code&gt;solvers&lt;/code&gt; section, which
allows Let&amp;rsquo;s Encrypt to verify that the certificate it is issuing is really
controlled by you.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cat &lt;span class=&#34;s&#34;&gt;&amp;lt;&amp;lt; EOF &amp;gt; letsencrypt-staging.yaml
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;apiVersion: cert-manager.io/v1alpha2
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;kind: ClusterIssuer
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;metadata:
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;  name: letsencrypt-staging
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;spec:
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;  acme:
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;    email: $EMAIL_ADDRESS
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;    privateKeySecretRef:
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;      name: letsencrypt-staging
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;    server: https://acme-staging-v02.api.letsencrypt.org/directory
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;    solvers:
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;    - http01:
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;        ingress:
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;          class: contour
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;EOF&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now &lt;code&gt;apply&lt;/code&gt; the file.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl apply -f letsencrypt-staging.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The process of creating the cluster issuer should be fairly quick, but you can
confirm it completed successfully by running the following and ensuring the
cluster issuer was issued.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ kubectl get clusterissuers.cert-manager.io
NAME                  READY   AGE
letsencrypt-staging   True    74s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;install-harbor&#34;&gt;Install Harbor&lt;/h2&gt;
&lt;p&gt;Now that you have your staging certificate, you can install Harbor, for which
this guide uses Helm in combination with the Bitnami repo set up earlier. As
with your other install steps, the first thing to do is create the namespace.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create namespace harbor
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Next, create the values file. The Bitnami Helm chart includes defaults that, for
the most part, will work for your needs. However, there are a few configuration
options that need to be set.&lt;/p&gt;
&lt;p&gt;Here you will notice that you are giving the TLS secret in Kubernetes a name,
&lt;code&gt;harbor-tls-staging&lt;/code&gt;. You can choose any name you like, but it should be
descriptive and reflect that this secret will be distinct from the production
certificate you will apply later.&lt;/p&gt;
&lt;p&gt;You are also setting up references to your domain so Harbor and Contour can set
up routing. The Annotations section is important as it tells Harbor about our
configuration. Notice that for
&lt;code&gt;cert-manager.io/cluster-issuer: letsencrypt-staging&lt;/code&gt; you are telling Harbor to
use the &lt;code&gt;ClusterIssuer&lt;/code&gt; called letsencrypt-staging, the one you set up earlier.
This will come up again later when you move to production certificates. Comments
are provided in the file for further detail.&lt;/p&gt;
&lt;p&gt;Finally, this values file will disable the Harbor Notary service. At the time of
this writing there is a bug in the Bitnami Helm chart (already reported) that
doesn&amp;rsquo;t allow a TLS certificate to be applied for both &lt;code&gt;notary.$DOMAIN&lt;/code&gt; and
&lt;code&gt;registry.$DOMAIN&lt;/code&gt;. I will try to update this post once that bug is fixed.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cat &lt;span class=&#34;s&#34;&gt;&amp;lt;&amp;lt; EOF &amp;gt; harbor-values.yaml
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;harborAdminPassword: Password12345
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;service:
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;  type: ClusterIP
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;  tls:
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;    enabled: true
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;    existingSecret: harbor-tls-staging
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;    notaryExistingSecret: notary-tls-staging
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;ingress:
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;  enabled: true
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;  hosts:
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;    core: registry.$DOMAIN
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;    notary: notary.$DOMAIN
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;  annotations:
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;    cert-manager.io/cluster-issuer: letsencrypt-staging  # use letsencrypt-staging as the cluster issuer for TLS certs
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;    ingress.kubernetes.io/force-ssl-redirect: &amp;#34;true&amp;#34;     # force https, even if http is requested
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;    kubernetes.io/ingress.class: contour                 # using Contour for ingress
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;    kubernetes.io/tls-acme: &amp;#34;true&amp;#34;                       # using ACME certificates for TLS
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;externalURL: https://registry.$DOMAIN
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;portal:
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;  tls:
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;    existingSecret: harbor-tls-staging
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;EOF&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now install Harbor using this values file.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;helm install harbor bitnami/harbor -f harbor-values.yaml -n harbor --version 9.4.4
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;And wait for the Pods to become &lt;code&gt;&amp;lt;READY&amp;gt;&lt;/code&gt;. This may take a minute or two.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;watch kubectl get pods -n harbor
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Finally, ensure that the certificates were requested and returned successfully.
This should happen fairly quickly, but may take up to an hour. It all depends on
the server load at that time.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ kubectl -n harbor get certificate
NAME                 READY   SECRET               AGE
harbor-tls-staging   True    harbor-tls-staging   2m26s
notary-tls-staging   True    notary-tls-staging   2m26s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Print out the URL, username, and password:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cat &lt;span class=&#34;s&#34;&gt;&amp;lt;&amp;lt; EOF
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;url: https://registry.$DOMAIN
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;username: admin
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;password: $(kubectl get secret --namespace harbor harbor-core-envvars -o jsonpath=&amp;#34;{.data.HARBOR_ADMIN_PASSWORD}&amp;#34; | base64 --decode)
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;EOF&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now open your browser of choice and go to your URL. You will notice that you
will need to accept the security warning that the site is &amp;ldquo;untrusted.&amp;rdquo; This is
because you are still using the staging certificates, which are not signed by a
trusted certificate authority (CA).&lt;/p&gt;
&lt;p&gt;Once you ensure that you can log in and that Harbor is working as intended, you
can move to production certificates.&lt;/p&gt;
&lt;h2 id=&#34;set-up-lets-encrypt-production-certificates&#34;&gt;Set Up Let&amp;rsquo;s Encrypt Production Certificates&lt;/h2&gt;


&lt;div class=&#34;aside aside-warning&#34;&gt;
    &lt;div class=&#34;aside aside-title&#34;&gt;
        &lt;i class=&#34;fas fa-exclamation-circle&#34;&gt;&lt;/i&gt;
        &lt;div&gt;Rate Limits&lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#34;aside aside-content&#34;&gt;
    &lt;p&gt;Reminder, if using &lt;code&gt;.xip.io&lt;/code&gt; you may encounter rate limit issues with Lets
Encrypt causing major delays in the certificate issuance.&lt;/p&gt;

    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Similar to how you set up the Let&amp;rsquo;s Encrypt staging certificate, you now need to
create the &lt;code&gt;ClusterIssuer&lt;/code&gt; for production certificates. First, &lt;code&gt;echo&lt;/code&gt; the
following to create the file.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cat &lt;span class=&#34;s&#34;&gt;&amp;lt;&amp;lt; EOF &amp;gt; letsencrypt-prod.yaml
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;apiVersion: cert-manager.io/v1alpha2
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;kind: ClusterIssuer
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;metadata:
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;  name: letsencrypt-prod
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;spec:
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;  acme:
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;    email: $EMAIL_ADDRESS
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;    privateKeySecretRef:
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;      name: letsencrypt-prod
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;    server: https://acme-v02.api.letsencrypt.org/directory
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;    solvers:
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;    - http01:
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;        ingress:
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;          class: contour
&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;EOF&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Then apply it to your Kubernetes cluster.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl apply -f letsencrypt-prod.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Again, you can confirm this process completed successfully by running the
following and ensuring the cluster issuer was issued.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl get clusterissuers
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Recall how earlier in the annotations of the Harbor &lt;code&gt;values.yml&lt;/code&gt; file you told
Harbor to use the &lt;code&gt;letsencrypt-staging&lt;/code&gt; cluster issuer, as well as the secret
&lt;code&gt;harbor-tls-staging&lt;/code&gt;. You must now tell Harbor to use the production cluster
issuer you just created, and trigger it to create a new secret based on that
certificate. To do this, you are going to update the &lt;code&gt;harbor-values.yaml&lt;/code&gt; file
using &lt;code&gt;sed&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sed -i &#39;s/-staging/-prod/&#39; harbor-values.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Run &lt;code&gt;helm delete&lt;/code&gt; then &lt;code&gt;helm install&lt;/code&gt; to uninstall and reinstall Harbor:&lt;/p&gt;
&lt;p&gt;Note: The persistent volumes are not deleted during the &lt;code&gt;helm delete&lt;/code&gt; so any
changes in Harbor you may have made should persist.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;helm delete -n harbor harbor
helm install harbor bitnami/harbor -f harbor-values.yaml -n harbor --version 9.4.4
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Wait for the new Pods to become &lt;code&gt;&amp;lt;READY&amp;gt;&lt;/code&gt;. This may take a minute or two.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;watch kubectl get pods -n harbor
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This process may take as little as a minute or as long as an hour. It just
depends on the server load at that time. But in most of my testing, it took less
than 10 minutes.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;watch kubectl get certificate harbor-tls-prod -n harbor
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once the certificates are generated successfully, your Harbor instance should be
up and running with valid and trusted TLS certificates. Try logging into Harbor
again.&lt;/p&gt;
&lt;p&gt;Your browser should no longer present a warning, as the certificate you are now
using is signed by a trusted CA.&lt;/p&gt;
&lt;p&gt;Note: If you still see certificate warnings, you may need to re-open it from a
fresh browser.&lt;/p&gt;
&lt;h2 id=&#34;test-harbor&#34;&gt;Test Harbor&lt;/h2&gt;
&lt;p&gt;Run &lt;code&gt;docker login&lt;/code&gt; to log into your new registry:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;docker login https://registry.&lt;span class=&#34;nv&#34;&gt;$DOMAIN&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Push an image to the new registry:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;docker pull nginx:latest
docker tag nginx:latest registry.&lt;span class=&#34;nv&#34;&gt;$DOMAIN&lt;/span&gt;/library/nginx:latest
docker push registry.&lt;span class=&#34;nv&#34;&gt;$DOMAIN&lt;/span&gt;/library/nginx:latest
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Test that Kubernetes can access the new image:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl create deployment nginx --image&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;registry.&lt;span class=&#34;nv&#34;&gt;$DOMAIN&lt;/span&gt;/library/nginx:latest
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Wait a few moments and check that the Pod is running:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ NAME                         READY   STATUS    RESTARTS   AGE
pod/nginx-7cdffc88cf-p8v9r   1/1     Running   &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;          5s

NAME                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;S&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;   AGE
service/kubernetes   ClusterIP   10.123.240.1   &amp;lt;none&amp;gt;        443/TCP   166m

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nginx   1/1     &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;            &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;           5s

NAME                               DESIRED   CURRENT   READY   AGE
replicaset.apps/nginx-7cdffc88cf   &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;         &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;         &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;       5s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;That&amp;rsquo;s it! You now have a service running in Kubernetes with TLS encryption
enforced and certificate generation automated. And by using these patterns, you
should be able to install and configure other services as well. Specific steps,
especially around the configuration of the service itself, will be different,
but after using this guide you should have a leg up in getting started.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: Deploy from a Private Helm Repository Using Kubeapps</title>
      
      <link>/guides/kubernetes/kubeapps-private-repo/</link>
      <pubDate>Mon, 15 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/guides/kubernetes/kubeapps-private-repo/</guid>
      <description>

        
        &lt;p&gt;&lt;a href=&#34;https://github.com/kubeapps/kubeapps/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Kubeapps&lt;/a&gt; is a web-based UI for
deploying and managing applications in Kubernetes clusters. Kubeapps includes a
built-in catalog of Helm charts and operators continuously maintained and up to
date. Now Kubeapps also provides support for private Helm repositories with
private Docker images. There is an option of associating Docker credentials to
an application repository so that Kubeapps can ensure they are used to pull any
matching private images within a chart. This option is really useful for
enterprise development team since it allows them to have more granular access
control as well as a known good source of images.&lt;/p&gt;
&lt;p&gt;Kubeapps officially supports the following Helm repositories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubeapps/kubeapps/blob/master/docs/user/private-app-repository.md#chartmuseum&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;ChartMuseum&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubeapps/kubeapps/blob/master/docs/user/private-app-repository.md#harbor&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Harbor&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubeapps/kubeapps/blob/master/docs/user/private-app-repository.md#artifactory&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Artifactory Pro&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This tutorial shows you how to create a private project in Harbor, push a
customized Helm chart to your registry and create an application repository to
have your chart ready from the Kubeapps UI to be deployed.&lt;/p&gt;
&lt;p&gt;Watch the following video or keep reading this tutorial to learn more:&lt;/p&gt;
&lt;iframe width=&#34;560&#34; height=&#34;360&#34; src=&#34;https://www.youtube.com/embed/LLw1Ib8IQQk&#34; frameborder=&#34;0&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;h2 id=&#34;assumptions-and-prerequisites&#34;&gt;Assumptions and prerequisites&lt;/h2&gt;
&lt;p&gt;This guide assumes that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You have a Docker environment installed and configured. Learn more about
&lt;a href=&#34;https://docs.docker.com/engine/install/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;installing Docker&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;You have a Docker Hub account.
&lt;a href=&#34;https://hub.docker.com/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Register for a free account&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;You have a Kubernetes cluster. Check out our
&lt;a href=&#34;https://docs.bitnami.com/kubernetes/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Getting Started with Kubernetes guides&lt;/a&gt;
for an easy way to get started with one.&lt;/li&gt;
&lt;li&gt;You have administrator access to a preexisting installation of
&lt;a href=&#34;https://github.com/bitnami/charts/tree/master/bitnami/harbor&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Harbor&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;You have
&lt;a href=&#34;https://docs.bitnami.com/kubernetes/get-started-kubernetes/#step-4-install-helm&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Helm installed in your cluster&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;You have
&lt;a href=&#34;https://github.com/kubeapps/kubeapps/blob/master/docs/user/getting-started.md&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Kubeapps installed in your cluster&lt;/a&gt;
and are logged into the Kubeapps UI with admin credentials.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;step-1-create-a-private-project-in-harbor&#34;&gt;Step 1: Create a private project in Harbor&lt;/h2&gt;
&lt;p&gt;The first step is to create a project in Harbor. To do so:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Log in to Harbor.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the &amp;ldquo;Projects&amp;rdquo; section, click &amp;ldquo;+ New Project&amp;rdquo;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the resulting screen, give a name to your project. This should be private
so don&amp;rsquo;t activate the &amp;ldquo;Public&amp;rdquo; check. To get an unlimited storage quota, set
that value as -1. Click &amp;ldquo;OK&amp;rdquo; to proceed.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/kubeapps-private-repo/harbor-create-new-project.png&#34; alt=&#34;Create a private project in Harbor&#34;  /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;step-2-pull-the-docker-image-and-push-it-to-your-private-harbor-registry&#34;&gt;Step 2: Pull the Docker image and push it to your private Harbor Registry&lt;/h2&gt;
&lt;p&gt;Next, pull the Docker image of the chart you want to add to your private
repository. Then, you need to push it to Harbor to make it available in your
project. Follow these steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Execute the following command to obtain the latest Bitnami Ghost image:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;docker pull bitnami/ghost:3.13.2-debian-10-r0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Tag the image by executing the command below. Remember to replace the
&lt;code&gt;HARBOR_DOMAIN_NAME&lt;/code&gt; placeholder with the domain name where Harbor is installed.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;docker tag docker.io/bitnami/ghost:3.13.2-debian-10-r0 HARBOR_DOMAIN_NAME/project-private/ghost:3.13.2-debian-10-r0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Login in to Harbor.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;docker login HARBOR_DOMAIN_NAME
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Push the image to your registry by executing this command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;docker push HARBOR_DOMAIN_NAME/project-private/ghost:3.13.2-debian-10-r0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You should see an output message similar to this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;The push refers to repository &lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;harbor.bkpr-kubeapps-gke.nami.run/project-private/ghost&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;
325a01bfb407: Preparing
17d308e7f8c1: Preparing
0a621af6678f: Preparing
2e366bd4c478: Preparing
315ad5c0230e: Preparing
1cfb963e6dd2: Waiting
4e78eb629a01: Waiting
f2e5c6cb0141: Waiting
8bce1f8ba802: Waiting
7d1d696c2212: Waiting
9f729ba7c732: Waiting
e048dd4e8543: Waiting
3.13.2-debian-10-r0: digest: sha256:9121f532fbe28f8e6d4cb11bf542374689c4595378ef83adeda5bff46731d972 size: &lt;span class=&#34;m&#34;&gt;2839&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Navigate to the Harbor UI and in your project, select the tab &amp;ldquo;Repositories&amp;rdquo;.
You should see the repository that contains the image you just pushed. Click
on it to check image details:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/kubeapps-private-repo/harbor-images-pushed.png&#34; alt=&#34;Harbor repositories&#34;  /&gt;&lt;/p&gt;
&lt;h2 id=&#34;step-3-enable-a-robot-account-in-your-project&#34;&gt;Step 3: Enable a Robot Account in your project&lt;/h2&gt;
&lt;p&gt;Next step is to enable a Robot Account in your project with access to pull both
Helm charts from the private repositories as well as Docker images in the
private project. To do so:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;From the Harbor UI, navigate to the &amp;ldquo;Robot Account&amp;rdquo; tab in your project and
click &amp;ldquo;+ New Robot Account&amp;rdquo;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the resulting window, give it a name, a description (optional) and in the
&amp;ldquo;Permissions&amp;rdquo; section, activate the &amp;ldquo;Pull&amp;rdquo; check in the Helm Chart line. Click
&amp;ldquo;Save&amp;rdquo; to proceed.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/kubeapps-private-repo/create-robot-account.png&#34; alt=&#34;Create a Robot Account in your project&#34;  /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Once it is created, remember to copy the token in a safe place or export it to file.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/kubeapps-private-repo/robot-account-created.png&#34; alt=&#34;Copy or export to file the token&#34;  /&gt;&lt;/p&gt;
&lt;h2 id=&#34;step-4-customize-your-helm-chart-and-push-it-to-your-private-harbor-registry&#34;&gt;Step 4: Customize your Helm chart and push it to your private Harbor Registry&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Get the Bitnami Ghost Helm chart and change to the chart&amp;rsquo;s directory by
executing the following command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;helm fetch bitnami/ghost --untar &lt;span class=&#34;o&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; ghost
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Edit the &lt;em&gt;values.yaml&lt;/em&gt; file of the chart of the chart so that the
image.registry and image.repository value point to your registry and
repository path respectively:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/kubeapps-private-repo/chart-values-yaml.png&#34; alt=&#34;Modify the chart values.yaml file&#34;  /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Once you have edited those values, package you chart by running:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; ../ &lt;span class=&#34;o&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; helm package ./ghost
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You will see an output message similar to this: &amp;ldquo;Successfully packaged chart&amp;rdquo;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;From the Harbor UI, navigate to the &amp;ldquo;Helm Charts&amp;rdquo; tab and click &amp;ldquo;Upload&amp;rdquo;.
Browse the resultant &lt;em&gt;tgz&lt;/em&gt; file of your packaged chart and click &amp;ldquo;Upload&amp;rdquo;. You
will see your Helm Chart uploaded in a few minutes:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/kubeapps-private-repo/harbor-helm-chart.png&#34; alt=&#34;Harbor Helm chart uploaded&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;You can click on it to check more information about the chart:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/kubeapps-private-repo/harbor-chart-details.png&#34; alt=&#34;Harbor Helm chart details&#34;  /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now that you have both the Ghost image and its Helm chart available in your
Harbor private repository, it is time to create an application repository in
Kubeapps to start deploying your charts on Kubernetes from its dashboard.&lt;/p&gt;
&lt;h2 id=&#34;step-5-create-an-application-repository-to-enable-your-harbors-private-repository-in-kubeapps&#34;&gt;Step 5: Create an application repository to enable your Harbor&amp;rsquo;s private repository in Kubeapps&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Log in to Kubeapps.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Select the namespace where the repository (and the secret) are to be created.
This should be different from the &lt;em&gt;kubeapps&lt;/em&gt; namespace.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;From the menu button in the top right corner, select the &amp;ldquo;App Repositories&amp;rdquo;
option, then click the &amp;ldquo;Add App Repository&amp;rdquo; button.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the resulting screen enter the following information:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Application repository name&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;URL: private repository URL&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Repository Authorization: select the &amp;ldquo;Basic Auth&amp;rdquo; option and enter as
&amp;ldquo;Username&amp;rdquo; the name you gave to the Robot Account created in Harbor, and as
&amp;ldquo;Password&amp;rdquo;, the token you obtain at the time of the creation. This way,
Kubeapps will be able to see the charts you have pulled into your Harbor
repository.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Associate Docker Registry Credentials: click &amp;ldquo;Add New Credentials&amp;rdquo; to add
the credentials that will allow Kubernetes to pull images from your private
repository. Add the values below, then click &amp;ldquo;Submit&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Secret name&lt;/li&gt;
&lt;li&gt;Server: Harbor&amp;rsquo;s server domain&lt;/li&gt;
&lt;li&gt;Username: in this case, as you created a Robot Account, use its name as username&lt;/li&gt;
&lt;li&gt;Password: use the Robot Account token as password&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/kubeapps-private-repo/app-repo-pull-secret.png&#34; alt=&#34;Add an application repository with the Harbor credentials&#34;  /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Click the &amp;ldquo;Install Repo&amp;rdquo; button to finish the process. You will see your new
application repository in the list of existing application repositories in
your namespace.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/kubeapps-private-repo/app-repositories.png&#34; alt=&#34;List of application repositories&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;If you click the repository link, you will be redirected to its catalog. You
should see your Ghost chart there ready to be deployed:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/kubeapps-private-repo/private-repo-catalog.png&#34; alt=&#34;Private repository application catalog&#34;  /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;step-6-deploy-your-custom-ghost-from-the-kubeapps-ui&#34;&gt;Step 6: Deploy your custom Ghost from the Kubeapps UI&lt;/h2&gt;
&lt;p&gt;Finally, you are able to install your custom application from your private
registry on Kubernetes using the Kubeapps UI.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In the application repository catalog you just created, click the Ghost entry
to go to the chart page.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;On the resulting screen, you can learn about the Ghost chart, the repository
where it is located, review older versions, and any related links. Click
&amp;ldquo;Deploy&amp;rdquo; to deploy the chart:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/kubeapps-private-repo/deploy-ghost.png&#34; alt=&#34;Deploy Ghost from your private repository&#34;  /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;This will take you to a page where you can configure your Ghost deployment.
You can use either the &amp;ldquo;Form&amp;rdquo; or the &amp;ldquo;YAML&amp;rdquo; tab to customize your deployment
as you want: give your chart a name, change the version you want to deploy,
add an admin password (if not, a random 10-character alphanumeric string will
be set), or configure Helm values.&lt;/p&gt;


&lt;div class=&#34;aside aside-info&#34;&gt;
    &lt;div class=&#34;aside aside-title&#34;&gt;
        &lt;i class=&#34;fas fa-exclamation-circle&#34;&gt;&lt;/i&gt;
        &lt;div&gt;Important&lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#34;aside aside-content&#34;&gt;
    &lt;p&gt;The Ghost chart requires a resolvable host. Specify it in the &amp;ldquo;Hostname&amp;rdquo; section.&lt;/p&gt;

    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/kubeapps-private-repo/ghost-values-kubeapps.png&#34; alt=&#34;Ghost values&#34;  /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Click &amp;ldquo;Submit&amp;rdquo; to start the application deployment. Once submitted, you will
be redirected to a page that describes the state of your deployment. The
status will be &amp;ldquo;Deploying&amp;rdquo; until Ghost is up and running.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/kubeapps-private-repo/ghost-deployment.png&#34; alt=&#34;Ghost deployment&#34;  /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Once the chart is deployed, you can see all the deployment details, including
the URLs to access the application.&lt;/p&gt;
&lt;p&gt;By default, Ghost creates a Service with LoadBalancer type to provide an
externally accessible URL for its web interface. Depending on your cloud
provider of choice, the load balancer can take some time to provision and will
stay in a &amp;ldquo;Pending&amp;rdquo; state until it is available. If using Minikube, you will
need to run minikube tunnel in a new terminal window in order for an IP
address to be assigned.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After some time, the URL should be visible in the Access URL table. Once it is
visible, click one of the URLs shown to access your freshly deployed Ghost
blog.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/kubeapps-private-repo/ghost.png&#34; alt=&#34;Ghost home page&#34;  /&gt;&lt;/p&gt;
&lt;h2 id=&#34;useful-links&#34;&gt;Useful links&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubeapps/kubeapps&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Kubeapps Github repository&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubeapps/kubeapps/blob/master/docs/user/private-app-repository.md#harbor&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Using a Private Repository with Kubeapps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://goharbor.io/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Harbor&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: Application Enhancements</title>
      
      <link>/guides/kubernetes/app-enhancements/</link>
      <pubDate>Tue, 16 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/guides/kubernetes/app-enhancements/</guid>
      <description>

        
        &lt;p&gt;Kubernetes does not demand specifics about the applications that run on top of
it. They don&amp;rsquo;t not need to be microservices, 12 factor, or maintain other specific
software philosophies. However, for an application to run well on Kubernetes, there
are aspects of your application you may wish to reconsider.&lt;/p&gt;
&lt;p&gt;Kubernetes is a distributed system that has behaviors different from what many
are used to in a traditional environment. These include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Workloads must be packaged in a container.&lt;/li&gt;
&lt;li&gt;Workloads may be moved (stopped and recreated) based on the needs of the
system.&lt;/li&gt;
&lt;li&gt;Workload IP addresses are [generally] ephemeral.&lt;/li&gt;
&lt;li&gt;Workloads have no affinity to hosts.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Aside from the above constraints, how your application is developed and run is
up to you. There are multiple considerations you can make to comply with more
idiomatic Kubernetes software practices. The following checklist covers the
considerations for running an application in Kubernetes.&lt;/p&gt;
&lt;h2 id=&#34;popular-tooling--approaches&#34;&gt;Popular Tooling &amp;amp; Approaches&lt;/h2&gt;
&lt;h3 id=&#34;application-probes&#34;&gt;Application Probes&lt;/h3&gt;
&lt;p&gt;Kubernetes supports a multitude of probes that indicate state of a running
workload. For details on implementing probes, see &lt;a href=&#34;../app-enhancements-probing-app-state&#34;&gt;the guide on Probing
Application State&lt;/a&gt;.&lt;/p&gt;
&lt;h4 id=&#34;readiness-probe&#34;&gt;Readiness Probe&lt;/h4&gt;
&lt;p&gt;You should use readiness probes as they gate whether your application is
considered &amp;lsquo;Ready&amp;rsquo; to a cluster. Until applications are &amp;lsquo;Ready&amp;rsquo; they do not
receive traffic. A simple HTTP check that reports ready, post initialization is
often adequate.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/app-enhancements/readiness-probe.png&#34; alt=&#34;Readiness Probe&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;For implementation details, see &lt;a href=&#34;../app-enhancements-probing-app-state&#34;&gt;the guide on Probing Application
State&lt;/a&gt;.&lt;/p&gt;
&lt;h4 id=&#34;liveness-probe&#34;&gt;Liveness Probe&lt;/h4&gt;
&lt;p&gt;You should consider liveness probes as a &amp;lsquo;safety&amp;rsquo; check for your application to
ensure it hasn&amp;rsquo;t halted operation unexpectedly. These checks should &lt;strong&gt;not&lt;/strong&gt; rely
on external dependencies as you don&amp;rsquo;t want your application to restart due to
external issues.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/app-enhancements/liveness-probe.png&#34; alt=&#34;Liveness Probe&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;For implementation details, see &lt;a href=&#34;../app-enhancements-probing-app-state&#34;&gt;the guide on Probing Application
State&lt;/a&gt;.&lt;/p&gt;
&lt;h4 id=&#34;startup-probe&#34;&gt;Startup Probe&lt;/h4&gt;
&lt;p&gt;You should consider a startup probe for legacy applications that require a
larger amount of time for their initial startup. This probe only executes on
first start-up and does not continue probing over time (like readiness and
liveness). After a startup probe succeeds, if configured, a liveness probe will
continue. Ideally, modern applications &lt;strong&gt;do not&lt;/strong&gt; require this kind of
protection, but it can be helpful when long start ups cannot be worked around.&lt;/p&gt;
&lt;h3 id=&#34;externalized-configuration&#34;&gt;Externalized Configuration&lt;/h3&gt;
&lt;p&gt;In Kubernetes based-platform, you should keep all application configuration
outside of the application / container when possible. This will enable you to
dynamically change how your application runs by altering ConfigMaps and Secrets.
It&amp;rsquo;s also helpful when deploying the application into different context or
environments.&lt;/p&gt;
&lt;p&gt;For details on how to externalize configuration, see the &lt;a href=&#34;../app-enhancements-externalizing-configuration&#34;&gt;externalizing
configuration guide&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;graceful-shutdown-mechanics-handlers&#34;&gt;Graceful Shutdown Mechanics Handlers&lt;/h3&gt;
&lt;p&gt;As Kubernetes moves workloads around the cluster, an instance of your
application can be asked to stop so a new instance of it can be started
elsewhere. If your application requires any amount of graceful shutdown, you
should ensure your application can handle this event.&lt;/p&gt;
&lt;p&gt;When a pod is being deleted, moved, or recreated each container&amp;rsquo;s PID 1 receives
a SIGTERM. The process then has a grace-period to do what it needs and exit.
The default grace-period is 30 seconds; check with your cluster administrator to
understand what your cluster is set to. If the grace period expires, a SIGKILL
is sent. For more details, see the &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Termination of Pods
documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If a process, script, or HTTP endpoints must be called to terminate gracefully,
you can also add a &lt;a href=&#34;https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;preStop
hook&lt;/a&gt;
to your Kubernetes manifest.&lt;/p&gt;
&lt;p&gt;For details on how to gracefully shutdown pods, see the &lt;a href=&#34;../app-enhancements-graceful-shutdown&#34;&gt;graceful shutdown guide&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;structured-logging&#34;&gt;Structured Logging&lt;/h3&gt;
&lt;p&gt;Structured logging allows log aggregation systems to provide a better view of your
applications logs. You should consider this approach for applications running
Kubernetes. Typically it&amp;rsquo;s as simple as creating log-helper functions or
introducing a logging library. For implementation details, see the &lt;a href=&#34;../app-enhancements-logging-practices&#34;&gt;logging
practice guide&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;export-application-metrics&#34;&gt;Export Application Metrics&lt;/h3&gt;
&lt;p&gt;When exposing metrics about your application, it is a common practice to use
an exporter. This allows you to introduce metrics to your application and a
scraping system, such as &lt;a href=&#34;https://prometheus.io&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;prometheus&lt;/a&gt;, to gather those
metrics over time.  There are exporter libraries for most languages. See the
&lt;a href=&#34;https://prometheus.io/docs/instrumenting/clientlibs/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Client Library&lt;/a&gt;
documentation for a list of examples.&lt;/p&gt;
&lt;p&gt;Work with your platform team to determine if scrape-based monitoring system such
as prometheus, Datadog, or Elastic is available.&lt;/p&gt;
&lt;p&gt;For more details, see the &lt;a href=&#34;../app-observability-exporting-metrics&#34;&gt;Exporting Application Metrics
guide&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;trace-points&#34;&gt;Trace Points&lt;/h3&gt;
&lt;p&gt;In service based architectures, a request can move through many
applications and thus across network boundaries multiple times. A trade-off to
these architectures is detecting where issues in the stack are occurring. For
example, where is most of the latency in a request coming from?&lt;/p&gt;
&lt;p&gt;If your workload falls into this architecture, implementing tracing in your
application(s) is worth considering. Often you need a system that lets you view
trace results, before running one, you may wish to check with your platform team
to see if something pre-exists.&lt;/p&gt;
&lt;p&gt;For more details on this concern, see the
&lt;a href=&#34;../app-observability&#34;&gt;Observability section&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: Application Lifecycle</title>
      
      <link>/guides/kubernetes/app-lifecycle/</link>
      <pubDate>Tue, 16 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/guides/kubernetes/app-lifecycle/</guid>
      <description>

        
        &lt;p&gt;The scripts and systems used in the CI/CD pipelines to deploy and update
applications are limited by the Kubernetes resources they can manage.  In many
cases this may be perfectly sufficient.  An update to the image in a Deployment
spec may be all that is required to perform an update of an application, for
example.&lt;/p&gt;
&lt;p&gt;However, this model is often insufficient
when dealing with workloads that are stateful, distributed and/or complex.
For example, an application that stores data in a relational database may require
a database schema update in coordination with an upgrade to the application itself.
This kind of coordination is awkward for a pipeline to orchestrate and usually
requires a series of manual operations to perform an application update or
upgrade.&lt;/p&gt;
&lt;h2 id=&#34;operators&#34;&gt;Operators&lt;/h2&gt;
&lt;p&gt;A Kubernetes operator is a piece of software that runs in your cluster and
manages the deployment and lifecycle management on your behalf.  In this
example you use a custom resource - an extension to the Kubernetes
API - to define the essential characteristics of your application.  The operator
responds by creating the various Kubernetes resources on your behalf.  When
an app upgrade is needed, the pipeline need only update the custom resource that
represents your application.  The operator contains the logic that allows it to
conduct the upgrade in response.  In the relational database-backed example, it
may put the application into a maintenance mode, initiate a backup of the
database, apply the schema update to the database, upgrade the application
version and restore it to normal functioning mode.&lt;/p&gt;
&lt;p&gt;The advantage of using Kubernetes operators is that you can reduce the human
toil from otherwise labor-intensive operations and perform these operations
reliably.  The drawback is that you have another piece of complex software to
develop and maintain.&lt;/p&gt;
&lt;h2 id=&#34;popular-tooling--approaches&#34;&gt;Popular Tooling &amp;amp; Approaches&lt;/h2&gt;
&lt;h3 id=&#34;kubebuilder&#34;&gt;Kubebuilder&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes-sigs/kubebuilder&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Kubebuilder&lt;/a&gt; is an SDK for
building custom Kubernetes controllers and operators.  It is a CLI you can use
to stamp out boilerplate and scaffolded Go code that is common to all such projects.
The code generated by Kubebuilder includes a Dockerfile, deployment manifests and a
Makefile that facilitates local development.  In addition to the base project
codebase, Kubebuilder can be used to add new APIs as CRDs along with their
corresponding controllers.&lt;/p&gt;
&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;is &lt;a href=&#34;https://book.kubebuilder.io/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;well documented&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;can include admission webhooks for your CRDs&lt;/li&gt;
&lt;li&gt;has experimental support for
&lt;a href=&#34;https://github.com/kubernetes-sigs/kubebuilder/tree/master/plugins&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;plugins&lt;/a&gt;
to build operators using alternative patterns&lt;/li&gt;
&lt;li&gt;is maintained and controlled by upstream Kubernetes SIG&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;requires the Go programming language&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;metacontroller&#34;&gt;Metacontroller&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/app-lifecycle/metacontroller.png&#34; alt=&#34;metacontroller&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/GoogleCloudPlatform/metacontroller&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Metacontroller&lt;/a&gt; or, the
more actively maintained fork &lt;a href=&#34;https://github.com/AmitKumarDas/metac&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;metac&lt;/a&gt;, is
a cluster add-on that runs in your cluster and takes care of the common
operations all controllers and operators must do.  Whereas Kubebuilder stamps
out boilerplate code for these common operations, metacontroller abstracts
those operations away into a separate workload.  Metacontroller manages all
interactions with the Kubernetes API.  The custom controller&amp;rsquo;s logic is
developed in what metacontroller calls the &amp;ldquo;lambda controller&amp;rdquo;.  The presence
and nature of the lambda controller is defined in a metacontroller custom
resource, for example the &amp;ldquo;Composite Controller&amp;rdquo;.  In the case of a Composite
Controller, the &amp;ldquo;parent resource&amp;rdquo; is the resource that is watched and which
triggers reconciliation.  When a change occurs in the parent resource,
metacontroller calls the lambda controller and passes that parent resource
object as JSON.  The lambda controller responds with the &amp;ldquo;child resource&amp;rdquo; as
JSON and metacontroller makes the change to the actual child resource in the API.&lt;/p&gt;
&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;lambda controllers can be developed in any language that can expose an HTTP
endpoint to metacontroller and use JSON&lt;/li&gt;
&lt;li&gt;smaller codebase for your controller logic&lt;/li&gt;
&lt;li&gt;fast to get started - great for prototyping&lt;/li&gt;
&lt;li&gt;can run multiple controllers behind a single metacontroller instance&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;your controller has the additional dependency and operational overhead of
metacontroller running in the cluster&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;operator-sdk&#34;&gt;Operator SDK&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/operator-framework/operator-sdk&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Operator SDK&lt;/a&gt; is a project
originally started at CoreOS that is similar to Kubebuilder in that it uses a
CLI to generate boilerplate and scaffolded code for a new project.  It is a
component of Red Hat&amp;rsquo;s &lt;a href=&#34;https://github.com/operator-framework&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Operator Framework&lt;/a&gt;
which could make it attractive if you are using other components of that toolkit.
Due to the overlap between Kubebuilder and the Go-based operator support in
Operator SDK, those features will be &lt;a href=&#34;https://github.com/kubernetes-sigs/kubebuilder/blob/master/designs/integrating-kubebuilder-and-osdk.md&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;merged into
Kubebuilder&lt;/a&gt;
in the future.  Operator SDK should only be considered where value can be
derived from the Ansible or Helm features.&lt;/p&gt;
&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;allows you to develop operators in Go, Ansible or Helm&lt;/li&gt;
&lt;li&gt;offers integrations with lifecycle manager from the Operator Framework&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;is managed by Red Hat for their ecosystem so may not fit your needs well if
not using OpenShift&lt;/li&gt;
&lt;li&gt;the Go-based operator features will be merged into Kubebuilder&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: Application Observability</title>
      
      <link>/guides/kubernetes/app-observability/</link>
      <pubDate>Tue, 16 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/guides/kubernetes/app-observability/</guid>
      <description>

        
        &lt;p&gt;The term &amp;ldquo;observability&amp;rdquo; in control theory states that the system is observable
if the internal states of the system and its behavior can be determined by only
looking at its inputs and outputs.&lt;/p&gt;
&lt;p&gt;In software, observability means we can answer most questions about a system&amp;rsquo;s
status and performance by looking from the outside. The system has been
instrumented to externalize and make available measurements useful to those
responsible for the platform&amp;rsquo;s success and reliability.&lt;/p&gt;
&lt;p&gt;Observability aims to provide highly granular insights into the behavior of
production systems along with rich context, perfect for debugging and
performance analysis.&lt;/p&gt;
&lt;h2 id=&#34;health-checks&#34;&gt;Health checks&lt;/h2&gt;
&lt;p&gt;Health checks (often custom HTTP endpoints) help orchestrators, like Kubernetes,
perform automated actions to maintain overall system health. These can be a
simple HTTP route that returns meaningful values, or a command that can be
executed from within the container.&lt;/p&gt;
&lt;h2 id=&#34;metrics&#34;&gt;Metrics&lt;/h2&gt;
&lt;p&gt;Metrics are a numeric representation of data that is collected at intervals into
a time series. Numerical time series data is easy to store and query, which
helps when looking for historical trends. Over a longer period of time,
this numerical data can be compressed into less granular aggregates like daily
or weekly, for example. This allows for longer retention periods for
historic purposes.&lt;/p&gt;
&lt;h2 id=&#34;logging&#34;&gt;Logging&lt;/h2&gt;
&lt;p&gt;Log entries represent discrete events. Log entries are essential for debugging,
as they often include stack traces and other contextual information that can
help identify the root cause of observed failures.&lt;/p&gt;
&lt;p&gt;Logging is used when the developer wants to explicitly output some message for
someone to see. It is coded directly into executable code, including passing
along values of relevant variables. When problems occur, the logs are useful for
debugging purposes, showing where a failure occurred, such as a stack trace for
an exception that got thrown.&lt;/p&gt;
&lt;h2 id=&#34;distributed-tracing&#34;&gt;Distributed Tracing&lt;/h2&gt;
&lt;p&gt;Distributed, request, or end-to-end tracing captures the end-to-end flow of a
request through the system. Tracing essentially captures both relationships
between services (the services the request touched), and the structure of work
through the system (synchronous or asynchronous processing, child-of or
follows-from relationships). Tracing is something unique to the cloud-native
world, allowing developers and operators to track the exchanges between
different microservices.&lt;/p&gt;
&lt;p&gt;In order to enable distributed tracing, an application needs to be &amp;ldquo;instrumented&amp;rdquo;
by adding and configuring distributed tracing client libraries. For example, you
can configure application to send trace records to an Open Tracing compliant
trace server whenever any JAX-RS annotated method is invoked. This way you have
an audit record of what got called when, by whom, and how long it took. By
adding Open Tracing annotations to traced methods you can also include
information about what private methods have been called in your code for
example.&lt;/p&gt;
&lt;h2 id=&#34;popular-tooling--approaches&#34;&gt;Popular Tooling &amp;amp; Approaches&lt;/h2&gt;
&lt;h3 id=&#34;health-checks-1&#34;&gt;Health checks&lt;/h3&gt;
&lt;p&gt;Kubernetes exposes the following health probes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Readiness -  whether application is considered &amp;lsquo;Ready&amp;rsquo; to a cluster.&lt;/li&gt;
&lt;li&gt;Liveness - safety check to ensure that application hasn&amp;rsquo;t halted operation unexpectedly.&lt;/li&gt;
&lt;li&gt;Startup - for applications that require a larger amount of time for their initial startup.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Use above health endpoints based on application&amp;rsquo;s requirements.&lt;/p&gt;
&lt;p&gt;For implementation details, see &lt;a href=&#34;../app-enhancements-probing-app-state&#34;&gt;the guide on Probing Application
State&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;metrics-1&#34;&gt;Metrics&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://prometheus.io/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Prometheus&lt;/a&gt; is an open-source systems monitoring and
alerting toolkit. It works well for recording any purely numeric time series. It
fits both machine-centric monitoring as well as monitoring of highly dynamic
service-oriented architectures. In a world of microservices, its support for
multi-dimensional data collection and querying is a particular strength. We
recommend the &lt;a href=&#34;https://github.com/coreos/prometheus-operator&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Prometheus
operator&lt;/a&gt;, which manages the
lifecycle of prometheus and comes with many sensible defaults.&lt;/p&gt;
&lt;p&gt;Managing Prometheus could become challenging overtime:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data retention - ability to efficiently store data for longer periods of time.&lt;/li&gt;
&lt;li&gt;High-Cardinality Metrics - metrics with dimensions that have many different
values can cause performance degradation.&lt;/li&gt;
&lt;li&gt;Dynamic service scraping - could cause performance degradation.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We recommend the reader to check with their platform team if any metrics
scraping services are available in the platform.&lt;/p&gt;
&lt;p&gt;For more details on this concern, see &lt;a href=&#34;../app-enhancements#export-application-metrics&#34;&gt;the opinion on Export Application
Metrics&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;logging-1&#34;&gt;Logging&lt;/h3&gt;
&lt;p&gt;In most Kubernetes deployments, all logging should happen to standard out and
standard error. Additionally, most platform teams will offer developers a log
shipping solution, so you should check with your platform team to understand
how this works.&lt;/p&gt;
&lt;p&gt;Typical Logging Stack:&lt;/p&gt;
&lt;h4 id=&#34;log-aggregator&#34;&gt;Log Aggregator&lt;/h4&gt;
&lt;p&gt;This component collects logs from pods running on different nodes and route them
to a central location.
&lt;a href=&#34;https://www.fluentd.org/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;fluentd&lt;/a&gt; has become a
popular log aggregator for Kubernetes deployments. It is small, efficient and
has a wide plugin ecosystem.&lt;/p&gt;
&lt;h4 id=&#34;log-collectorstoragesearch&#34;&gt;Log Collector/Storage/Search&lt;/h4&gt;
&lt;p&gt;This component stores the logs from log aggregators and provides an interface to
search logs efficiently. It should also provide storage management and archival
of logs. Ideally, this component should be resilient to node failures, so that
logging does not become unavailable in case of infrastructure failures.
&lt;a href=&#34;https://www.elastic.co/products/elasticsearch&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Elasticsearch&lt;/a&gt; is one of the
options, as it can ingest logs from fluentd, creates inverted indices on
structured log data making efficient search possible, and has multi-master
architecture with ability to shard data for high availability.&lt;/p&gt;
&lt;h4 id=&#34;ui-and-alerting&#34;&gt;UI and Alerting&lt;/h4&gt;
&lt;p&gt;Visualizations are key for log analysis of distributed applications. A good UI
with query capabilities makes it easier to sift through application logs,
correlate and debug issues. Custom dashboards can provide high level overview of
the health of the distributed application.
&lt;a href=&#34;https://www.elastic.co/products/kibana&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Kibana&lt;/a&gt; from Elasticsearch can be used
as the UI for the log storage, and will be explored as an option here. Alerting
is typically an actionable event in the system. It can be set up in conjunction
with logging and monitoring.&lt;/p&gt;
&lt;p&gt;For additional implementation details, see &lt;a href=&#34;../app-enhancements-logging-practices&#34;&gt;the guide on Logging
Practices&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;distributed-tracing-1&#34;&gt;Distributed Tracing&lt;/h3&gt;
&lt;p&gt;Consider using Distributed Tracing in complex multi-service architectures. It
can help with detection of cascading failures in service calls, optimization of
Database requests, latency problems etc.&lt;/p&gt;
&lt;p&gt;We recommend to check with the platform team to see what tracing platform tools
pre-exist / are available.&lt;/p&gt;
&lt;p&gt;Typical Distributed Tracing stack:&lt;/p&gt;
&lt;h4 id=&#34;zop-stack-zipkin-opentracing-prometheus&#34;&gt;ZOP stack (Zipkin, OpenTracing, Prometheus)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://zipkin.io/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Zipkin&lt;/a&gt; - distributed tracing system. Applications need
to be &amp;ldquo;instrumented&amp;rdquo; to report trace data to Zipkin. This usually means
configuration of a &lt;a href=&#34;https://zipkin.io/pages/tracers_instrumentation&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;tracer or instrumentation
library&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://opentracing.io/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;OpenTracing&lt;/a&gt; - vendor-neutral APIs and
instrumentation for distributed tracing.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://prometheus.io/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Prometheus&lt;/a&gt; - open-source systems monitoring and alerting toolkit. Before
you can monitor your services, you need to add instrumentation to their code
via one of the &lt;a href=&#34;https://prometheus.io/docs/instrumenting/clientlibs/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;client
libraries&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;jop-stack-jaeger-opentracing-prometheus&#34;&gt;JOP stack (Jaeger, OpenTracing, Prometheus)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.jaegertracing.io/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Jaeger&lt;/a&gt; - distributed tracing system. It is
used for monitoring and troubleshooting microservices-based distributed
systems. Applications must be instrumented before they can send tracing data
to Jaeger backend. Check the &lt;a href=&#34;https://www.jaegertracing.io/docs/1.16/client-libraries/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Client
Libraries&lt;/a&gt; section
for information about how to use the OpenTracing API and how to initialize and
configure Jaeger tracers.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://opentracing.io/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;OpenTracing&lt;/a&gt; - vendor-neutral APIs and
instrumentation for distributed tracing.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://prometheus.io/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Prometheus&lt;/a&gt; - open-source systems monitoring and alerting toolkit. Before
you can monitor your services, you need to add instrumentation to their code
via one of the &lt;a href=&#34;https://prometheus.io/docs/instrumenting/clientlibs/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;client
libraries&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: Carvel</title>
      
      <link>/guides/kubernetes/carvel/</link>
      <pubDate>Wed, 14 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/guides/kubernetes/carvel/</guid>
      <description>

        
        &lt;p&gt;&lt;a href=&#34;https://carvel.dev/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Carvel&lt;/a&gt;&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; is an open source suite of tools. Carvel provides a set of reliable, single-purpose, composable tools that aid in your application building, configuration, and deployment to Kubernetes.&lt;/p&gt;
&lt;p&gt;It currently&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; contains the following tools:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://carvel.dev/ytt/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;ytt&lt;/a&gt;&lt;/strong&gt;: Template and overlay Kubernetes configuration via YAML structures, not text documents. No more counting spaces, or manual quoting.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://carvel.dev/kbld/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;kbld&lt;/a&gt;&lt;/strong&gt;: Build or reference container images in Kubernetes configuration in an immutable way.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://carvel.dev/kapp/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;kapp&lt;/a&gt;&lt;/strong&gt;: Install, upgrade, and delete multiple Kubernetes resources as one &amp;ldquo;application&amp;rdquo;. Be confident your application is fully reconciled.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://carvel.dev/imgpkg/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;imgpkg&lt;/a&gt;&lt;/strong&gt;: Bundle and relocate application configuration (with images) via Docker registries. Be assured app contents are immutable.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/vmware-tanzu/carvel-kapp-controller&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;kapp-controller&lt;/a&gt;&lt;/strong&gt;: Capture application deployment workflow declaratively via App CRD. Reliable GitOps experience powered by kapp.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://carvel.dev/vendir/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;vendir&lt;/a&gt;&lt;/strong&gt;: Declaratively state what files should be in a directory.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;learn-more&#34;&gt;Learn more&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;/%28https:/carvel.dev/%29&#34;&gt;carvel.dev&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Guides&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;/guides/kubernetes/kapp-gs/&#34;&gt;Getting Started with kapp&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Workshops&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://tanzu.vmware.com/developer/workshops/lab-getting-started-with-carvel/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Getting Started with Carvel&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Formerly k14s&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;As of April 13, 2021&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: Container Networking</title>
      
      <link>/guides/kubernetes/container-networking/</link>
      <pubDate>Wed, 24 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/guides/kubernetes/container-networking/</guid>
      <description>

        
        &lt;p&gt;Kubernetes uses the &lt;a href=&#34;https://github.com/containernetworking/cni&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Container Network
Interface&lt;/a&gt; (CNI) to provide
networking functionality to containers. Networking is implemented in CNI
plugins. The interface / plugin model enables Kubernetes to support many
networking options implemented via plugins such as Calico, Antrea, and Cilium.&lt;/p&gt;
&lt;p&gt;Anyone may write a CNI-plugin. The expectation is the plugin will support
specific operations defined in the specification (e.g.
&lt;a href=&#34;https://github.com/containernetworking/cni/blob/spec-v0.4.0/SPEC.md#parameters&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;0.4.0&lt;/a&gt;).
These operations include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ADD&lt;/code&gt;: Add a container to the network.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;DEL&lt;/code&gt;: Delete a container from the network.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;CHECK&lt;/code&gt;: Check whether the container&amp;rsquo;s network is as expected.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;CNI Plugins are often only concerned with container to container networking.
Kubernetes constructs such as
&lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/service/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;services&lt;/a&gt; are
still handled by kube-proxy. This means selecting a target pod for a service may
still happen via IPtables (round-robin) on the host. Once the target pod is
selected, the networking facilitated by the CNI plugin will pick up from there.
Some plugins replace kube-proxy for their own alternative implementation.
&lt;a href=&#34;https://cilium.io/blog/2019/08/20/cilium-16&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Cilium&lt;/a&gt;, for example, has replaced
kube-proxy for an implementation using
&lt;a href=&#34;https://en.wikipedia.org/wiki/Berkeley_Packet_Filter&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;BPF&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Enforcement of &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/network-policies&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;network
policy&lt;/a&gt;
is contingent on your choice of plugin. Some plugins, such as
&lt;a href=&#34;https://github.com/coreos/flannel&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Flannel&lt;/a&gt;, do not enforce policy at all. This
means that adding network policy objects to your cluster will have no impact.
Most plugins, however, do enforce policy. Policy can be set for both ingress
traffic to pods and egress traffic from pods. Kubernetes has a policy API that
plugins may choose to respect. Some plugins, such as
&lt;a href=&#34;https://docs.projectcalico.org/v3.11/reference/resources/networkpolicy&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Calico&lt;/a&gt;
and &lt;a href=&#34;https://docs.cilium.io/en/v1.6/kubernetes/policy&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Cilium&lt;/a&gt; offer extended
APIs through Custom Resource Definitions (CRDs) that provide additional
functionality. Kubernetes network policy is namespace scoped with no ability to
apply cluster-wide policy. The only CNI-plugin that offers cluster-level policy
is Calico&amp;rsquo;s
&lt;a href=&#34;https://docs.projectcalico.org/v3.11/reference/resources/globalnetworkpolicy&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;GlobalNetworkPolicy&lt;/a&gt;
CRD.&lt;/p&gt;
&lt;p&gt;Choosing a CNI Plugin comes down to your network topology, desired container
networking features, and understanding of different routing protocols. Plugins
that can be used in Kubernetes have all sorts of routing features such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;BGP for route sharing&lt;/li&gt;
&lt;li&gt;Tunneling protocols (VXLAN, GRE, IP-in-IP, and more)&lt;/li&gt;
&lt;li&gt;Native routing (no encapsulation)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;popular-tooling-and-approaches&#34;&gt;Popular Tooling and Approaches&lt;/h2&gt;
&lt;h3 id=&#34;calico&#34;&gt;Calico&lt;/h3&gt;
&lt;p&gt;Calico has been one of the predominant CNI-plugins since Kubernetes became
popular. It supports a variety of routing modes including IP-in-IP, VXLAN, and
Native (non-encapsulated). It even supports versatile routing options such as
only encapsulating when crossing subnet boundaries. With Calico&amp;rsquo;s native-routing
abilities, it does not need to incur encapsulation overhead. This means Calico
can achieve near native network speeds.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Border_Gateway_Protocol&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;BGP&lt;/a&gt; is used to
distribute routes when running in IP-in-IP or native routing modes. BGP is a
well known route sharing protocol that many enterprise networks are capable of
peering with. This enables enterprises to integrate pod networks into their
networking fabric, making pods routable. This capability unlocks a multitude of
network topologies.&lt;/p&gt;
&lt;p&gt;Network Policy support in Calico is arguably the strongest of all CNI plugins.
Historically, the work Calico did with network policy influenced Kubernetes
adoption of those constructs. For example, Calico implemented egress policy
before the Kubernetes project did. Along with full support for Kubernetes
network policy, Calico offers its own
&lt;a href=&#34;https://docs.projectcalico.org/v3.11/reference/resources/networkpolicy&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;NetworkPolicy&lt;/a&gt;
and
&lt;a href=&#34;https://docs.projectcalico.org/v3.11/reference/resources/globalnetworkpolicy&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;GlobalNetworkPolicy&lt;/a&gt;
CRDs. These network policies offer advanced features. The global policy enables
administrators to apply cluster-wide policies. Calico also supports mixing both
Kubernetes native policies and its own CRDs.&lt;/p&gt;
&lt;p&gt;Calico is the most common CNI-plugin we have seen in deployments. Tigera, the
creators of Calico, offer extended enterprise features and support. Additionally
VMware offers break-fix Calico support for those with the appropriate support
subscription. With the above in mind, Calico is generally our first choice for
CNI-plugin.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Diverse routing mode support.
&lt;ul&gt;
&lt;li&gt;IP-in-IP&lt;/li&gt;
&lt;li&gt;Native&lt;/li&gt;
&lt;li&gt;VXLAN&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Integrates with the Kubernetes API server.
&lt;ul&gt;
&lt;li&gt;No direct etcd access required.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Native routing incurs minimal overhead.
&lt;ul&gt;
&lt;li&gt;Supports cross-subnet only encapsulation.&lt;/li&gt;
&lt;li&gt;Uses native routing for all intra-subnet routing.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;BGP route sharing enables advanced topologies.&lt;/li&gt;
&lt;li&gt;Most capable network policy support.
&lt;ul&gt;
&lt;li&gt;Includes Calico-specific GlobalNetworkPolicy.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Break-fix support offered by VMware.&lt;/li&gt;
&lt;li&gt;External data store not required
&lt;ul&gt;
&lt;li&gt;Uses Kubernetes API.&lt;/li&gt;
&lt;li&gt;Scales with &lt;a href=&#34;https://github.com/projectcalico/typha&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;typha&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;BGP might not be possible in your environment
&lt;ul&gt;
&lt;li&gt;If so, Calico&amp;rsquo;s VXLAN mode does not use BGP.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;antrea&#34;&gt;Antrea&lt;/h3&gt;
&lt;p&gt;Antrea provides container networking based on Open vSwitch. Using Open vSwitch,
Antrea is capable of offering routing via VXLAN, Geneve, GRE, or STT
encapsulation methods. Unlike Calico, Antrea can enforce networking rules inside
Open vSwitch, which should provide more performant policy enforcement relative
to IPtables.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/container-networking/antrea.png&#34; alt=&#34;Antrea&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Those familiar with Open vSwitch are likely to find Antrea a very compelling
option. Since Antrea is still in pre-release, we don&amp;rsquo;t recommend it for
production use cases at this time.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Support many encapsulation protocols.
&lt;ul&gt;
&lt;li&gt;VXLAN&lt;/li&gt;
&lt;li&gt;Geneve&lt;/li&gt;
&lt;li&gt;GRE&lt;/li&gt;
&lt;li&gt;STT&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Familiar to users of Open vSwitch.&lt;/li&gt;
&lt;li&gt;Performant network policy enforcement.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/vmware-tanzu/octant&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Octant&lt;/a&gt; UI support.&lt;/li&gt;
&lt;li&gt;Break-fix support from VMware.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Must have Open vSwitch kernel module.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;nsx-t&#34;&gt;NSX-T&lt;/h3&gt;
&lt;p&gt;NSX-T is an extremely capable network virtualization technology. It is used to
facilitate networking in datacenters around the world. As such, it is very
uncommon to see the NSX-T CNI plugin used unless an existing NSX-T deployment
exists. Introducing a net new NSX-T deployment for a Kubernetes platform
introduces a lot of operational overhead. It is also common for teams running
Kubernetes on top of vSphere + NSX-T to run a different CNI-plugin (such as
Calico or Flannel) on top of it.&lt;/p&gt;
&lt;p&gt;With this in mind, we only recommend considering the NSX-T CNI plugin if there
is an existing NSX-T deployment and a strong desire to integrate container
networking with it directly.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NSX-T is familiar to many VI Admins.&lt;/li&gt;
&lt;li&gt;NSX-T makes container networking feel more like normal VM networking.
&lt;ul&gt;
&lt;li&gt;e.g. Namespaces are assigned their own subnets.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Architecting, Deploying, and Operating NSX-T is a non-trivial task.
&lt;ul&gt;
&lt;li&gt;It can be overkill for just container networking.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Unlike most plugins, you&amp;rsquo;re not running the pod network on top of another.
&lt;ul&gt;
&lt;li&gt;You&amp;rsquo;re instead using the existing network to run the pod network.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cilium&#34;&gt;Cilium&lt;/h3&gt;
&lt;p&gt;Cilium is a powerful CNI-plugin that uses
&lt;a href=&#34;https://en.wikipedia.org/wiki/Berkeley_Packet_Filter&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;BPF&lt;/a&gt; to make routing
decisions in a highly performant manner. Cilium has replaced kube-proxy, which
facilitates services, for it&amp;rsquo;s own eBPF implementation. This makes service
routing decisions O(1) rather than the time complexity it takes to traverse many
IPtables chain rules.&lt;/p&gt;
&lt;p&gt;Cilium&amp;rsquo;s network policy enforcement also uses BPF. Calico leverages IPtables,
which can have its own scalability issues and troubleshooting complexity.
Overall, Cilium appears to be a very promising CNI in the Kubernetes ecosystem.&lt;/p&gt;
&lt;p&gt;Cilium is new to the ecosystem and does not have as many production stories as
we&amp;rsquo;d expect before recommending it. It may be worth considering for lab
environments, depending on your tolerance for risk and comfort with BPF, or
you may want to hold off a little longer.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;BPF enables extremely fast routing decisions.
&lt;ul&gt;
&lt;li&gt;Services&lt;/li&gt;
&lt;li&gt;Network Policy&lt;/li&gt;
&lt;li&gt;Routing&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Community support and enthusiasm is growing.
&lt;ul&gt;
&lt;li&gt;Likely to become a predominant CNI-plugin.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Need to ensure BPF kernel support.
&lt;ul&gt;
&lt;li&gt;Sometimes not possible in highly regulated environments.&lt;/li&gt;
&lt;li&gt;Especially with older versions of RHEL.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;You cannot mix Cilium network policy with Kubernetes network policy.&lt;/li&gt;
&lt;li&gt;Newer player in the ecosystem, still experiencing paper cuts.
&lt;ul&gt;
&lt;li&gt;CRD-based backend not scalable yet.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: Developer Workflow</title>
      
      <link>/guides/kubernetes/dev-workflow/</link>
      <pubDate>Tue, 09 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/guides/kubernetes/dev-workflow/</guid>
      <description>

        
        &lt;p&gt;The developer workflow typically involves writing code, executing automated
tests, building the application, and running the app locally. In most cases,
developers repeat these steps throughout the day, creating a development cycle.
The efficiency of the development cycle has a direct impact on the time it takes
development teams to ship new features and fix bugs. For this reason, minimizing
the time it takes to iterate through the cycle is desirable.&lt;/p&gt;
&lt;p&gt;In the context of the development workflow, Kubernetes only impacts the final
steps of the cycle, which involve running the application. Depending on the
complexity of the application, developers might choose to run it in a
development Kubernetes cluster that mimics a production environment. In doing
so, developers leverage Kubernetes deployment manifests to deploy their
application alongside its dependencies, such as other services, databases, and
message queues.&lt;/p&gt;
&lt;p&gt;The ability to deploy and test applications in a production-like environment is
one of the most significant benefits of including Kubernetes in the development
cycle. Without appropriate processes and tooling, however, Kubernetes could
hinder developers&#39; efficiency, as they would need to add new steps to their
cycle. (Steps such as building container images and running kubectl commands to
deploy new containers).&lt;/p&gt;
&lt;h2 id=&#34;application-development-cycle-on-kubernetes&#34;&gt;Application Development Cycle on Kubernetes&lt;/h2&gt;
&lt;p&gt;For simple projects it is preferable to keep Kubernetes out of a developer&amp;rsquo;s
workflow. As a starting point, a development cycle might involve changing code
and rerunning an executable on one&amp;rsquo;s local machine. Containers usually enter the
development process as a means of easily running dependencies with pinned
versions (for example a SQL database). As the application grows and multiple
containers are needed, it becomes useful to codify the development environment.&lt;/p&gt;
&lt;p&gt;As more services are introduced it becomes increasingly attractive to favor
Kubernetes API manifests as the means of specifying a development environment.
This allows the developer to reuse &lt;code&gt;deployment.yaml&lt;/code&gt; files which are often
already in place in production environments. At this point there is a high level
of parity between development, staging, and production environments. However, a
new challenge arises: how does the developer efficiently update the application
running on their development cluster every time the code is updated?&lt;/p&gt;
&lt;p&gt;The Kubernetes community and the broader Cloud Native ecosystem offers a variety
of tools that improve the development workflow. Each tool attempts to solve one
of these problems: 1) Running Kubernetes in the developer&amp;rsquo;s local environment,
and 2) Continuously building, pushing, and deploying containers as the developer
changes source code.&lt;/p&gt;
&lt;h2 id=&#34;local-vs-remote-cluster&#34;&gt;Local vs Remote Cluster&lt;/h2&gt;
&lt;p&gt;An organization will eventually need to provide a cluster for
sandbox/development workloads for teams of developers. There are an array of
tools and strategies to provide a cluster for development workloads, but the
first question often asked is &amp;ldquo;Where should my developer&amp;rsquo;s develop?&amp;rdquo; As
organizations move towards a cloud native model, the journey often starts with
simple local development machines and matures towards multi-cluster development
environments hosted on-prem or with a cloud provider.&lt;/p&gt;
&lt;p&gt;The following tools provide developers with a local Kubernetes environment:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kind.sigs.k8s.io/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Kind&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/tasks/tools/install-minikube/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Minikube&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.docker.com/products/docker-desktop&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Docker Desktop&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As the development model matures, on-prem or hosted clusters are used together
with CI/CD tooling to standardize environments and workflows.&lt;/p&gt;
&lt;h2 id=&#34;kubeconfig--multiple-clusters&#34;&gt;Kubeconfig &amp;amp; Multiple Clusters&lt;/h2&gt;
&lt;p&gt;As more clusters are created for teams, projects, or production environments, it
is important to efficiently manage configurations and navigate between clusters.
Your &lt;code&gt;$HOME/.kube/config&lt;/code&gt; file, known as the &lt;code&gt;kubeconfig&lt;/code&gt;, is used to organize
information about clusters, credentials, and namespaces. Below are some common
commands to navigate contexts.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl config view &lt;span class=&#34;c1&#34;&gt;# Show Merged kubeconfig settings.&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# use multiple kubeconfig files at the same time and view merged config&lt;/span&gt;
&lt;span class=&#34;nv&#34;&gt;KUBECONFIG&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;~/.kube/config:~/.kube/kubconfig2


kubectl config get-contexts                &lt;span class=&#34;c1&#34;&gt;# display list of contexts&lt;/span&gt;
kubectl config current-context             &lt;span class=&#34;c1&#34;&gt;# display the current-context&lt;/span&gt;
kubectl config use-context my-cluster-name &lt;span class=&#34;c1&#34;&gt;# set the default context to my-cluster-name&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# permanently save the namespace for all subsequent kubectl commands in that context.&lt;/span&gt;
kubectl config set-context --current --namespace&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;ggckad-s2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Additionally, useful tooling such as
&lt;a href=&#34;https://github.com/ahmetb/kubectx&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;kubectx&lt;/a&gt; can help developers navigate
between contexts.&lt;/p&gt;
&lt;h2 id=&#34;popular-tooling--approaches&#34;&gt;Popular Tooling &amp;amp; Approaches&lt;/h2&gt;
&lt;h3 id=&#34;skaffold&#34;&gt;Skaffold&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/GoogleContainerTools/skaffold&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Skaffold&lt;/a&gt; is a project from Google that bundles multiple steps (build, tag, test, deploy) into a single command.
The tool can watch for changes and re-run the steps as needed.
Skaffold is configurable with pluggable integrations for each step.
It also assists with debugging by aggregating and tailing logs.
In addition to aiding development workflows, Skaffold can also serve double duty as the basis for executing CI/CD pipelines.&lt;/p&gt;
&lt;h3 id=&#34;tilt&#34;&gt;Tilt&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/windmilleng/tilt&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Tilt&lt;/a&gt; is a CLI+GUI project backed by a startup (Windmill Engineering).
Like Skaffold, Tilt supports pluggable templating solutions (Helm, Kustomize, etc).
However, unlike other tools which only present the developer with aggregated logs, Tilt also shows a summarized view of all their running applications.
This is done by displaying a web view of each application, or showing a high-level status for each service (success/error).
The Tilt team also provides a method of sharing snapshots of one&amp;rsquo;s development environment with teammates via TiltCloud.&lt;/p&gt;
&lt;p&gt;Tilt is able to speed up build processes by avoiding in-container builds during development through copying source files (and injecting restart scripts).
This process is more seamless for interpreted languages than for compiled languages where more workarounds are needed.&lt;/p&gt;
&lt;p&gt;Tilt represents a step forward in development workflow tools because it focuses on optimizing feedback loops within a Developer&amp;rsquo;s own environment as well as across a team.
However, Tilt is still an early stage project and the direction of the startup behind the project is not yet clear.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: Identity and Access Control</title>
      
      <link>/guides/kubernetes/identity/</link>
      <pubDate>Wed, 24 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/guides/kubernetes/identity/</guid>
      <description>

        
        &lt;p&gt;In order to deploy Kubernetes securely you need to implement the principle of
least privilege. What this means is that you will allow users to take actions
against the cluster (e.g. create Pods, Services, etc.), but you will ensure that
any privileges that you extend to a user will be constrained to include only
those that are necessary to fulfill the user&amp;rsquo;s needs, and nothing more.&lt;/p&gt;
&lt;p&gt;To ensure that users only have what they need, you will need to know about who
our users are. You will need to both authenticate their credentials, and then
once you have, you will need to ensure that they are allowed to perform the
actions they have requested. Similarly, you will want to be sure that you always
have a consistent view of this user data. This consistent view will ensure that
as users come and go, you always grant them only the access they are entitled
to.&lt;/p&gt;
&lt;p&gt;Many newcomers to Kubernetes are sometimes surprised to learn that Kubernetes
does not have a User resource type. Unlike other platforms in the distributed
computing space, Kubernetes deliberately seeks to offload this functionality
onto other systems.&lt;/p&gt;
&lt;p&gt;While Kubernetes does provide some primitive user management capabilities in the
form of Service Accounts, a production deployment should leverage an
organization-wide user identity store. Often times an organization will have a
common LDAP, Active Directory, or Open ID Connect (OIDC) infrastructure that is
leveraged across its environments. Kubernetes is capable of integrating
(directly or indirectly) with these systems, and doing so will provide that
common user identity integration that you are looking for. When a user leaves
the organization, their access is revoked from this common identity store, and
Kubernetes will in-turn revoke their access to the cluster.&lt;/p&gt;
&lt;h2 id=&#34;open-identity-connect-oidc&#34;&gt;Open Identity Connect (OIDC)&lt;/h2&gt;
&lt;p&gt;If you have examined the Kubernetes documentation, you will notice that there
are quite a few options for integrating user identity systems, but that the only
standard protocol-based configuration involves OIDC.&lt;/p&gt;
&lt;p&gt;With OIDC, a user will authenticate with the identity store, and upon successful
login, will obtain a JSON Web Token (JWT). This token will be presented as a
base64 string, but when decoded, it contains human-readable metadata about the
user. Some of this data may include the username, token issue time, token
expiration time, and most importantly, a field that provides what OIDC calls
&amp;ldquo;claims.&amp;rdquo; These claims are typically an array of strings that indicate which
user groups this user should have access to. These groups will eventually be
mapped to Roles and ClusterRoles for the implementation of Kubernetes RBAC
rules.&lt;/p&gt;
&lt;p&gt;Note that the Kubernetes API server may be configured to specify both a user
claim and a group claim. These fields would correspond to attributes within the
token.&lt;/p&gt;
&lt;p&gt;A sample JWT may look like the following:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;#34;iss&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;https://auth.example.com&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;#34;sub&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Ch5hdXRoMHwMTYzOTgzZTdjN2EyNWQxMDViNjESBWF1N2Q2&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;#34;aud&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;dDblg7xO7dks1uG6Op976jC7TjUZDCDz&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;#34;exp&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1517266346&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;#34;iat&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1517179946&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;#34;at_hash&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;OjgZQ0vauibNVcXP52CtoQ&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;#34;username&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;marysmith&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;#34;email&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;marysmith@example.com&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;#34;email_verified&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;true&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;#34;groups&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;qa&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;infrastructure&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In this example, you would configure the API server to use the &amp;ldquo;username&amp;rdquo;
attribute as the username claim field and the &amp;ldquo;groups&amp;rdquo; attribute as the groups
claim. When developing RBAC rules, you will now be able to utilize these claims
as subjects for RoleBinding and ClusterRoleBinding resources:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;RoleBinding&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;rbac.authorization.k8s.io/v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;web-rw-deployment&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;namespace&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;some-web-app-ns&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;roleRef&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Role&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;web-rw-deployment&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;apiGroup&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;rbac.authorization.k8s.io&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;subjects&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;User&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;marysmith&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;apiGroup&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;rbac.authorization.k8s.io&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;or&amp;hellip;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;ClusterRoleBinding&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;rbac.authorization.k8s.io/v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;web-infra&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;roleRef&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Role&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;web-infra&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;apiGroup&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;rbac.authorization.k8s.io&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;subjects&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Group&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;infrastructure&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;apiGroup&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;rbac.authorization.k8s.io&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&#34;aside aside-info&#34;&gt;
    &lt;div class=&#34;aside aside-title&#34;&gt;
        &lt;i class=&#34;fas fa-exclamation-circle&#34;&gt;&lt;/i&gt;
        &lt;div&gt;What about other protocols?&lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#34;aside aside-content&#34;&gt;
    &lt;p&gt;There are no direct LDAP or Active Directory integrations, but it is possible to
integrate these systems with tools that will act as identity brokers.&lt;/p&gt;
&lt;p&gt;One very common tool for brokering various identity backends is
&lt;a href=&#34;https://github.com/dexidp/dex&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Dex&lt;/a&gt;. This service, deployed in-cluster, will
allow us to connect various backends, such as LDAP, SAML, Active Directory, and
similar to an OIDC front-end. Kubernetes may then be configured to utilize Dex
as its identity source.&lt;/p&gt;
&lt;p&gt;This project was developed by CoreOS, and is currently being proposed for
donation to the Cloud Native Computing Foundation.&lt;/p&gt;

    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Once a user has authenticated against their identity store, they will receive a
JWT. The user then adds this token to their kubeconfig, and all subsequent
client requests will include this bearer token in the request&amp;rsquo;s Authorization
header. The Kubernetes API server uses this token to identify and authorize the
user.&lt;/p&gt;
&lt;p&gt;This flow can be complex, however, there are tools that will make this process
straightforward. &lt;a href=&#34;https://github.com/heptiolabs/gangway&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;gangway&lt;/a&gt; is such a
tool, and it will provide an end user with all of the steps necessary to
integrate with an OIDC identity provider in a self-service fashion.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/identity/gangway.png&#34; alt=&#34;Gangway&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;It should be noted that JWT tokens are not able to be revoked, but are also
time-bound. This expiration time is embedded in the token itself, and is
immutable. The identity server may scope this token for as long as it would
like, but remember that the token will be valid until it is expired. So, be sure
to choose a scope that makes sense - an hour is typically sufficient.&lt;/p&gt;
&lt;p&gt;In addition to the access JWT token, the OIDC specification also calls for an
optional refresh token as well. If your identity server supports refresh tokens,
these may be exchanged for a new access token upon expiration. This
functionality makes it possible for a user to continually get new time-scoped
access tokens seamlessly.&lt;/p&gt;
&lt;h2 id=&#34;other-means&#34;&gt;Other Means&lt;/h2&gt;
&lt;p&gt;While OIDC is the preferred integration protocol for identity and access
control, there are scenarios where, despite the availability of OIDC brokering
tools, this is still not possible. Fortunately, Kubernetes affords a number of
alternative means of identity and access control. Each of these, while capable,
may have drawbacks that make it a less favorable approach.&lt;/p&gt;


&lt;div class=&#34;aside aside-info&#34;&gt;
    &lt;div class=&#34;aside aside-title&#34;&gt;
        &lt;i class=&#34;fas fa-exclamation-circle&#34;&gt;&lt;/i&gt;
        &lt;div&gt;Why choose one?&lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#34;aside aside-content&#34;&gt;
    &lt;p&gt;Identity in Kubernetes is not mutually exclusive. It allows for multiple
identity configurations simultaneously. While you are not likely in a scenario
where it will be hard to choose the best integration, you will want to consider
this feature in order to facilitate cluster operations. If you encounter a
scenario where an external user identity system were to become unavailable,
having alternate means of authentication is a great tool to have in order to
effect change during an incident.&lt;/p&gt;

    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Let&amp;rsquo;s take a brief look at some of the other integrations.&lt;/p&gt;
&lt;h3 id=&#34;x509-certificates&#34;&gt;X.509 Certificates&lt;/h3&gt;
&lt;p&gt;Kubernetes, when properly configured, leverages TLS to secure communication
throughout the cluster. All control plane surfaces utilize the encryption
features of TLS to encrypt all over-the-wire communication, but perhaps just as
importantly, they utilize the identity aspects of TLS to ensure that only the
clients that we have authorized will be able to communicate with each other.&lt;/p&gt;
&lt;p&gt;Just as we can utilize these X.509 certificates to secure service-to-service
communication, we can use these certificates to secure and identify users as
well. In order to do so, we will need to generate an X.509 certificate signing
request that will then be signed against a certificate authority common to the
Kubernetes API server. This is a multi-step process that is outside the scope of
this documentation, but instructions may be found in the &lt;a href=&#34;https://kubernetes.io/docs/concepts/cluster-administration/certificates/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Kubernetes
documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;With X.509 certificates we may specify the username and groups that a user is a
member of by manipulating the standard fields of the cert.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;openssl req -new -key marysmith.pem -out marysmith-csr.pem -subj &amp;quot;/CN=marysmith/O=group1/O=group2&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The Common Name field is used to indicate the username of the identity, and we
add the user to user groups by way of the Organization fields. Just as with the
OIDC case, these fields may be used in RBAC RoleBinding and ClusterRoleBindings.&lt;/p&gt;
&lt;p&gt;X.509 certificates can be very difficult to work with in a practical way. First,
many users are often confused by the steps required to generate a certificate.
And this confusion often leads to reluctance when it comes to proper issuance of
credentials to new users and/or the reissuance in the case of compromise. In
fact, a common pattern that we have encountered in-the-wild has been for
certificates with broad privileges (e.g. cluster-admin) to be shared among many
users. This not only severely compromises security, but also limits your ability
to leverage features like audit logging.&lt;/p&gt;
&lt;p&gt;Secondarily, these certificates must be signed against a common certificate
authority. In the best of scenarios, users will have a certificate authority
that may be used across the organization. But, in most cases, these certificates
are dedicated to the cluster itself. This will require new certificates for each
cluster a user interacts with.&lt;/p&gt;
&lt;p&gt;Finally, and perhaps most problematic, is the fact that x.509 certificates are
note able to be revoked easily. The certificate will be valid until the
certificate expires or the server certificate has been rotated.&lt;/p&gt;
&lt;p&gt;In short, the user experience with certificates is not great. And, poor user
experiences within the realm of security often leads to users circumventing
processes meant to secure all users. So, for general-purpose needs, we do not
recommend this approach.&lt;/p&gt;
&lt;p&gt;With this said, however, X.509 certificates can be a very good option for &amp;ldquo;break
glass&amp;rdquo; access. As these certificates do not require any runtime infrastructure
they may be readily used in the event that other authentication means are
unexpectedly unavailable. Operations teams should ensure that these keys are
only used for remediation.&lt;/p&gt;
&lt;h3 id=&#34;webhooks&#34;&gt;Webhooks&lt;/h3&gt;
&lt;p&gt;Kubernetes also supports a generic mechanism for authentication by way of
webhooks. In this scenario, you may configure the API server to POST webhooks to
a service that will respond with the appropriate HTTP responses. These POSTs
will issue the Kubernetes TokenReview resource type to the authenticating
service. This type includes an attribute for the bearer token associated with
the request that is attempting to authenticate.&lt;/p&gt;
&lt;p&gt;The authenticating webhook service will simply update the TokenReview object
with an &lt;code&gt;authenticated&lt;/code&gt; boolean field, and return it to the calling Kubernetes
API service. In the case of &lt;code&gt;authenticated: true&lt;/code&gt;, the webhook will also provide
detail about the user; username, groups, etc.&lt;/p&gt;
&lt;p&gt;While this mechanism may be used in cases where there are no other viable
methods, we strongly discourage this type of integration. First, any
introduction of a downstream webhook into the regular API request flow will
introduce latencies. Secondly, this is now a (yet another) service that must be
maintained by those who are operating the Kubernetes platform, and this type of
work, while seemingly innocuous, can be non-trivial.&lt;/p&gt;
&lt;p&gt;This should only be used when no other options exist.&lt;/p&gt;
&lt;h3 id=&#34;static-tokens-and-basic-authentication&#34;&gt;Static Tokens and Basic Authentication&lt;/h3&gt;
&lt;p&gt;And, finally, Kubernetes offers some mechanisms to provide the Kubernetes API
server with pointers to files that contain either static tokens and/or basic
authentication credentials. These files must exist on a disk local to the API
server and are neither secure nor scalable. These should be avoided at all cost.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: Challenges Managing Multiple Clusters Across Multiple Clouds</title>
      
      <link>/guides/kubernetes/multi-cloud-multi-cluster-management/</link>
      <pubDate>Fri, 26 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/guides/kubernetes/multi-cloud-multi-cluster-management/</guid>
      <description>

        
        &lt;p&gt;While Kubernetes provides a rich and capable environment for modern applications, it introduces a lot of moving parts and day 2 operating issues. How do you create and enforce security policy in a highly fluid environment? How do you make sure that your identity and access control systems are configured? How do you make certain that everything stays properly configured?&lt;/p&gt;
&lt;p&gt;These challenges are hard enough to get right in a single Kubernetes cluster, but we don’t live in a world of single Kubernetes clusters. In the marketplace of ideas, multi-cluster has won. Rather than creating one giant Kubernetes cluster for everyone to share, there’s a clear need to have many small clusters. It makes sense from a security perspective, and can improve resilience and stability, as well. But that brings with it new management challenges.&lt;/p&gt;
&lt;h2 id=&#34;taming-multi-cluster-management&#34;&gt;Taming Multi-Cluster Management&lt;/h2&gt;
&lt;p&gt;While many cloud providers offer managed Kubernetes services, the control planes for these services only work for a particular provider. Several vendors are working to address the issue of multi-cloud cluster management with solutions to unify and simplify cross-cloud management in public clouds and on premises.&lt;/p&gt;
&lt;p&gt;For example, VMware &lt;a href=&#34;https://tanzu.vmware.com/content/tanzu-mission-control/introducing-vmware-tanzu-mission-control-to-bring-order-to-cluster-chaos&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;introduced Tanzu Mission Control&lt;/a&gt; in 2019 to allow operators to apply policy to individual clusters or groups of clusters and establish guardrails, freeing developers to work more freely within defined boundaries. Under the covers, VMware Tanzu Mission Control leverages &lt;a href=&#34;https://github.com/kubernetes-sigs/cluster-api&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Cluster API&lt;/a&gt; for lifecycle management.&lt;/p&gt;
&lt;h2 id=&#34;introducing-cluster-api&#34;&gt;Introducing Cluster API&lt;/h2&gt;
&lt;p&gt;The Cluster API is an open-source, cross-vendor effort to simplify cluster lifecycle management. It’s a member of a set of tools and emerging projects that help create/curate/update base virtual machine images, tackling the problem in a Kubernetes-native way, bringing control and consistency across clouds or on-premises, on virtualized or bare metal infrastructure.&lt;/p&gt;
&lt;p&gt;Extensibility is important and a diverse provider universe already exists. You’ll find provider implementations for a host of clouds, including AWS, Microsoft Azure, Baidu Cloud, Digital Ocean, Google Cloud Platform, IBM Cloud, vSphere, Packet, and more.&lt;/p&gt;
&lt;p&gt;VMware is investing heavily in the Cluster API effort, including baking a supervisor cluster into &lt;a href=&#34;https://tanzu.vmware.com/content/blog/vsphere-7-and-tanzu-kubernetes-grid-powerful-platform-for-architecting-modern-apps&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;vSphere 7&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;keep-learning&#34;&gt;Keep Learning&lt;/h2&gt;
&lt;p&gt;If you want to learn more about Cluster API or get started using it, &lt;a href=&#34;https://cluster-api.sigs.k8s.io/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;The Cluster API Book&lt;/a&gt; provides a deep dive that will help you get started.&lt;/p&gt;
&lt;p&gt;Two recent Tanzu blogs, &lt;a href=&#34;https://tanzu.vmware.com/content/blog/cluster-api-is-a-big-deal-joe-beda-craig-mcluckie-tell-you-why&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Cluster API is a Big Deal&lt;/a&gt; and &lt;a href=&#34;https://tanzu.vmware.com/content/blog/cluster-api-provider-for-azure-is-another-giant-leap-for-the-community&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Cluster API Provider for Azure&lt;/a&gt; provide more context to help you understand what’s happening in multi-cloud, multi-cluster management.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: Packaging</title>
      
      <link>/guides/kubernetes/packaging/</link>
      <pubDate>Tue, 16 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/guides/kubernetes/packaging/</guid>
      <description>

        
        &lt;p&gt;In Kubernetes, the desired state of the system is declared via resources sent to the API Server.
Resources are stored as JSON or YAML files called &lt;em&gt;manifests&lt;/em&gt;.
The management of manifests can be cumbersome but there are many tools which can help.
To inform tooling choices it is helpful to define the nature of the problems commonly encountered and identify the approaches that each tool takes to address them.&lt;/p&gt;
&lt;h2 id=&#34;challenges&#34;&gt;Challenges&lt;/h2&gt;
&lt;h3 id=&#34;value-duplication&#34;&gt;Value Duplication&lt;/h3&gt;
&lt;p&gt;Managing manifests as flat data files (YAML) violates the &lt;a href=&#34;https://en.wikipedia.org/wiki/Don%27t_repeat_yourself&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;DRY principle&lt;/a&gt;.
For example, a service port defined in one resource needs to be defined in multiple other resources.&lt;/p&gt;
&lt;h3 id=&#34;resource-duplication&#34;&gt;Resource Duplication&lt;/h3&gt;
&lt;p&gt;Resource variants must be accounted for.
The number of environments manifests are applied to may be bounded (dev, stage, prod) or unbounded (per developer).
Manifests may be defined inside of an organization (SaaS app) or by a third party (Wordpress).&lt;/p&gt;
&lt;h4 id=&#34;differences-in-application-configuration&#34;&gt;Differences in Application Configuration&lt;/h4&gt;
&lt;p&gt;A. Logging level as defined by env variable &lt;code&gt;LOG_LEVEL&lt;/code&gt; (staging: Debug / production: Info)&lt;/p&gt;
&lt;p&gt;B. Access token for accessing an external API located at &lt;code&gt;/secrets/credentials.json&lt;/code&gt; (staging/prod access separate accounts)&lt;/p&gt;
&lt;p&gt;Kubernetes accounts for these differences via resource references.
Maintaining and applying different ConfigMap and Secret manifests for each environment allows for a common Deployment definition to be used across environments.&lt;/p&gt;
&lt;h4 id=&#34;differences-in-application-operation&#34;&gt;Differences in Application Operation&lt;/h4&gt;
&lt;p&gt;Example: Deployment replica count (staging: 3 / production: 12)&lt;/p&gt;
&lt;p&gt;These types of differences cannot be accounted for by native resource references.&lt;/p&gt;
&lt;h2 id=&#34;techniques&#34;&gt;Techniques&lt;/h2&gt;
&lt;h3 id=&#34;1-configuration-language&#34;&gt;1. Configuration Language&lt;/h3&gt;
&lt;h4 id=&#34;1a-variable-substitution&#34;&gt;1.A. Variable Substitution&lt;/h4&gt;
&lt;p&gt;Value duplication can be solved by elevating the level of abstraction at which resources are specified: using a language in place of data files.
A configuration language can be as simple as an interpolator or it can expose features such as expressions (&lt;code&gt;appName + &amp;quot;:&amp;quot; + appPort&lt;/code&gt;) and conditionals (&lt;code&gt;if appPort != &amp;quot;80&amp;quot;&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Some languages suffer from readability issues through introducing interpolation symbols (for example: &lt;code&gt;foo: {{ .myBarVar }}&lt;/code&gt; vs &lt;code&gt;foo: bar&lt;/code&gt;).
Language features also bring complexity and complexity brings bugs.
Both of these issues are more prominent in languages that lack support for the underlying data structures and formats.
Generic text templating languages fall into this category.
On the opposite end of the spectrum, languages that output structured data can help avoid bugs through data validation.&lt;/p&gt;
&lt;h4 id=&#34;1b-parameterization&#34;&gt;1.B. Parameterization&lt;/h4&gt;
&lt;p&gt;Most configuration languages allow for grouping manifests into modules.
Resource variants are accounted for by exposing module input parameters.
Any value that needs to vary across environments is exposed outside of the module.&lt;/p&gt;
&lt;p&gt;The pitfall of this approach is that encapsulation tends to break down over time.
As the use-cases increase, the number of exposed parameter trends towards the total of the configurable options in the underlying resources.&lt;/p&gt;
&lt;p&gt;You end up with an API contract that is defined by your module.
This is different from (and not as well documented) as the API of the underlying resources that live in your module, yet similar in surface-area after enough time has passed.&lt;/p&gt;
&lt;h3 id=&#34;2-patching&#34;&gt;2. Patching&lt;/h3&gt;
&lt;p&gt;Some tools allow for patches (partial resource definitions) to be applied to resources before they are sent to the API Server.
Patches are defined as YAML manifests that contain enough information to identify the resource they are intended to patch (for instance: &lt;code&gt;kind&lt;/code&gt;, &lt;code&gt;name&lt;/code&gt;, &lt;code&gt;namespace&lt;/code&gt;) as well as target fields they are going to update/insert (for example: &lt;code&gt;replicas&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;While useful, patches introduce a layer of complexity.
Looking at a single file no longer represents the full resource that will be applied.
In addition, ordering must be accounted for: applying patch A before B can result in a different outcome as applying B before A.&lt;/p&gt;
&lt;h3 id=&#34;3-branching&#34;&gt;3. Branching&lt;/h3&gt;
&lt;p&gt;It is best practice to store manifests in version control.
Tools like git can be used to maintain branches for each environment.
This allows for native VCS tools to be used for viewing diffs across environments.
Pull Requests targeting separate branches, as opposed to separate directories can be utilized as a means of promoting changes across environments.&lt;/p&gt;
&lt;h3 id=&#34;4-convention&#34;&gt;4. Convention&lt;/h3&gt;
&lt;p&gt;Favoring convention over configuration can help mitigate repeated values.
Establishing conventions for standard ports (for example: 80 or 443 for REST APIs) can reduce required configuration as most libraries default to these values.
This can be done by taking advantage of the IP-per-pod networking model, where every pod gets a unique IP so port conflict concerns are a non-issue.
In addition, utilizing namespaces in lieu of prefixing/suffixing names (&lt;code&gt;namespace: staging, name: my-app&lt;/code&gt; as opposed to &lt;code&gt;name: my-app-staging&lt;/code&gt;) can help with keeping resource references constant.&lt;/p&gt;
&lt;h2 id=&#34;popular-tooling--approaches&#34;&gt;Popular Tooling &amp;amp; Approaches&lt;/h2&gt;
&lt;h3 id=&#34;helm&#34;&gt;Helm&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://helm.sh/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Helm&lt;/a&gt; is the dominant player in the world of Kubernetes
templating. Helm manifests are organized into modules called &lt;em&gt;charts&lt;/em&gt;. Charts
expose parameters that can be used to customize deployments. Customization of
the underlying resources is accomplished by using Golang text templates as a
DSL. Helm also incorporates the concept of a package registry to facilitate easy
consumption of third-party charts. It&amp;rsquo;s recommended to avoid the use of Helm for
managing internal projects (a business specific API, a website deployment, etc).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Large ecosystem of charts&lt;/li&gt;
&lt;li&gt;Third-party software is usually available via Helm charts
(a message queue, a database, etc)&lt;/li&gt;
&lt;li&gt;Deploying applications and their associated objects is done in one command&lt;/li&gt;
&lt;li&gt;Parameterizing values makes it easier to adjust configurations per environment
(a values file for dev, test, prod)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Charts trend towards becoming a 1:1 mapping of parameters into Kubernetes
resource definition fields&lt;/li&gt;
&lt;li&gt;Instead of configuring a well-documented Kubernetes API resource, users end up
configuring a less well known and under-documented helm chart API (as in the
exposed chart input parameters).&lt;/li&gt;
&lt;li&gt;The Go text templating language is not aware that it is outputting schemas
defined by OpenAPI or even that it is being used to format data at all. Helm
is not able to validate that &lt;code&gt;replicas: &amp;quot;three&amp;quot;&lt;/code&gt; is an invalid Deployment
field.&lt;/li&gt;
&lt;li&gt;YAML&amp;rsquo;s sensitivity to indention along with conditionally defined template
blocks often results in unforeseen issues when input parameters are changed.
Even when indentation is accounted for correctly, the result is often hard to
read.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;kustomize&#34;&gt;Kustomize&lt;/h3&gt;
&lt;p&gt;As of &lt;a href=&#34;https://kubernetes.io/blog/2019/03/25/kubernetes-1-14-release-announcement/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Kubernetes
1.14&lt;/a&gt;,
the &lt;a href=&#34;https://kustomize.io/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Kustomize&lt;/a&gt; tool is a part of the native toolchain
via &lt;code&gt;kubectl apply -k&lt;/code&gt;. Kustomize does not use templates. Instead, it relies on
patching. This attribute allows Kustomize to modify vanilla manifests. Kustomize
also provides &lt;em&gt;Kubernetes aware&lt;/em&gt; functionality such as applying a prefix to all
managed resource names. Behavior is controlled by a &lt;code&gt;kustomization.yaml&lt;/code&gt; file.&lt;/p&gt;
&lt;p&gt;The main negative to using Kustomize is the inability to encapsulate Kubernetes
resource implementation aspects. Because patching operates at the resource
level, updating implementation details (Deployment vs StatefulSet) almost always
results in breaking changes to downstream users. The gravity of this downside
varies greatly based on the use-case. For a team that manages 3 separate
environments, encapsulation is less of a concern. On the other hand an
open-source MySQL implementation might have thousands of consumers, each with
relatively little knowledge of internal implementation details.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Part of the native &lt;code&gt;kubectl&lt;/code&gt; toolchain&lt;/li&gt;
&lt;li&gt;Able to modify vanilla manifests.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Main negative to using Kustomize is the inability to encapsulate Kubernetes
resource implementation aspects&lt;/li&gt;
&lt;li&gt;Because patching operates at the resource level, updating implementation
details (Deployment vs StatefulSet) almost always results in breaking changes
to downstream users.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;operators&#34;&gt;Operators&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/operator/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Operators&lt;/a&gt; are
composed of CustomResourceDefinitions (CRDs) and a control loop that is running
on a cluster. Open source projects such as &lt;a href=&#34;https://github.com/coreos/prometheus-operator&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Prometheus now have
operators&lt;/a&gt;. Operators use CRDs to
create a domain-specific API for a specialized application. Similar in concept
to templating tools, this approach uses a programming language (the controller,
usually written in Go) to translate parameters (the CRD) into a set of manifests
(the created resources).&lt;/p&gt;
&lt;p&gt;As the name suggests, operators are responsible for &lt;em&gt;operating&lt;/em&gt; applications.
This means a sufficiently sophisticated operator probably has the ability to
perform application-specific steps required to upgrade to a new app version.
This is done in response to the user (human operator) making a declarative
version update. Because they have an active reconciliation loop running on the
cluster, operators also naturally address config-drift.&lt;/p&gt;
&lt;p&gt;Operators can be seen as tools that move edge-case imperative operations towards
the ideal declarative approach that Kubernetes promises. For internally
developed applications, operators should be considered when there are advanced
lifecycle concerns (for example: stateful or legacy apps).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Naturally address configuration drift&lt;/li&gt;
&lt;li&gt;When running complex third party applications, sufficiently mature operators
should be favored over basic templating tools.&lt;/li&gt;
&lt;li&gt;Mature operators can act like cloud services, making it more simple to install
and update Kubernetes applications&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kubernetes API pollution, creating additional cognitive load on cluster users&lt;/li&gt;
&lt;li&gt;Rogue operators can be hard to debug&lt;/li&gt;
&lt;li&gt;Building an operator involves a large amount of effort&lt;/li&gt;
&lt;li&gt;Most cloud native workloads do not warrant the development of an operator&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: Platform Observability</title>
      
      <link>/guides/kubernetes/observability/</link>
      <pubDate>Wed, 24 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/guides/kubernetes/observability/</guid>
      <description>

        
        &lt;p&gt;Observability is crucial for successfully operating a complex software system
such as Kubernetes. With this in mind, Kubernetes offers multiple facilities
that enable operators to observe the system at runtime. With that said, the onus
is on the platform operator to consume, evaluate, and act on the information
exposed by Kubernetes.&lt;/p&gt;
&lt;p&gt;Kubernetes has multiple sources of information that operators can use to observe
the platform&amp;rsquo;s behavior. These include component logs, audit logs, events, and
metrics.&lt;/p&gt;
&lt;h2 id=&#34;component-logs&#34;&gt;Component Logs&lt;/h2&gt;
&lt;p&gt;Kubernetes is a distributed system composed of multiple processes that run
across multiple hosts. Each component writes logs to stdout and stderr to
provide visibility into what is happening in the process. These logs are one of
the most important troubleshooting tools available to platform operators when
there is an issue with the system.&lt;/p&gt;
&lt;h2 id=&#34;audit-logs&#34;&gt;Audit Logs&lt;/h2&gt;
&lt;p&gt;The Kubernetes API server records every interaction with an audit log. The audit
logging capability is configurable using a policy file that controls which
events should be recorded and what data they should include. Typically, each log
entry contains what happened, when it happened, and who was involved in the
action. The API server supports multiple audit backends for persisting the audit
log: a file on disk, a statically-configured webhook, and a
dynamically-configured webhook.&lt;/p&gt;
&lt;h2 id=&#34;events&#34;&gt;Events&lt;/h2&gt;
&lt;p&gt;In addition to logs, Kubernetes components emit events that track what is
happening in the system. Examples of events include the Kubelet pulling a
container image, or the Scheduler assigning a pod to a node. Events are
first-class resources in the Kubernetes API, and thus are stored in the
Kubernetes API server. Because they are stored in the API, there is a retention
policy that controls the length of time the events are stored. To persist events
for historical analysis, platform operators should implement a solution that
ships the events to an external system.&lt;/p&gt;
&lt;h2 id=&#34;metrics&#34;&gt;Metrics&lt;/h2&gt;
&lt;p&gt;Each Kubernetes component exposes a set of metrics that track the component&amp;rsquo;s
state. These metrics are available through an HTTP endpoint at &lt;code&gt;/metrics&lt;/code&gt; and
are exposed using the Prometheus data format. The API server, for example,
exposes metrics such as &lt;code&gt;apiserver_current_inflight_requests&lt;/code&gt; and
&lt;code&gt;apiserver_watch_events_total&lt;/code&gt;. To take advantage of these metrics, platform
operators should use a monitoring system that can scrape Prometheus metrics and
generate alerts on those metrics.&lt;/p&gt;
&lt;p&gt;Kubernetes also exposes node and pod-level resource consumption metrics through
the Metrics API. These metrics are accessible directly via the API, or more
easily, through the &lt;code&gt;kubectl top&lt;/code&gt; command. To enable this API, operators must
deploy the &lt;a href=&#34;https://kubernetes.io/docs/tasks/debug-application-cluster/resource-metrics-pipeline/#metrics-server&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Kubernetes Metrics
server&lt;/a&gt;,
which is not deployed by default.&lt;/p&gt;
&lt;h2 id=&#34;distributed-tracing&#34;&gt;Distributed Tracing&lt;/h2&gt;
&lt;p&gt;Distributed Tracing is another important pillar of observability. As a platform
operator, consider deploying a tracing system to offer it as a platform service.
This will enable developers to perform distributed tracing in their
applications.&lt;/p&gt;
&lt;h2 id=&#34;popular-tooling-and-approaches&#34;&gt;Popular Tooling and Approaches&lt;/h2&gt;
&lt;h3 id=&#34;logging&#34;&gt;Logging&lt;/h3&gt;
&lt;h4 id=&#34;elasticsearch&#34;&gt;Elasticsearch&lt;/h4&gt;
&lt;p&gt;Open source search and analytics database for all types of data that is commonly
utilized in the Kubernetes ecosystem for log storage.&lt;/p&gt;
&lt;p&gt;It is the storage and search engine component of the ELK/EFK stack.&lt;/p&gt;
&lt;p&gt;Its distributed, fast and scalable data ingestion nature make it a natural fit
for container log aggregation, search and analytics.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Built for scale&lt;/li&gt;
&lt;li&gt;Automated failover handling&lt;/li&gt;
&lt;li&gt;Distributed by design&lt;/li&gt;
&lt;li&gt;Widely tested&lt;/li&gt;
&lt;li&gt;Commonly used and known as part of the ELK/EFK stack&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Geographic distribution of nodes not recommended&lt;/li&gt;
&lt;li&gt;Relatively large memory footprint&lt;/li&gt;
&lt;li&gt;The difficulty configuring and tuning it is considerable&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It&amp;rsquo;s worth considering buying and paying support for an alternative hosted
platform such as vRealize Log Insight.&lt;/p&gt;
&lt;h4 id=&#34;logstash&#34;&gt;Logstash&lt;/h4&gt;
&lt;p&gt;The data processing pipeline that aggregates and ships logs to the storage
engine utilized in the cluster. Commonly combined with Elasticsearch as
component of the ELK stack. It can ingest and process multiple sources
simultaneously, making it a good candidate to funnel container logs from a host,
process and forward them to the storage engine.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Commonly used&lt;/li&gt;
&lt;li&gt;Flexible&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Performance and resource consumption can be problematic&lt;/li&gt;
&lt;li&gt;Written in JRuby (java runtime required)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;fluentd&#34;&gt;Fluentd&lt;/h4&gt;
&lt;p&gt;Fluentd is another data collector/processing engine that is commonly combined
with Elasticsearch as a container logging platform. It can parse, analyze and
transform logs before sending them to the storage engine. Written in Ruby and C
for the speed-sensitive components, it has a lot of plugins provided by the Ruby
community.&lt;/p&gt;
&lt;p&gt;Fluentd runs containerized in the cluster as a DaemonSet. Typically all nodes in
the platform will run a Fluentd Pod which is configured to read the standard
output of all containers running in the node and funnel them for storage in a
database. The database can live inside or outside of the cluster.&lt;/p&gt;
&lt;p&gt;Typically, containerized log storage leverages Elasticsearch. Fluentd can ship
logs to external entities such as Splunk as well.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Part of CNCF&lt;/li&gt;
&lt;li&gt;Wide ecosystem of plugins out of the box provided
by the Ruby community&lt;/li&gt;
&lt;li&gt;Excellent support for Elastic&lt;/li&gt;
&lt;li&gt;Written in CRuby (No java runtime required)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Partly written in Ruby make it slower than other options&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;fluent-bit&#34;&gt;Fluent Bit&lt;/h4&gt;
&lt;p&gt;Fluent Bit is Fluentd&amp;rsquo;s smaller counterpart. It&amp;rsquo;s part of the Fluentd ecosystem.
It&amp;rsquo;s more suitable for containerized workloads given its smaller footprint. It&amp;rsquo;s
fully written in C, but significantly fewer plugins are available for it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Created with a highly distributed use case in mind&lt;/li&gt;
&lt;li&gt;Super light memory footprint&lt;/li&gt;
&lt;li&gt;Extensible&lt;/li&gt;
&lt;li&gt;Fully written in C, zero dependencies&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Smaller number of plugins available compared to Fluentd&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Fluent Bit should be your first choice for Kubernetes, unless you find a
specific need or missing plugin to consider Fluentd.&lt;/p&gt;
&lt;h4 id=&#34;loki&#34;&gt;Loki&lt;/h4&gt;
&lt;p&gt;Loki is a horizontally-scalable, highly-available, multi-tenant log aggregation
system inspired by Prometheus. It is designed to be very cost effective and easy
to operate. It does not index the contents of the logs, but rather a set of
labels for each log stream.&lt;/p&gt;
&lt;p&gt;Loki supports a number of database options for the indexes and can store logs on
disk or on any of the major object storage options such as S3, Minio, Google
Cloud Object Storage.&lt;/p&gt;
&lt;p&gt;It is usually paired with &lt;a href=&#34;#grafana&#34;&gt;Grafana&lt;/a&gt; for visualization and
&lt;a href=&#34;#promtail&#34;&gt;Promtail&lt;/a&gt; for log ingestion.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Considerably easier to use and operate than Elasticsearch&lt;/li&gt;
&lt;li&gt;Offloads indexing and log storage to reliable external services&lt;/li&gt;
&lt;li&gt;Allows you to use Grafana for both logs and metrics&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Less sophisticated search capabilities than Elasticsearch&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;promtail&#34;&gt;Promtail&lt;/h4&gt;
&lt;p&gt;Promtail is the log shipping component built specifically for shipping logs to
Loki. It would not be used otherwise.&lt;/p&gt;
&lt;h4 id=&#34;vrealize-log-insight&#34;&gt;vRealize Log Insight&lt;/h4&gt;
&lt;p&gt;vRealize Log Insight (vRLI) is VMware&amp;rsquo;s product offering that delivers
heterogeneous and highly scalable log management with intuitive, actionable
dashboards, sophisticated analytics and broad third-party extensibility.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A Fluentd plugin exists for vRLI&lt;/li&gt;
&lt;li&gt;Scalability. Supports up to 15,000 events / second by scaling out appliances&lt;/li&gt;
&lt;li&gt;Automatic visualizations based on existing data&lt;/li&gt;
&lt;li&gt;Integration with vRealize Operations Manager&lt;/li&gt;
&lt;li&gt;Accepts rsyslog, syslog-ng, log4j agents or through API ingestion&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Requires a licenses to be purchased
&lt;ul&gt;
&lt;li&gt;The version that&amp;rsquo;s bundled with NSX-T can only be used for NSX-T or vSphere
logs.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Must be deployed in a virtual environment through an appliance.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;visualization&#34;&gt;Visualization&lt;/h3&gt;
&lt;h4 id=&#34;kibana&#34;&gt;Kibana&lt;/h4&gt;
&lt;p&gt;Open source data discovery and visualization dashboard for accessing information
stored in Elasticsearch. It&amp;rsquo;s the &amp;lsquo;K&amp;rsquo; in the ELK/EFK stacks. It and provides
insight into the container logs aggregated from the cluster. With the stored
data in Elasticsearch it is possible to create colorful dashboards, charts and
reports, gaining immediate valuable insight into the state of the cluster and
the applications running in it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Integrates seamlessly with Elasticsearch&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;grafana&#34;&gt;Grafana&lt;/h4&gt;
&lt;p&gt;Grafana is an interactive visualization tool popularly used as a frontend for
metrics. It has native integration with Prometheus and Elasticsearch, making it
an easy choice to make for visualizing performance data of a running Kubernetes
cluster.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s easy to install and expandable, making it appealing for visualizing
application performance data.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s typically installed as an application in the cluster it will operate on.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Integrates seamlessly with Prometheus&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;monitoring&#34;&gt;Monitoring&lt;/h3&gt;
&lt;h4 id=&#34;prometheus&#34;&gt;Prometheus&lt;/h4&gt;
&lt;p&gt;Prometheus is a CNCF project widely used for Kubernetes platform monitoring as
well as metrics collection and aggregation. Prometheus works by scraping data
from configured endpoints, parsing it and storing it in its internal time-series
database. This data can then be easily queried directly with PromQL, or
displayed using a visualization tool such as Grafana.&lt;/p&gt;
&lt;p&gt;Prometheus has push-gateway facility as well, for instrumenting applications
with the available client libraries to push metrics when exposing an endpoint to
scrape is not suitable. Ephemeral jobs such as pipelines are a good example of
tasks in which pushing data to the metrics server make sense.&lt;/p&gt;
&lt;p&gt;A &lt;a href=&#34;https://github.com/coreos/prometheus-operator&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Prometheus Operator&lt;/a&gt; exists to
install and manage the operation of your Prometheus cluster. We highly recommend
taking advantage of it when installing Prometheus.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Part of CNCF&lt;/li&gt;
&lt;li&gt;Easy installation&lt;/li&gt;
&lt;li&gt;Well documented&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Long term storage is limited&lt;/li&gt;
&lt;li&gt;Often requires addon solutions&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;datadog&#34;&gt;Datadog&lt;/h4&gt;
&lt;p&gt;System and application monitoring tool that runs as a DaemonSet in the cluster.
Allows you to get metrics and logs from services in real time to visualize
kubernetes states and get notified of failovers and events.&lt;/p&gt;
&lt;p&gt;Additionally, it can be configured outside of the cluster and have it gather
Kubernetes metrics.&lt;/p&gt;
&lt;p&gt;Alerting is also possible based on collected metrics from the platform or
the applications running in it.&lt;/p&gt;
&lt;p&gt;Similarly to ELK, it provides log aggregation, live tail facility, log archiving
and custom visualizations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Platform administrators have less to worry about&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Agent is not open source&lt;/li&gt;
&lt;li&gt;Agent configuration not properly documented&lt;/li&gt;
&lt;li&gt;Cost $$$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If customers aren&amp;rsquo;t price sensitive, this is a good option and offloads from the
platform administrator duties.&lt;/p&gt;
&lt;h4 id=&#34;tanzu-observability-by-wavefront&#34;&gt;Tanzu Observability by Wavefront&lt;/h4&gt;
&lt;p&gt;Tanzu Observability by Wavefront is a Software as a Service solution from VMware
that collects and aggregates Kubernetes and Application metrics.&lt;/p&gt;
&lt;p&gt;The Tanzu Observability by Wavefront service deploys daemon sets on Kubernetes
cluster nodes to collect metrics on clusters, nodes, pods, containers, and
control plane objects. These metrics are then pushed to the SaaS application
through a proxy.&lt;/p&gt;
&lt;p&gt;Wavefront integrates with many different platforms and applications out of the
box and builds initial dashboards to quickly visualize the health of workloads
and clusters.&lt;/p&gt;
&lt;p&gt;Wavefront integrates with many different platforms including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kubernetes&lt;/li&gt;
&lt;li&gt;Pivotal Cloud Foundry&lt;/li&gt;
&lt;li&gt;VMware vSphere&lt;/li&gt;
&lt;li&gt;Amazon Web Services&lt;/li&gt;
&lt;li&gt;Microsoft Azure&lt;/li&gt;
&lt;li&gt;Google Cloud Platform&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;VMware Tanzu Kubernetes Grid Integrated has an integration built in to enable
the Wavefront functionality.&lt;/p&gt;
&lt;p&gt;Wavefront also integrates with application services including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Apache HTTP&lt;/li&gt;
&lt;li&gt;Envoy&lt;/li&gt;
&lt;li&gt;Istio&lt;/li&gt;
&lt;li&gt;Elasticsearch&lt;/li&gt;
&lt;li&gt;NGINX&lt;/li&gt;
&lt;li&gt;WebSphere&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Wavefront can also ingest data from other monitoring tools such as: Fluentd,
Logstash, Splunk, vROps, Jaeger, and Prometheus.&lt;/p&gt;
&lt;p&gt;Alert notifications can be sent to a variety of solutions including: Slack,
PagerDuty, ServiceNow, or a Webhook for custom alerts.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Platform administrators have less to worry about&lt;/li&gt;
&lt;li&gt;Wavefront can scale to manage large numbers of clusters simultaneously&lt;/li&gt;
&lt;li&gt;Built in Integrations for platforms, applications, and alerts&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data is stored outside the customer&amp;rsquo;s data center&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: Platform Security</title>
      
      <link>/guides/kubernetes/platform-security/</link>
      <pubDate>Wed, 24 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/guides/kubernetes/platform-security/</guid>
      <description>

        
        &lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Best practices in Kubernetes security are rapidly evolving. Many security
problems in early versions of Kubernetes are resolved by default in recent
versions. However, like any complex system, there are still risks you should
understand before you trust it with your production data. We’ve tried to
summarize the most important things you should have in mind when you host
sensitive workloads on Kubernetes.&lt;/p&gt;
&lt;p&gt;The topics discussed here help you understand potential risks in your cluster.
The risk in your environment depends on your threat model and the types of
applications you run in your cluster. You’ll have to consider how best to invest
in security controls and hardening based on the sensitivity of your data, the
amount of time and staff you’re able to dedicate to security, and your company’s
particular compliance requirements.&lt;/p&gt;
&lt;p&gt;Kubernetes provides several mechanisms to enforce security within the cluster.
These range from API security controls, down to container isolation, resource
limiting, and network policy control.&lt;/p&gt;
&lt;h2 id=&#34;general-points&#34;&gt;General points&lt;/h2&gt;
&lt;p&gt;Kubernetes core components cooperate to schedule and run your workloads in a
cluster. Kubernetes provides a range of access control mechanisms, however their
default values tend to be overly permissive. You should carefully determine what
access your system components and users need, and configure the most restrictive
controls possible.&lt;/p&gt;
&lt;p&gt;Remember also to secure the infrastructure that your clusters run on - for
example, SSH access, or cloud provider access such as AWS IAM.&lt;/p&gt;
&lt;h2 id=&#34;tls-certificates&#34;&gt;TLS Certificates&lt;/h2&gt;
&lt;p&gt;Kubernetes clusters require
&lt;a href=&#34;https://en.wikipedia.org/wiki/Public_key_infrastructure&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;PKI&lt;/a&gt; certificates for
secure communication between cluster components. Default CAs and certificates
are provided by kubeadm, but you should consider your requirements before
accepting only the defaults.&lt;/p&gt;
&lt;p&gt;Kubernetes requires TLS for the communication between the control plane
components of your cluster. For details about the required PKI certificates, see
the
&lt;a href=&#34;https://kubernetes.io/docs/setup/certificates/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;certificates documentation&lt;/a&gt;.
You can reuse the control plane CA certificate bundle for TLS in your
application/workloads. See the documentation about &lt;a href=&#34;https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;managing TLS
certificates&lt;/a&gt;.
kubeadm automatically generates the certificates required by the cluster; this
topic explains when and why you might want to generate your own certificates. It
also discusses options for managing certificates for your applications.&lt;/p&gt;
&lt;h3 id=&#34;certificate-authorities&#34;&gt;Certificate Authorities&lt;/h3&gt;
&lt;p&gt;The certificates generated with a kubeadm install rely on a single cluster CA
for all certificates. You might want to manage your certificates differently in
the following cases:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For finer-grained control over authentication, you might set up different
certificate authorities for server certificates and client certificates.&lt;/li&gt;
&lt;li&gt;Your company’s policies might require TLS certificates that are issued by your
own PKI.&lt;/li&gt;
&lt;li&gt;If you integrate with an OpenID Connect (OIDC) provider, you can use the OIDC CA.&lt;/li&gt;
&lt;li&gt;Publicly facing workloads may require a Commercial or
&lt;a href=&#34;https://en.wikipedia.org/wiki/Let%27s_Encrypt&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;non-profit&lt;/a&gt; certificate
bundle.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;if-you-issue-your-own-certificates&#34;&gt;If you issue your own certificates&lt;/h3&gt;
&lt;p&gt;The certificate for the API server control plane component requires a
subjectAltName (&lt;a href=&#34;https://en.wikipedia.org/wiki/Subject_Alternative_Name&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;SAN&lt;/a&gt;):
kubernetes. We recommend you use a corporate CA on a load balancer in front of
the API server instead of replacing the CA. Note that the &lt;code&gt;--root-ca-file&lt;/code&gt; flag
for the controller manager must also include a copy of the CA for the
API-server.&lt;/p&gt;
&lt;h3 id=&#34;certificate-rotation&#34;&gt;Certificate rotation&lt;/h3&gt;
&lt;p&gt;By default, kubeadm generates certificate authorities that expire after 10
years. The server and client certificates expire after one year. It is strongly
recommended to not let certificates expire as your cluster will become
inoperable.&lt;/p&gt;
&lt;p&gt;To help make sure your certificates do not expire, you can use the &lt;a href=&#34;https://github.com/prometheus/blackbox_exporter&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Prometheus
BlackBox exporter&lt;/a&gt;, which
allows probing and alerting on certificate expiry dates.&lt;/p&gt;
&lt;p&gt;You can also configure automatic certificate rotation. See the documentation for
&lt;a href=&#34;https://kubernetes.io/docs/tasks/tls/certificate-rotation/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;kubelet certificate
rotation&lt;/a&gt; and
&lt;a href=&#34;https://kubernetes.io/docs/reference/setup-tools/kubeadm/implementation-details/#setup-nodes-certificate-rotation-with-auto-approval&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;node certificate rotation&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;authentication-and-authorizationhttpskubernetesiodocsreferenceaccess-authn-authzcontrolling-access&#34;&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/controlling-access/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Authentication and Authorization&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id=&#34;authentication&#34;&gt;Authentication&lt;/h3&gt;
&lt;p&gt;Kubernetes uses client certificates, bearer tokens, an authenticating proxy, or
HTTP basic auth to authenticate API requests through authentication plugins. As
HTTP requests are made to the API server, plugins attempt to associate the
following attributes with the request: Username, UID, Groups, and Extra fields.&lt;/p&gt;
&lt;p&gt;A critical component of cluster security is making sure that human users,
Kubernetes services accounts, cluster components, and application components
have the right permissions to access only the resources they need to get their
respective jobs done. Authentication and authorization are critical parts of
access control.&lt;/p&gt;
&lt;h3 id=&#34;integrate-an-identity-provider&#34;&gt;Integrate an identity provider&lt;/h3&gt;
&lt;p&gt;Certificates take care of authentication for clients, servers, clusters and
applications. To authenticate human users, we recommend integrating an existing
corporate identity system. Kubernetes lets you provide authentication with any
compliant OpenID Connect provider (for example, GitHub or Google). Kubernetes
authentication and authorization can also be extended with webhook-based plugins
to create a custom identity integration.&lt;/p&gt;
&lt;p&gt;If your organization integrates multiple identity providers,
&lt;a href=&#34;https://github.com/dexidp/dex&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Dex&lt;/a&gt; can be integrated with Gangway to act as
the OIDC endpoint. Dex acts as a broker for identity, providing a standard OIDC
frontend for a variety of backends such as LDAP servers, SAML providers, or
established identity providers like GitHub, Google, and Active Directory.&lt;br&gt;
Multi-factor authentication is not required, but provides additional protection
for end user flows.&lt;/p&gt;
&lt;h3 id=&#34;authorization&#34;&gt;Authorization&lt;/h3&gt;
&lt;p&gt;Human users and service accounts need to be carefully authorized to access only
the resources they need to get their jobs done, and no more. The principle of
least privilege is central to good authorization policies.&lt;/p&gt;
&lt;p&gt;Kubernetes expects attributes that are common to REST API requests. This means
that Kubernetes authorization works with existing organization-wide or
cloud-provider-wide access control systems, which may handle other APIs besides
the Kubernetes API. Every authenticated request to the Kubernetes API server
made by a human user or a service account needs to be authorized.&lt;/p&gt;
&lt;h3 id=&#34;rbac-authorizationhttpskubernetesiodocsreferenceaccess-authn-authzrbac&#34;&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/rbac/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;RBAC Authorization&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Role-Based Access Control (RBAC) allows the control of actions performed on
resources in the cluster, and defines who is allowed to perform them. Every
resource in Kubernetes is represented as an API object (Pods, Namespaces,
Secrets, ConfigMaps, etc.) These resources can be created, read, updated, and
deleted (verbs). A rule is composed of a verb and a resource, as an operation to
be performed on an API group. These rules are bundled together in Roles. Roles
are scoped to a namespace. Cluster-wide roles are defined in ClusterRole
objects. Roles can then be bound to users, groups and service accounts by
creating a role binding thereby granting them the ability to perform actions
described in the roles.&lt;/p&gt;
&lt;p&gt;At a minimum, we recommend that you enable RBAC. RBAC is enabled by default in
most recent installers and provides a framework for implementing the principle
of least privilege for humans and applications that access the Kubernetes API.&lt;/p&gt;
&lt;p&gt;To get the most benefit from RBAC, an appropriate configuration is required:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Run each component with the most restrictive permissions that still allow for
expected functionality. Most applications in a cluster will need little or no
access to the Kubernetes API. System components such as an ingress controller
or monitoring system may need more access, but can often be limited to
read-only access or access within a particular namespace.&lt;/li&gt;
&lt;li&gt;Make sure that trusted components don’t act as pivots that allow less
privileged users to escalate privileges. The Kubernetes Dashboard and Helm
tiller daemon are examples that deserve special attention. Isolate these
components with application-level authentication/ authorization and network
access controls to prevent unauthorized access.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When creating RBAC policies, prefer Roles and RoleBindings over ClusterRoles and
ClusterRoleBindings whenever possible as they are scoped to namespaces by
default. While Kubernetes comes with default RBAC policies in place, we
recommend setting up your baseline policies with the least required privileges
that you need.&lt;/p&gt;
&lt;p&gt;The Kubernetes API audit logs are a useful tool for discovering which APIs a
particular application is using, and for testing locked-down RBAC policies. The
&lt;a href=&#34;https://github.com/liggitt/audit2rbac&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;audit2rbac&lt;/a&gt; tool can act as a reference
as it can generate RBAC roles and bindings to cover the API requests made by a
user. See also
&lt;a href=&#34;https://kubernetes.io/docs/tasks/debug-application-cluster/audit/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Auditing&lt;/a&gt; in
the Kubernetes documentation.&lt;/p&gt;
&lt;h3 id=&#34;admission-controllershttpskubernetesioblog20190321a-guide-to-kubernetes-admission-controllers&#34;&gt;&lt;a href=&#34;https://kubernetes.io/blog/2019/03/21/a-guide-to-kubernetes-admission-controllers/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Admission Controllers&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;An admission controller is a piece of code that intercepts requests to the
Kubernetes API. This happens before the persistence of the object, but after the
request is authenticated and authorized. These are plugins that govern and
enforce the acceptance of requests. There are two individual Admission
Controllers: MutatingAdmissionWebhook and ValidatingAdmissionWebhook, which
execute mutating and validating actions, respectively. Mutating controllers may
modify the objects they admit; validating controllers do not. The admission
control process has two phases: the mutating phase is executed first, followed
by the validating phase. An excellent example of a mutating admission controller
is Istio&amp;rsquo;s
&lt;a href=&#34;https://istio.io/docs/setup/additional-setup/sidecar-injection/#automatic-sidecar-injection&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;automatic sidecar injection&lt;/a&gt;
mechanism. If any of the controllers in either phase reject the request, the
entire request is rejected immediately and an error is returned to the end-user&lt;/p&gt;
&lt;h2 id=&#34;network-and-application-access-control&#34;&gt;Network and Application Access Control&lt;/h2&gt;
&lt;p&gt;Access to the cluster network should be carefully controlled and permissions
granted only to the components or resources that need access. The Kubernetes
NetworkPolicy API allows users to express ingress and egress policies (starting
with Kubernetes 1.8.0) to Kubernetes pods based on labels and ports.&lt;/p&gt;
&lt;p&gt;Many existing applications assume that network-level access implies a level of
authorization. Even if applications include strong application-layer
authentication and authorization, network-level access control provides an
additional layer of defense. For example, it provides crucial protection against
pre-auth vulnerabilities such as the &lt;a href=&#34;https://en.wikipedia.org/wiki/Heartbleed&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Heartbleed
(CVE-2014-0160)&lt;/a&gt; vulnerability in
OpenSSL.&lt;/p&gt;
&lt;h3 id=&#34;network-policyhttpskubernetesiodocsconceptsservices-networkingnetwork-policies&#34;&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/network-policies/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Network Policy&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;By default, Kubernetes clusters do not restrict traffic. Pods can communicate
with any other pods. External clients can also communicate with pods, assuming
they are routable from the client&amp;rsquo;s network.&lt;/p&gt;
&lt;p&gt;The NetworkPolicy resource in Kubernetes allows you to control how pods are
allowed to communicate with each other and other network endpoints. The
NetworkPolicy resource is namespace scoped. Rules defined in the policy allow
traffic and are combined additively.&lt;/p&gt;
&lt;p&gt;Kubernetes provides core data types for specifying network access controls
between pods. Network policy in Kubernetes can limit inbound traffic to a pod
based on the source pod’s namespace and labels, plus the IP address for traffic
that originates outside the cluster. Network policy can also limit outbound
traffic using the same set of selectors. A good starting point is to restrict
ingress to only the application namespace by default. For details, see the
&lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-deny-all-ingress-traffic&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Kubernetes Network Policy
documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The enforcement of network policy relies on the cluster’s
&lt;a href=&#34;https://github.com/containernetworking/cni&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;CNI&lt;/a&gt; provider. Without them,
Kubernetes “fails open” — the API happily accepts any network policy, but the
policies are not enforced. We recommend
&lt;a href=&#34;https://www.projectcalico.org/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Calico&lt;/a&gt; as your CNI provider, because it
enforces controls. Examples of Network policies can be found
&lt;a href=&#34;https://github.com/ahmetb/kubernetes-network-policy-recipes&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;restricting-access-to-control-plane-services&#34;&gt;Restricting access to control plane services&lt;/h3&gt;
&lt;p&gt;Network controls in the infrastructure underlying the cluster must also be
considered. In a cloud provider environment, make sure that pods cannot
communicate with the instance metadata service. We also recommend the use of the
&lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/node/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Node Authorizer&lt;/a&gt;
to limit kubelet access to the API. When enabled, this special-purpose
authorization module restricts kubelet access to resources that are referenced
by Pods running on that specific node. The Node Authorizer is enabled by default
in recent releases of kubeadm. For example, instead of being able to access all
Secrets in the cluster, a kubelet can access only Secrets that are referenced by
Pods scheduled to that kubelet.&lt;/p&gt;
&lt;p&gt;Enforcing network controls in the infrastructure underlying your cluster is also
critical. In a cloud provider environment, make sure that your pods cannot talk
to the instance metadata service (for example,
&lt;code&gt;http://169.254.169.254/latest/meta-data&lt;/code&gt; on AWS EC2). Depending on your
requirements, you may also need to restrict access to the kubelet localhost
read-only port (10255 by default). This port exposes metadata about the pods
running on the node, which you may not want access to your applications.&lt;/p&gt;
&lt;h3 id=&#34;application-layer-access-control&#34;&gt;Application-layer access control&lt;/h3&gt;
&lt;p&gt;One solution to the problem of network-level access controls is strong
application-layer authentication such as
&lt;a href=&#34;https://en.wikipedia.org/wiki/Mutual_authentication&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;mutual TLS&lt;/a&gt;. Cryptographic
application identity is powerful because it allows identity to be efficiently
expressed across network boundaries. Securely provisioning certificates for
applications is still a hard problem in Kubernetes.&lt;/p&gt;
&lt;h3 id=&#34;limitations&#34;&gt;Limitations&lt;/h3&gt;
&lt;p&gt;Network access controls have some limitations in dynamic environments like
Kubernetes, which results in the following difficulties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Federating Kubernetes network policy across multiple clusters.&lt;/li&gt;
&lt;li&gt;Integrating Kubernetes network-level controls and granular network-level
controls expressed outside of the pod networking layer (for example, in AWS
EC2 Security Groups).&lt;/li&gt;
&lt;li&gt;When services running in a Kubernetes cluster need to communicate with
services outside the cluster, NetworkPolicy is often unable to filter traffic
as expected due to source and destination IP address translation.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you encounter any of these issues, we recommend that you define a more
coarse-grained network policy and rely on the application layer for fine-grained
access control.&lt;/p&gt;
&lt;h2 id=&#34;container-security&#34;&gt;Container Security&lt;/h2&gt;
&lt;h3 id=&#34;security-contextshttpskubernetesiodocstasksconfigure-pod-containersecurity-context&#34;&gt;&lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/security-context/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Security Contexts&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Security contexts limit what a Pod or Container can do and what privileges the
object has when running in the cluster. Example controls are the UID of the
process running inside the container, the filesystem access group, the process
capabilities, SELinux labels, etc.&lt;/p&gt;
&lt;p&gt;These can be applied to individual Pods and containers, and they define a set of
conditions that it must run with.&lt;/p&gt;
&lt;h3 id=&#34;pod-security-policies-psphttpskubernetesiodocsconceptspolicypod-security-policy&#34;&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/policy/pod-security-policy/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Pod Security Policies PSP&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Pod security policies are cluster-wide resources that provide automation of the
above described security contexts. PSPs can be used to automatically set
security context parameters or to prevent out-of-policy pods from running in the
cluster. For example, if you don&amp;rsquo;t want any containers in your cluster to run as
root, you can enforce this using a PSP with a &lt;code&gt;runAsUser&lt;/code&gt; rule of
&lt;code&gt;MustRunAsNonRoot&lt;/code&gt;. They define a set of conditions that a pod must run with to
be accepted into the system. Pod Security Policies are comprised of settings and
strategies that control the security features a pod has access to, and hence
this must be used to control pod access permissions. Strong pod security
policies make sure that pod access is appropriately controlled.&lt;/p&gt;
&lt;p&gt;Pod Security Policies provide a policy-driven mechanism for requiring
applications in your cluster to use container sandboxing in an approved way. For
example, you can require that all pods in a particular namespace run as
non-root, that they don&amp;rsquo;t mount host file systems and do not use host
networking.&lt;/p&gt;
&lt;p&gt;To use pod security policies, the PodSecurityPolicy admission controller must be
enabled in the API server configuration. Policies must be present before
enabling the controller, or no pods would be allowed to run. For more
information, see to the
&lt;a href=&#34;https://kubernetes.io/docs/concepts/policy/pod-security-policy/#run-another-pod&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Kubernetes documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;credentials-security-secrets&#34;&gt;Credentials security (Secrets)&lt;/h2&gt;
&lt;p&gt;Secrets are sensitive pieces of data such as passwords, tokens or keys.
Applications use secrets to access internal resources like the Kubernetes API or
external resources such as git repositories, databases, etc. The following
section details concerns related to secrets in the context of Kubernetes.&lt;/p&gt;
&lt;h3 id=&#34;secrets-management&#34;&gt;Secrets Management&lt;/h3&gt;
&lt;p&gt;Kubernetes has a core primitive for managing application secrets, appropriately
called a &lt;a href=&#34;https://kubernetes.io/docs/concepts/configuration/secret/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Secret&lt;/a&gt;.
Applications typically need secrets for two reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;They need access to a credential that proves their identity to another system
(for example, a database password or third-party API token).&lt;/li&gt;
&lt;li&gt;They need a cryptographic secret for some essential operation (for example, an
HMAC signing key for issuing signed HTTP cookies).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;identity-secrets&#34;&gt;Identity Secrets&lt;/h3&gt;
&lt;p&gt;For the first use case of application identity, follow the efforts of
&lt;a href=&#34;https://spiffe.io/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;SPIFFE&lt;/a&gt; and the Container Identity working group for a long
term solution to dynamically provisioning unique application identities. In the
near term, there is no well-established best practice in this area. Still, some
users have success integrating with existing certificate provisioning workflows
as part of a CI/CD pipeline. Simple Kubernetes-native solutions like
&lt;a href=&#34;https://github.com/jetstack/cert-manager&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;cert-manager&lt;/a&gt; may also work for your
use case.&lt;/p&gt;
&lt;h3 id=&#34;non-identity-secrets&#34;&gt;Non-identity Secrets&lt;/h3&gt;
&lt;p&gt;For the other use case, systems such as
&lt;a href=&#34;https://github.com/hashicorp/vault&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Vault&lt;/a&gt; perform cryptographic operations in
a centralized service. If you choose this option, make sure you understand the
entire chain of attestations involved in authenticating to the system. Often
these systems depend on Kubernetes secret resources as one step in the chain. In
Vault, the
&lt;a href=&#34;https://www.vaultproject.io/docs/auth/kubernetes.html&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Vault Kubernetes auth backend&lt;/a&gt;
authenticates pods by consuming a Kubernetes Service Account token. Still, the
token is stored as a secret object before it’s injected into the pod. This
pattern requires that you trust Vault not to replay your token and impersonate
the pod to the Kubernetes API.&lt;/p&gt;
&lt;h3 id=&#34;caveats-for-kubernetes-secrets&#34;&gt;Caveats for Kubernetes Secrets&lt;/h3&gt;
&lt;p&gt;You should be aware of the following limitations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Many standard components – for example, ingress controllers – require
permission to read all secrets in your cluster.&lt;/li&gt;
&lt;li&gt;Secrets are not encrypted at rest by default. You can, however, configure,
etcd, to encrypt secret data at rest. For details, see &lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Encrypting Secret Data
at Rest&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;auditing-to-support-security&#34;&gt;Auditing to support security&lt;/h2&gt;
&lt;p&gt;Audit logging must be explicitly enabled. It provides valuable insight into
access rules, compliance, and potential access issues. Kubernetes auditing
provides a security-relevant chronological set of records documenting the
sequence of activities that have affected the system by individual users,
administrators, or other components of the system.&lt;/p&gt;
&lt;p&gt;The Kubernetes API server audit log documents the sequence of cluster activities
performed by users, administrators, and system services. Each API request has
multiple stages that can be tracked and logged using an Audit Policy.&lt;/p&gt;
&lt;h3 id=&#34;enabling-audit-logging&#34;&gt;Enabling Audit Logging&lt;/h3&gt;
&lt;p&gt;The Kubernetes API server does not perform audit logging by default. We
recommend that you enable audit logging to a file by setting the following flags
in the API server configuration:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;--audit-log-path&lt;/code&gt; specifies the log file path that log backend uses to write
audit events.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--audit-log-maxage&lt;/code&gt; defines the maximum number of days to retain old audit log
files&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--audit-log-maxbackup&lt;/code&gt; defines the maximum number of audit log files to retain&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--audit-log-maxsize&lt;/code&gt; defines the maximum size in megabytes of the audit log
file before it gets rotated&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--audit-policy-file&lt;/code&gt; specifies the Audit policy file to be used&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Audit events can also be sent to a webhook backend, but we recommend logging to
a file that can be aggregated. Audit data should be treated as a high priority,
and care should be taken that the file specified for &amp;ndash;audit-log-path is
aggregated for multiple control plane nodes and handled by systems with high
reliability. In the case of an outage or other issue, administrators need to be
able to rely on the data produced by audit systems.&lt;/p&gt;
&lt;p&gt;When logging to files on the control plane hosts, you should set
&lt;code&gt;--audit-log-maxage&lt;/code&gt;, &lt;code&gt;--audit-log-maxbackup&lt;/code&gt;, and &lt;code&gt;--audit-log-maxsize&lt;/code&gt;
appropriately based on the available disk space before aggregation.&lt;/p&gt;
&lt;h2 id=&#34;node-and-container-runtime-hardening&#34;&gt;Node and container runtime hardening&lt;/h2&gt;
&lt;p&gt;It is of critical importance to consider the security of the container-host
boundary. This is important even in single-tenant environments since a remote
code execution vulnerability like &lt;a href=&#34;https://en.wikipedia.org/wiki/Shellshock_%28software_bug%29&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Shellshock
(CVE-2014-6271)&lt;/a&gt; or the
&lt;a href=&#34;https://groups.google.com/forum/#!topic/rubyonrails-security/61bkgvnSGTQ/discussion&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Ruby YAML parsing vulnerability (CVE-2013-0156)&lt;/a&gt;
can turn your otherwise trusted workload into a malicious agent. Without proper
hardening, that single remote code execution vulnerability can escalate into a
whole-node or whole-cluster takeover.&lt;/p&gt;
&lt;p&gt;Current container runtimes don’t provide the most reliable possible sandboxing,
but there are some steps you can take to help mitigate the risk of container
escape vulnerabilities:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Segment your Kubernetes clusters by integrity level — a simple but very
effective way to limit your exposure to container escape vulnerabilities. For
example, your dev/test environments might be hosted in a different cluster
than your production environment.&lt;/li&gt;
&lt;li&gt;Invest in streamlined host/kernel patching. Make sure that you have a way to
test new system updates (for example, a staging environment) and that your
applications can tolerate a rolling upgrade of the cluster without affecting
application availability.&lt;/li&gt;
&lt;li&gt;Kubernetes shines at orchestrating these upgrades. Once you build confidence
in letting Kubernetes dynamically rebalance application pods, patch management
at the node level becomes relatively easy. You can automate a rolling upgrade
that gracefully drains each node and either upgrade it in place or (in an IaaS
environment) replaces it with a new node. Investments in this area also
improve your overall resiliency to node-level outages.&lt;/li&gt;
&lt;li&gt;Run your applications as a non-root user. Root (UID 0) in a Linux container is
still the same user as root on the node. A combination of sandboxing
mechanisms restrict what code running in the container can do. Still, future
Linux kernel vulnerabilities are more likely to be exploitable by a root user
than by a non-privileged user.&lt;/li&gt;
&lt;li&gt;Enable and configure extra Linux security modules like SELinux and AppArmor.
These tools let you enforce more restrictive sandboxing on particular
containers. They are valuable in many situations, but building and maintaining
appropriate configurations requires a time investment. They may not be
appropriate for every application or environment.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;image-security&#34;&gt;Image Security&lt;/h2&gt;
&lt;h3 id=&#34;runtime-security&#34;&gt;Runtime Security&lt;/h3&gt;
&lt;p&gt;Runtime security is concerned with potential changes to a running container
through its lifetime, invalidating an initial security scan. A container image
could have been scanned and approved but become a liability as new
vulnerabilities, bugs, and threats are found. Runtime security tools help to
mitigate this problem by looking at what&amp;rsquo;s happening inside containers:
filesystem, process activity, networking behavior, etc. Examples of runtime
security tools: Falco, Aquasec, Twistlock, Sysdig.&lt;/p&gt;
&lt;h3 id=&#34;attack-surface-minimization&#34;&gt;Attack Surface Minimization&lt;/h3&gt;
&lt;p&gt;Minimize container footprint and attack surface by excluding extraneous
libraries and utilities that are not needed and could be leveraged during an
attack. Consider building images from scratch and include only what is necessary
at runtime. Also leverage multi-stage builds where applicable so that build
tools are not included in the final image used in production.&lt;/p&gt;
&lt;h3 id=&#34;container-image-scanning&#34;&gt;Container Image Scanning&lt;/h3&gt;
&lt;p&gt;Container image scanning is an integral part of building container images,
whether from source or third party base images, to discover any known
vulnerabilities and mitigate them before cluster deployment. One of the last
steps of your CI (Continuous Integration) pipeline involves building the
container images that would be pulled and executed in your environment.
Therefore, whether you are building Docker images from your code or using
unmodified third party images, it’s crucial to identify and find any known
vulnerabilities that may be present in those images. This process is known as
container image scanning. Container image scanning helps make sure that your
images are free from known vulnerabilities before they are deployed.
Appropriately managed scanning keeps your clusters safe by not introducing
malicious or malformed artifacts.&lt;/p&gt;
&lt;p&gt;Services and open source tools that provide image scanning include the
following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/coreos/clair&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Clair&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sysdig.com/opensource/falco/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Falco&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.aquasec.com/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Aqua Security&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.twistlock.com/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Twistlock&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We recommend integrating a container scan as part of a continuous delivery
pipeline. In addition, we recommend running periodic scans against stored images
so you can identify and mitigate new vulnerabilities.&lt;/p&gt;
&lt;p&gt;We also recommend deploying an ImagePolicyWebhook
&lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;admission controller&lt;/a&gt;
with the Kubernetes API server that only allows images that have passed security
scans to run in a cluster.&lt;/p&gt;
&lt;h4 id=&#34;deploying-the-imagepolicywebhook-in-kubernetes&#34;&gt;Deploying the ImagePolicyWebhook in Kubernetes&lt;/h4&gt;
&lt;p&gt;The ImagePolicyWebhook admission controller plugin queries a backend service to
determine whether a workload can be run on a cluster. It does not make any
changes to the submitted workload, but instead accepts or rejects it as-is based
on whether the images associated with the workload comply with the policy set
for the cluster. The webhook service queries the scanner tool to scan an image
and either accept the workload or reject it based on the scan results.&lt;/p&gt;
&lt;p&gt;The webhook service must use TLS.&lt;/p&gt;
&lt;p&gt;See &lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers#imagepolicywebhook&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;the documentation on ImagePolicyWebhook&lt;/a&gt;
for a detailed explanation.&lt;/p&gt;
&lt;h2 id=&#34;image-signing&#34;&gt;Image signing&lt;/h2&gt;
&lt;p&gt;Image signing helps make sure that your container images have not been tampered
with. In other words, image signing is used to prove the provenance of your
images. Signed images do not guarantee compliance, however.&lt;/p&gt;
&lt;p&gt;Image signing establishes image trust, ensuring that the image you run in your
cluster is the image you intended to run.&lt;/p&gt;
&lt;p&gt;Private image registries or private accounts on public registries let you
establish some degree of trust. Still, if your registry is compromised, bad
actors could replace your images with malicious versions.&lt;/p&gt;
&lt;p&gt;Image signing adds a layer of protection by cryptographically signing an image.
As long as your private keys are not compromised, you can be guaranteed that the
image you run is trusted.&lt;/p&gt;
&lt;p&gt;Note that you must also deploy a mechanism to verify image trust. An example is
&lt;a href=&#34;https://github.com/IBM/portieris&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;portieris&lt;/a&gt;, which allows you to configure
image security policies and stop the workload from being deployed if it is not
signed.&lt;/p&gt;
&lt;h2 id=&#34;patch-management-and-cicd&#34;&gt;Patch management and CI/CD&lt;/h2&gt;
&lt;h3 id=&#34;deployment-pipelines&#34;&gt;Deployment pipelines&lt;/h3&gt;
&lt;p&gt;A successful cluster access pattern is to have most users interact with the
production cluster only through a deployment pipeline. This pipeline consists of
one or more automated systems that handle building code into a container image,
running unit and integration tests and other validation steps such as pausing
for any manual approval. Depending on your needs, developers could still have
direct read-only access to the Kubernetes API or have a way to “break the glass”
and exec into pods during an incident.&lt;/p&gt;
&lt;p&gt;A robust application deployment pipeline is also the key to remediating
vulnerabilities in container images. You can use tools like
&lt;a href=&#34;https://github.com/coreos/clair&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Clair&lt;/a&gt; to identify known vulnerabilities in
the libraries and packages you use. Still, to release patches on time, you need
a trusted, automated way of rebuilding and testing patched versions of the
container.&lt;/p&gt;
&lt;h3 id=&#34;limiting-churn&#34;&gt;Limiting churn&lt;/h3&gt;
&lt;p&gt;Healthy Kubernetes clusters are dynamic environments. New versions of
applications are deployed, nodes disappear for kernel upgrades, deployments
scale up and down, and (hopefully) the users of your application never notice.
Making all this work in practice requires some diligence, but it’s critical to
reaping all the benefits of Kubernetes.&lt;/p&gt;
&lt;p&gt;One tool that can help put bounds on the amount of chaos introduced into your
cluster is the &lt;a href=&#34;https://kubernetes.io/docs/tasks/run-application/configure-pdb/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Pod Disruption Budget&lt;/a&gt;.
It’s useful when you have multiple automated systems, and you want to make sure
they don’t interact in unwanted ways. For example, an application-level bug might
leave some pods of your application temporarily unavailable. A pod disruption
budget could make sure that an automated rolling node upgrade doesn’t terminate
the remaining healthy copies of your application.&lt;/p&gt;
&lt;h3 id=&#34;overly-privileged-container-builds&#34;&gt;Overly privileged container builds&lt;/h3&gt;
&lt;p&gt;One Docker-specific anti-pattern to avoid in your build pipeline is mounting the
host-level Docker control socket “/var/run/docker.sock” into a container during
a build. Access to this socket is equivalent to root on the host, which means
any running build could compromise the node. This is doubly true if your build
system runs build before a manual code review (a typical pattern).&lt;/p&gt;
&lt;h2 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h2&gt;
&lt;h3 id=&#34;what-to-do-now&#34;&gt;What to do now&lt;/h3&gt;
&lt;p&gt;We keep saying it: how you secure your Kubernetes cluster depends in part on
your available resources and your application requirements. Consider each
element in the broader security picture and spend some time upfront assessing
how important it is to your needs overall. At a very high level, some of our
recommendations fit nicely into larger best practices in deployment:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Automated deployment pipeline and scheduler. Lets you simplify host and
application patch management with rolling upgrades that are integrated into
the rest of your overall development cycle.&lt;/li&gt;
&lt;li&gt;Integrated access controls at appropriate levels. (authz/authn with API
integration)&lt;/li&gt;
&lt;li&gt;Integrated logging and monitoring. You log and monitor for performance and
reliability &amp;ndash; adding support for security-specific events and pod metadata is
non-trivial but vital. Precisely what to monitor depends as always on your
specific needs.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;planning-for-the-future&#34;&gt;Planning for the future&lt;/h3&gt;
&lt;p&gt;Security is an increasing concern for everyone, and initiatives are well
underway to improve the security landscape. Keep an eye out for developments on
these fronts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;More strongly encrypted identity specific to your hardware or cloud provider&lt;/li&gt;
&lt;li&gt;Stronger provenance for cryptographically signed binaries/images&lt;/li&gt;
&lt;li&gt;Automatically updated inventories&lt;/li&gt;
&lt;li&gt;More sophisticated alerts and monitoring&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;other-resources&#34;&gt;Other Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Securing a Cluster (Kubernetes documentation)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1xeagoDn-6kQ6FPdfX9IlD5MjWalW2o8PeCt20DEfFpg/edit&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Hacking and Hardening Kubernetes By Example (slides from Brad Geesaman)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;popular-tooling-and-approaches&#34;&gt;Popular Tooling and Approaches&lt;/h2&gt;
&lt;h3 id=&#34;encryption-configuration&#34;&gt;Encryption Configuration&lt;/h3&gt;
&lt;p&gt;The Kubernetes docs provide
&lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;instructions to enable encryption at rest&lt;/a&gt;
for specified resources. Most importantly, this allows cluster operators to
ensure the data contained in Secrets has a layer of protection against a
malicious actor gaining access to the storage disk for etcd.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Adds a layer of protection for data contained in Secrets&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Is compromised if attacker gains access to the encryption keys used by the API Server&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;validating-admission-webhook-controllers&#34;&gt;Validating Admission Webhook Controllers&lt;/h3&gt;
&lt;p&gt;Kubernetes RBAC provides a useful way to manage access to resources. However, it
does not provide ways to restrict access based on external systems or the
attributes of the resource being created. Proper authorization may require a
deeper understanding of the requester, evaluation of business logic, or
understanding of the cluster&amp;rsquo;s current state to determine whether a request
should be authorized. Kubernetes offers preset admission controllers built into
the &lt;code&gt;kube-apiserver&lt;/code&gt;, such as &lt;code&gt;PodSecurityPolicy&lt;/code&gt;. These can be enabled by
altering flags on the &lt;code&gt;kube-apiserver&lt;/code&gt;. To provide custom admission control
without modifying the API server, Kubernetes offers an Admission webhook
functionality. In this model, Kubernetes offers selected inbound request to an
external service that can approve or deny the request.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/platform-security/diagrams/k8s-admission-flow.png&#34; alt=&#34;Kubernetes Admission Flow&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Writing an admission webhook controller in code enables you to do complex logic
and access Kubernetes objects structurally, through a language&amp;rsquo;s type system.
For example, see &lt;a href=&#34;https://github.com/kubernetes/apimachinery&#34;&gt;https://github.com/kubernetes/apimachinery&lt;/a&gt;. A downside to this
approach is maintaining the controller code base over time, which can involve
keeping logic up to date against changing Kubernetes API versions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Flexible option for resource validation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Webhook is on the critical path for resource management in the cluster
&lt;ul&gt;
&lt;li&gt;Development &amp;amp; maintenance overhead for the webhook&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;open-policy-agent-opa&#34;&gt;Open Policy Agent (OPA)&lt;/h3&gt;
&lt;p&gt;Similar to Validating Admission Webhook Controllers,
&lt;a href=&#34;https://www.openpolicyagent.org&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;OPA&lt;/a&gt; performs validation on requests sent to
it. OPA is a general purpose policy agent. It&amp;rsquo;s primary goal is to unify
policies around a centralized model and language. It uses a DSL called
&lt;a href=&#34;https://www.openpolicyagent.org/docs/latest/#rego&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Rego&lt;/a&gt; that analyzes input,
usually JSON, and provides an output. For Kubernetes admission control, the
request is typically the JSON from an
&lt;a href=&#34;https://godoc.org/k8s.io/api/admission/v1beta1#AdmissionReview&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;AdmissionReview&lt;/a&gt;
object and the response is the same object with the
&lt;a href=&#34;https://godoc.org/k8s.io/api/admission/v1beta1#AdmissionResponse&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;AdmissionResponse&lt;/a&gt;
filled in.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/platform-security/diagrams/k8s-admission-flow-opa.png&#34; alt=&#34;Admission Flow with OPA&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;While this unified model is great, it can be harder to do complex logic in rego
over a general purpose language.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Powerful validation framework&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Requires learning a new policy definition language: Rego&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;network-policy&#34;&gt;Network Policy&lt;/h3&gt;
&lt;p&gt;Kubernetes provides a &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/network-policies/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;NetworkPolicy
API&lt;/a&gt;.
Verify that your CNI-Plugin enforces policies. By default, Kubernetes allows
traffic to and from any pod or external source that can reach it. With this in
mind, enforcing Network Policy is critical to protect applications from unwanted
access. How policy is enforced depends on the CNI-plugin. For example, in Calico
it&amp;rsquo;s enforced via IPtables and in Cilium it&amp;rsquo;s BPF. If network policy if a large
part of your Kubernetes design, ensure the solution you&amp;rsquo;re using offers a
scalable approach to enforcement.&lt;/p&gt;
&lt;p&gt;Along with the Kubernetes network policy API, some CNI-Plugins offer their own
CRDs that extend the base functionality. Calico offers a
&lt;a href=&#34;https://docs.projectcalico.org/v3.9/reference/resources/globalnetworkpolicy&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;GlobalNetworkPolicy&lt;/a&gt;
and
&lt;a href=&#34;https://docs.projectcalico.org/v3.9/reference/resources/networkpolicy&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;NetworkPolicy&lt;/a&gt;
CRD. The primary trade-off to using a CNI-specific CRD is portability. Should
you choose to change plugins in the future, you may need to convert policies
between types. However, some of the plugin-specific policy features are very
compelling for various cluster architectures.&lt;/p&gt;
&lt;p&gt;Historically, organizations have attempted to keep their existing network models
outside of Kubernetes and make Kubernetes fit within it. For example, this can
include making layers of Kubernetes nodes such as running nodes in Kubernetes
dedicated to certain layers such as Web, App, and Data. This moves away from
Kubernetes declarative approach and add a lot of complexity to the system. We
highly recommend considering intra-cluster and workload traffic policies into
the Kubernetes API.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Essential network controls can be enforced&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Differences in implementation between CNI plugins can be a challenge&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: Service Routing</title>
      
      <link>/guides/kubernetes/service-routing/</link>
      <pubDate>Wed, 24 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/guides/kubernetes/service-routing/</guid>
      <description>

        
        &lt;p&gt;Fundamental to the deployment of most software is the ability to route traffic
to network services. This is especially true when the software platform adopts a
microservices architecture.&lt;/p&gt;
&lt;p&gt;Traditionally, exposing such services has been an arduous task. Concerns such as
service discovery, port contention, and even load balancing were often left as
an exercise for the operator. These capabilities were, no doubt, available, but
were often configured and operated through manual user intervention.&lt;/p&gt;
&lt;p&gt;Fortunately, Kubernetes provides a number of primitives that allow us to route
service traffic across the cluster as well as from external sources. Just as all
other Kubernetes resources, these are configured in a declarative way.&lt;/p&gt;
&lt;h2 id=&#34;the-kubernetes-service-resource&#34;&gt;The Kubernetes Service Resource&lt;/h2&gt;
&lt;p&gt;The Kubernetes Service resource is a core API type that allows users to define
how service endpoints may be exposed to client applications. This construct
operates between layer 3 and 4 of the OSI networking stack. This resource,
natively, offers three types of services that provide various levels of service
exposure.&lt;/p&gt;
&lt;p&gt;An example Service declaration may take the following form:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Service&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;mysql&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;namespace&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;myapp&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;type&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;ClusterIP&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# this is the default value if unspecified&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;selector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;app&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;mysql&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;ports&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;protocol&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;TCP&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;port&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;3307&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;targetPort&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;3306&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This Service will provide Layer 3/4 access to mysql Pods that are labeled with
the &lt;code&gt;app: mysql&lt;/code&gt; key-value pair within the &lt;code&gt;myapp&lt;/code&gt; namespace. While the Pod
exposes the service on the standard mysql port (3306), the Service may
selectively expose it on an alternate port (in this case, 3307).&lt;/p&gt;
&lt;p&gt;Each of the Service types build on the previous type, beginning with ClusterIP
as the most basic type.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/service-routing/diagrams/service-routing.png&#34; alt=&#34;Service Routing&#34;  /&gt;&lt;/p&gt;
&lt;h3 id=&#34;clusterip&#34;&gt;ClusterIP&lt;/h3&gt;
&lt;p&gt;The ClusterIP Service type is used to expose a Pod&amp;rsquo;s layer 4 endpoint to the
rest of the cluster. As stated earlier, this service type serves as the most
basic of all of the types. This construct is namespaced, and has two primary
functions: to provide a cluster-local virtual IP and an associated DNS entry.
This virtual IP is pulled from an IP pool that is dedicated for all services,
and once created it is used as the target for a consistent DNS record.&lt;/p&gt;
&lt;p&gt;These DNS records take the fully-qualified form of:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;&amp;lt;service name&amp;gt;.&amp;lt;namespace&amp;gt;.svc.cluster.local&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;From the example above, this record would take the form:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;mysql.myapp.svc.cluster.local&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;These records may be used for service discovery from within the cluster. They
may be used to address services across namespaces as well as within the same
namespace. In the case of intra-namespace resolution, they may be addressed with
just the short name of the record.&lt;/p&gt;
&lt;p&gt;When a service type is not specified, ClusterIP is the default.&lt;/p&gt;
&lt;h3 id=&#34;nodeport&#34;&gt;NodePort&lt;/h3&gt;
&lt;p&gt;NodePort services provide a mechanism for exposing services to external
entities. In this case, a high port will be opened on all Nodes in the cluster.
The range from which this port will be allocated may be specified through the
&lt;code&gt;--service-node-port-range&lt;/code&gt; &lt;code&gt;kube-api-server&lt;/code&gt; configuration parameter. The
default port range can be consulted in the
&lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/service/#nodeport&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;official documentation.&lt;/a&gt;
While a valid, unused port that falls within the range may be specified in the
declaration, the most common case calls for the port to be selected
automatically by Kubernetes itself.&lt;/p&gt;
&lt;p&gt;When a NodePort service type is specified, this functionality will build upon
the ClusterIP construct. In other words, when specified, NodePort will
instantiate the functionality of both NodePort as well as ClusterIP. An external
user will be able to address an in-cluster service by connecting to any single
node in the cluster at the high port which has been dedicated to the service.
Likewise, clients that originate their connection from within the cluster may
utilize either the virtual IP and/or DNS entries provided by the ClusterIP
functionality.&lt;/p&gt;
&lt;p&gt;This functionality can be a critical component for exposing services to external
clients, but it can be a bit cumbersome. An external client would need to know
the IP address of cluster Nodes as well as the ephemeral high port that has been
delegated to the Service. While custom service discovery mechanisms may be
reliably configured with this ephemeral data, in the next section we will
demonstrate how NodePort may be utilized to front Services with an external load
balancer.&lt;/p&gt;
&lt;p&gt;Node ports are opened on all Nodes within the cluster. Therefore, there is no
guarantee that your destination Pod is also colocated on that Node. Once the
traffic hits the Node port, it will be forwarded through the ClusterIP service
that may direct it to a Pod on another Node, even if a suitable Pod is running
on that Node. This behavior can be controlled by configuring your Service with
an &lt;code&gt;externalTrafficPolicy&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;An &lt;code&gt;externalTrafficPolicy&lt;/code&gt; denotes if the traffic received is desired to be
routed to node-local or cluster-wide endpoints. The default value for this
parameter is &lt;code&gt;Cluster&lt;/code&gt;, which as described above will forward the NodePort
traffic to any Pod running in the cluster, even if it is running on a Node
different than the one receiving the traffic. When choosing &lt;code&gt;Local&lt;/code&gt; for this
setting, the client source IP is preserved and a second hop to another node is
avoided, but this potentially limits the load-spread across the cluster and
risks imbalanced traffic spreading.&lt;/p&gt;
&lt;h3 id=&#34;loadbalancer&#34;&gt;LoadBalancer&lt;/h3&gt;
&lt;p&gt;The LoadBalancer Service type, again, builds on the foundation provided by the
NodePort and ClusterIP functionality. When this type is specified, both
in-cluster and external access is configured as outlined above. Additionally, an
external load balancer will also be configured to front all of the NodePort
services.&lt;/p&gt;
&lt;p&gt;This Service type requires the cluster is built on top of infrastructure that
provides load balancing functionality. Nearly all cloud IaaS providers (i.e. AWS,
GCP, Azure, etc.), provide support for these services through Kubernetes cloud
provider integrations. Additionally, there are third-party integrations (i.e. F5,
NSX, etc.) that may also provide load balancing functionality that implements
the Kubernetes Service resource.&lt;/p&gt;
&lt;h2 id=&#34;ingress&#34;&gt;Ingress&lt;/h2&gt;
&lt;p&gt;As web-based services continue to grow in popularity, whether those are
user-facing interfaces or REST/GraphQL APIs, it is likely that these types of
applications will constitute the majority of what gets deployed on an average
cluster. With these types of applications, there are a number of features that
are necessary for a production deployment. First and foremost, these
applications will require redundancy, and this is typically achieved by
deploying a number of discreet instances of the web-based application and, in
turn, fronting these with a layer 7 proxy. This reverse proxy will register a
number of upstream instances of the application, ensuring that each is reachable
by way of a health check, and forwarding traffic to healthy upstreams according
to the declared configuration.&lt;/p&gt;
&lt;p&gt;These configurations will provide mechanisms for the traffic to be qualified by
a number of rules before it is forwarded on to the upstream instances of the
application. These rules for evaluation may include conditions such as the value
of the &lt;code&gt;Host&lt;/code&gt; header in the HTTP request, as well as the specific paths that are
being requested. Once a rule has evaluated to a known upstream, the traffic may
be passed as-is or forwarded on with specified modifications (i.e. header
changes, path rewrites, etc).&lt;/p&gt;
&lt;p&gt;Kubernetes provides a mechanism for easy configuration of an in-cluster reverse
proxy deployment with its Ingress resource. This resource serves as a generic
configuration for nearly any reverse proxy. As Pods for a Service scale up or
down (again, these are determined by label selectors), the Ingress controller
will, in turn, add or remove the Pod&amp;rsquo;s IP from the pool of upstream Endpoints.
Endpoints for a Service may be queried with the &lt;code&gt;kubectl get endpoints&lt;/code&gt; command.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/service-routing/diagrams/service-routing-ingress.png&#34; alt=&#34;Service Routing Ingress&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Some proxies, however, also provide functionality above and beyond what would be
considered a common feature set. In this case, custom features may be
supplemented with annotations on the resource.&lt;/p&gt;
&lt;p&gt;In addition to specific implementation details, annotations may be utilized for
adjacent and/or complementary functionality. For instance, with the
&lt;a href=&#34;https://github.com/jetstack/cert-manager&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;cert-manager&lt;/a&gt; project, annotations
are placed on an Ingress resource so that the reverse proxy may secure the
deployment with TLS certificates.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;networking.k8s.io/v1beta1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Ingress&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;test-ingress&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;annotations&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;nginx.ingress.kubernetes.io/rewrite-target&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;rules&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;http&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;paths&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;/testpath&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;            &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;backend&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;              &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;serviceName&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;test&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;              &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;servicePort&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;80&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;As with any Kubernetes controller, a user may declare a desired state and the
controller is responsible for reconciling that configuration towards that
desired state. The case for Ingress controllers is no different. The controller
will watch for any updates to the full collection of Ingress objects and, in
turn, configure the reverse proxy to reflect the desired state. In the case of
proxies such as nginx, this will amount to the controller writing a collection
of nginx configuration files, and reloading the nginx service. Alternatively,
with more modern proxies (e.g. Envoy managed by the Contour Controller),
upstreams and other configurations may be manipulated with an API, and thus do
not require reloads. This functionality is advantageous as it will not disrupt
any in-flight connections.&lt;/p&gt;
&lt;h2 id=&#34;service-mesh&#34;&gt;Service Mesh&lt;/h2&gt;
&lt;p&gt;As mentioned earlier, natively, Kubernetes service routing is concerned with
providing the layer 3 and 4 plumbing that will connect a client to a network
service that is being served from within the cluster. There is, however, a
desire amongst the industry to provide additional routing capabilities. While
layer 3 and 4 can provide adequate mechanisms for connectivity, it is incapable
of providing any data or features for application-centric concerns. As one
extends into the higher layers, we can start to change the way we deploy and
observe these applications.&lt;/p&gt;
&lt;p&gt;Service mesh deployments make use of the native Kubernetes service constructs,
but layer on features implemented with layers 5 through 7. Some of the features
afforded by service mesh controllers include mutual TLS, tracing, circuit
breaking, dynamic routing, rate limiting, load balancing, and more. This is
typically implemented by placing lightweight proxies at application boundaries
within the Kubernetes cluster. Specifically, in the case of projects like
&lt;a href=&#34;https://istio.io&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Istio&lt;/a&gt; this is accomplished by automatically attaching an
Envoy proxy container to all Pods. Envoy, when deployed in this way, can provide
the features mentioned above. This does, however, come at the price of
additional resource utilization.&lt;/p&gt;
&lt;p&gt;While service mesh implementations are relatively nascent in the wild, it is
quickly becoming a must-have feature set. And, we fully expect that service mesh
deployments will continue to expand in size and scope. Let&amp;rsquo;s review some of the
common service mesh features.&lt;/p&gt;
&lt;h3 id=&#34;mutual-tls&#34;&gt;Mutual TLS&lt;/h3&gt;
&lt;p&gt;Mutual TLS provides some very attractive features for application developers and
platform operators alike. As with most deployments, there will be requirements
for adhering to security constraints before going to production. One of the most
popular requests is that all network traffic should be encrypted. While this may
be achieved with some CNI implementations, many do not provide this type of
overlay capability. Regardless of what the nature of our network fabric looks
like, we can provide this capability seamlessly with a service mesh operating at
layer 5 and 7 of the OSI model.&lt;/p&gt;
&lt;p&gt;In addition to the requirement that all traffic be encrypted, mutual TLS also
provides service-to-service identity. In other words, we can ensure that only
services that we have been authorized to transact with one another may do so. Of
course we can implement firewall policies that would also provide similar
capabilities, but these are not nearly as advanced as what TLS can provide.
Firewall rules merely indicate which IP and port combinations may initiate and
complete connections. Mutual TLS, however, can ensure cryptographically which
layer 7 connections will be able to be initiated and completed.&lt;/p&gt;
&lt;h3 id=&#34;tracing&#34;&gt;Tracing&lt;/h3&gt;
&lt;p&gt;In dynamic environments like Kubernetes, there is often a desire to understand
how microservices are connected to one another. Not only are we concerned with
what services speak to each other, we are also concerned with understanding to
what degree they do so. How many connections are there, and how frequently? Or,
are there some service-to-service connections that are slower than others?&lt;/p&gt;
&lt;p&gt;Due to service mesh-placed proxies alongside the application, we can now
automatically add Open Tracing headers to help us understand how services
interact. And, this is precisely how tracing works with service mesh
technologies: headers are placed on intra-service traffic, and may then be
analyzed by tools such as &lt;a href=&#34;https://jaegertracing.io&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Jaeger&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;circuit-breaking&#34;&gt;Circuit Breaking&lt;/h3&gt;
&lt;p&gt;Circuit breaking allows for advanced patterns concerning failure detection. Now,
instead of relying on the, relatively simplistic, health checks offered by layer
3 and layer 4 (i.e. can we connect?), we can now programmatically remove
upstreams based on more qualified data. Because service mesh operates at the
application layer, we can add criteria concerning HTTP status codes, response
times, number of pending requests and the like. Utilizing application data in
this way helps to provide a better end-user experience.&lt;/p&gt;
&lt;h3 id=&#34;advanced-request-routing&#34;&gt;Advanced Request Routing&lt;/h3&gt;
&lt;p&gt;Service mesh implementations also allow for routing traffic based upon policy
that extend above and beyond what is capable with native Kubernetes constructs
alone. Advanced patterns, such as canary, weighted, and/or blue/green
deployments, are available through service mesh-specific resource types.
Likewise, this capability also enables advanced development patterns. Users may
be directed to specific application instances based on designated headers and
even the request&amp;rsquo;s attached user identity.&lt;/p&gt;
&lt;h3 id=&#34;load-balancing&#34;&gt;Load Balancing&lt;/h3&gt;
&lt;p&gt;While Kubernetes Ingress controllers are implemented with load balancing reverse
proxies, service mesh is capable of providing features above what is typically
provided by these third-party controllers. Whereas most Ingress controllers are
a bit limited with regard to how they maybe configured through the Ingress
resource type, service mesh provides additional resource types that allow a user
to be more expressive. Service mesh also typically provides for some advanced
capabilities like locality-based balancing, with failover based upon priority
designations.&lt;/p&gt;
&lt;h2 id=&#34;popular-tooling-and-approaches&#34;&gt;Popular Tooling and Approaches&lt;/h2&gt;
&lt;h3 id=&#34;ingress-1&#34;&gt;Ingress&lt;/h3&gt;
&lt;h4 id=&#34;nginx-ingress-controller&#34;&gt;NGINX Ingress Controller&lt;/h4&gt;
&lt;p&gt;The &lt;a href=&#34;https://github.com/kubernetes/ingress-nginx&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;NGINX Ingress Controller&lt;/a&gt; is
the most commonly deployed Ingress controller within the Kubernetes ecosystem.
It is maintained in the open as a Kubernetes community project. It is built on
the decades-old NGINX reverse proxy and, is therefore, well understood and a
technology that many organizations have already operationalized and/or have
familiarity with.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Broad adoption.&lt;/li&gt;
&lt;li&gt;Well-tested in production scenarios.&lt;/li&gt;
&lt;li&gt;Extends the Ingress resource with dozens of implementation-specific
annotations.&lt;/li&gt;
&lt;li&gt;Functionality may be extended with Lua scripting.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Technology is not well-suited for highly dynamic environments.
&lt;ul&gt;
&lt;li&gt;All Ingress changes require that NGINX reloads the process in order to apply
the changes. This may have a detrimental impacts on in-flight connections.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;contour&#34;&gt;Contour&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://projectcontour.io&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Contour&lt;/a&gt; is an Ingress controller that is developed
and maintained by VMware. Built on top of the
&lt;a href=&#34;https://www.envoyproxy.io/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Envoy&lt;/a&gt; proxy from Lyft. It offers a performant,
cloud native solution for Ingress control.&lt;/p&gt;
&lt;p&gt;Because Envoy is configurable via gRPC APIs, this means that new route
configurations may be applied dynamically and without disrupting any in-flight
connections.&lt;/p&gt;
&lt;p&gt;Contour adds a Custom Resource Definition called HTTPProxy, which enables
advanced capabilities that may not be expressed with Ingress normally. Features
such as route delegation, multi-service routes, weighted endpoints, and load
balancing strategies allow end users to craft highly-specific application
rollout patterns, thus further enabling CI/CD.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Built on top of Envoy, a highly performant and scalable reverse proxy.&lt;/li&gt;
&lt;li&gt;Ingress configuration is applied via an API; not static configuration files.&lt;/li&gt;
&lt;li&gt;Extends Ingress control to support multi-team environments.
&lt;ul&gt;
&lt;li&gt;HTTPProxy provides richer configuration than is available with the Ingress
resource alone. Some of these extended features include weighted routes
and specification of load balancing strategies without the use of
annotations.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Relatively new within the ecosystem, only reaching GA recently.
&lt;ul&gt;
&lt;li&gt;Features that already exist for other Ingress controllers may not yet be
available for Contour.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;traefik&#34;&gt;Traefik&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://containo.us/traefik/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Traefik&lt;/a&gt; bills itself as the &amp;ldquo;Cloud Native Edge
Router.&amp;rdquo; While we have seen limited numbers of deployments leveraging Traefik,
it has often been utilized in cases where other solutions did not provide
equivalent functionality. Specifically, features like header-based routing have
been implemented with Traefik in the past, but as this functionality becomes
more commonplace, the need for Traefik to fill this gap will likely diminish.&lt;/p&gt;
&lt;p&gt;Traefik is supported by &lt;a href=&#34;https://containo.us&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Containous&lt;/a&gt; and is developed as
open source software.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Has some extended features that may not be available with other Ingress
solutions.&lt;/li&gt;
&lt;li&gt;Seamless integrations with services such as Let&amp;rsquo;s Encrypt&lt;/li&gt;
&lt;li&gt;Full-featured dashboard&lt;/li&gt;
&lt;li&gt;Multi-platform support beyond Kubernetes alone.
&lt;ul&gt;
&lt;li&gt;Docker&lt;/li&gt;
&lt;li&gt;Rancher&lt;/li&gt;
&lt;li&gt;Marathon&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Has not seen strong traction within the community.&lt;/li&gt;
&lt;li&gt;Deep feature list, but configuration can be a bit complicated.&lt;/li&gt;
&lt;li&gt;Some Enterprise features are not open sourced.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;service-mesh-1&#34;&gt;Service Mesh&lt;/h3&gt;
&lt;h4 id=&#34;linkerd&#34;&gt;Linkerd&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://linkerd.io/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Linkerd&lt;/a&gt; is an ultra lightweight service mesh for
Kubernetes. It provides end users with the features that they would expect from
a service mesh solution: runtime debugging, observability, reliability, and
security. Linkerd is a fully open source solution falling under the Apache 2
license.&lt;/p&gt;
&lt;p&gt;Features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;HTTP, HTTP/2, and gRPC Proxying&lt;/li&gt;
&lt;li&gt;TCP Proxying and Protocol Detection&lt;/li&gt;
&lt;li&gt;Retries and Timeouts&lt;/li&gt;
&lt;li&gt;Automatic mTLS&lt;/li&gt;
&lt;li&gt;Ingress&lt;/li&gt;
&lt;li&gt;Telemetry and Monitoring&lt;/li&gt;
&lt;li&gt;Automatic Proxy Injection&lt;/li&gt;
&lt;li&gt;Dashboard and Grafana&lt;/li&gt;
&lt;li&gt;Distributed Tracing&lt;/li&gt;
&lt;li&gt;Fault Injection&lt;/li&gt;
&lt;li&gt;High Availability&lt;/li&gt;
&lt;li&gt;Service Profiles&lt;/li&gt;
&lt;li&gt;Traffic Split (canaries, blue/green deploys)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lightweight service mesh solution which requires absolutely no changes to
application code.&lt;/li&gt;
&lt;li&gt;Contains all of the features that would be expected from a service mesh
solution.&lt;/li&gt;
&lt;li&gt;Was one of the first service mesh options and is quite mature as a result.&lt;/li&gt;
&lt;li&gt;Recently rewritten in the aim of improving performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linkerd doesn&amp;rsquo;t provide an Ingress Controller. The &lt;a href=&#34;https://linkerd.io/2/tasks/using-ingress/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;project
documentation&lt;/a&gt; has information
about integrating with Nginx, Contour, and others. This means Linkerd requires
managing Ingress as additional operational overhead.&lt;/li&gt;
&lt;li&gt;Traffic splitting syntax can be cumbersome.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;istio&#34;&gt;Istio&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://istio.io&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Istio&lt;/a&gt; is perhaps the most popular service mesh offering
today. It is an open source project that is being maintained by Google, IBM, and
Red Hat. Just as with Linkerd, it has dozens of features that one would expect
to see in a service mesh solution.&lt;/p&gt;
&lt;p&gt;Istio makes it easy to create a network of deployed services with load
balancing, service-to-service authentication, monitoring, and more, with few or
no code changes in service code. You add Istio support to services by deploying
a special sidecar proxy throughout your environment that intercepts all network
communication between microservices, then configure and manage Istio using its
control plane functionality, which includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Automatic load balancing for HTTP, gRPC, WebSocket, and TCP traffic.&lt;/li&gt;
&lt;li&gt;Fine-grained control of traffic behavior with rich routing rules, retries,
failovers, and fault injection.&lt;/li&gt;
&lt;li&gt;A pluggable policy layer and configuration API supporting access controls,
rate limits and quotas.&lt;/li&gt;
&lt;li&gt;Automatic metrics, logs, and traces for all traffic within a cluster,
including cluster ingress and egress.&lt;/li&gt;
&lt;li&gt;Secure service-to-service communication in a cluster with strong
identity-based authentication and authorization.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Istio has a huge amount of momentum behind it. It is currently the most
popular service mesh offering on the market.&lt;/li&gt;
&lt;li&gt;It forms the basis of VMware&amp;rsquo;s NSX-SM solution.&lt;/li&gt;
&lt;li&gt;It is extraordinarily full-featured, but this also creates a large degree of
complexity.&lt;/li&gt;
&lt;li&gt;Istio may be used as both an Ingress and a service mesh with the Ingress
Gateway feature.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Istio does not have an open governance model. Its steering committee is run by
IBM, Google, and Red Hat.&lt;/li&gt;
&lt;li&gt;Istio is in rapid development and many features are at various levels of
stability, some of which may not be suitable for production.&lt;/li&gt;
&lt;li&gt;Configuration can be extraordinarily complex.&lt;/li&gt;
&lt;li&gt;You may be required to leverage the Istio ingress controller exclusively in
order to leverage the features you are interested in.&lt;/li&gt;
&lt;li&gt;The complexity that Istio introduces to a Kubernetes deployment often mandates
a team dedicated to its operation and support.&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: Storage Integration</title>
      
      <link>/guides/kubernetes/storage-integration/</link>
      <pubDate>Wed, 24 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/guides/kubernetes/storage-integration/</guid>
      <description>

        
        &lt;p&gt;Core Kubernetes does not concern itself with storage integration. At most, it
provides a set of APIs, &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/persistent-volumes/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Persistent
Volumes&lt;/a&gt;,
&lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Persistent Volume
Claims&lt;/a&gt;,
and &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/storage-classes&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Storage
Classes&lt;/a&gt;. By
default, containers can use their own ephemeral storage system and/or leverage
host storage. Both of these solutions are typically inadequate for enterprise
workloads. Ephemeral storage goes away if the container dies. Host storage ties
the container to a specific host and (depending on how you access host storage)
it can be insecure in multi-tenant environments.&lt;/p&gt;
&lt;p&gt;The model used by most enterprise platforms is to introduce a provider that can
enable dynamic provisioning of volumes for workloads requiring some amount of
persistence. Integration to providers is typically accomplished through a
container storage interface (CSI) plugin. The following demonstrates the
relationship.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/storage-integration/diagrams/dynamic-storage-provisioning.png&#34; alt=&#34;Dynamic Storage Provisioning&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;There is high variance in how the above works based on the provider. Some
providers create multiple PVs ahead of time and make them available to workloads.
The above is only meant to give a conceptual overview of how the flow might
work.&lt;/p&gt;
&lt;h3 id=&#34;container-storage-interface-csi&#34;&gt;Container Storage Interface (CSI)&lt;/h3&gt;
&lt;p&gt;Kubernetes uses the &lt;a href=&#34;https://github.com/container-storage-interface/spec&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;container storage
interface&lt;/a&gt; (CSI) to provide
storage functionality to containers. Storage is implemented in CSI plugins. This
interface / plugin model enables Kubernetes to support many storage options
implemented via plugins (or drivers) such as
&lt;a href=&#34;https://github.com/kubernetes-sigs/vsphere-csi-driver&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;vSphere&lt;/a&gt;,
&lt;a href=&#34;https://dell.github.io/storage-plugin-docs/docs/dell-csi-driver/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;DellEMC&lt;/a&gt;,
&lt;a href=&#34;https://github.com/libopenstorage/openstorage/tree/master/csi&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;portworx&lt;/a&gt;, &lt;a href=&#34;https://github.com/kubernetes-sigs/aws-ebs-csi-driver&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;AWS
EFS&lt;/a&gt;, and
&lt;a href=&#34;https://github.com/NetApp/trident&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;NetApp&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A more complete list is available in the &lt;a href=&#34;https://kubernetes-csi.github.io/docs/drivers.html&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;CSI driver documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Prior to standardization around CSI, the implementation of storage integrations
had high-variance across providers. There are two common models for running
storage drivers in Kubernetes. These are through cloud providers and dedicated
storage providers. Cloud providers (such as AWS and vSphere) package their
storage driver into the provider, so that it can handle all the integration
points such as provisioning load balancers and storage volumes. Dedicated
integrations run as independent processes managing only storage. Below you&amp;rsquo;ll
find explanations on each.&lt;/p&gt;
&lt;h3 id=&#34;in-tree-providers&#34;&gt;In-tree Providers&lt;/h3&gt;
&lt;p&gt;Historically, Kubernetes relied on in-tree &amp;ldquo;cloud&amp;rdquo; provider functionality for
most storage integration. This method of integration predates CSI. These
providers are referred to as in-tree because &lt;a href=&#34;https://github.com/kubernetes/kubernetes/tree/v1.18.0-alpha.2/pkg/cloudprovider&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;their code lives in the core
kubernetes/kubernetes
repo&lt;/a&gt;.
With this model, every Kubernetes cluster has cloud provider logic in it, even
if it wasn&amp;rsquo;t activated. In a cluster integrated with vCenter, the
kube-apiserver, controller-manager, and kubelet will look as follows.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/storage-integration/diagrams/in-tree-provider.png&#34; alt=&#34;In-tree provider&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;As you can imagine, shipping these components with cloud-provider logic for
every cut of Kubernetes is not a good model. Additionally, the in-tree model
does not allow you to update the provider without updating your cluster. In-tree
providers have been deprecated and are planned to be removed. You should &lt;strong&gt;not&lt;/strong&gt;
use an in-tree provider for storage integration of your platform.&lt;/p&gt;
&lt;h3 id=&#34;out-of-tree-providers&#34;&gt;Out-of-tree Providers&lt;/h3&gt;
&lt;p&gt;Out-of-tree providers encapsulate cloud-provider logic in a controller. This
controller is commonly referred to as a cloud-controller-manager (CCM). The CCM
is deployed to your cluster and interacts with the cloud-provider&amp;rsquo;s APIs. The
storage driver (CSI-plugin) often runs outside of the CCM, but can require the
CCM to function correctly. In the case of vSphere, you install 3 components.&lt;/p&gt;





&lt;table class=&#34;table&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Component&lt;/th&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Cloud Controller Manager&lt;/td&gt;
&lt;td&gt;&lt;code&gt;vsphere-cloud-controller&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Deployment&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Storage Controller&lt;/td&gt;
&lt;td&gt;&lt;code&gt;vsphere-csi-controller&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;StatefulSet&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Storage Driver (CSI-plugin)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;vsphere-csi-node&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;DaemonSet&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;With these components installed, a 4 node Kubernetes cluster (assuming 1 master)
would look as follows.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/storage-integration/diagrams/out-of-tree-provider-and-csi.png&#34; alt=&#34;Out-of-tree provider&#34;  /&gt;&lt;/p&gt;
&lt;h3 id=&#34;dedicated-storage-integrations&#34;&gt;Dedicated Storage Integrations&lt;/h3&gt;
&lt;p&gt;Some storage providers have nothing to do with cloud-provider integration. In
this case they run their integrations / drivers as isolated processes (pods in
Kubernetes). An example of this model is NetApp&amp;rsquo;s
&lt;a href=&#34;https://netapp-trident.readthedocs.io/en/stable-v19.10/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Trident&lt;/a&gt; integration.
Running trident in a cluster will provision volumes against its supported
providers, such as NetApp&amp;rsquo;s SolidFire. It also handles concerns around backup
and recovery. Another common project that follows this model is
&lt;a href=&#34;https://rook.io&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;rook&lt;/a&gt;, which provides integration with providers like Ceph and
NFS.
Also, Dell EMC &lt;a href=&#34;https://github.com/dell/karavi&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Container Storage Modules&lt;/a&gt; (formerly known as Karavi)
enriches CSI with enterprise storage capabilities such as Authorization, Resiliency and others.&lt;/p&gt;
&lt;h3 id=&#34;option-considerations&#34;&gt;Option Considerations&lt;/h3&gt;
&lt;p&gt;There is no shortage of CSI-plugin options. For your environment, the decision
may be easy because you only have one type of storage available to you, for
example vSAN. However, if you&amp;rsquo;re thinking about integrating a new storage
provider, it&amp;rsquo;s important that you consider the storage offerings and resiliency
guarantees your platform needs to offer. Some key considerations are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What I/O speeds are required for platform workloads?&lt;/li&gt;
&lt;li&gt;What disaster scenarios does the persistence layer need to handle?&lt;/li&gt;
&lt;li&gt;How many workloads will need persistent storage?
&lt;ul&gt;
&lt;li&gt;How much storage do you need and what are your expansion requirements?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Do you need to offer dynamic volume resizing if a workload uses up its
storage?&lt;/li&gt;
&lt;li&gt;What backup scenarios (if any) do you plan to offer?
&lt;ul&gt;
&lt;li&gt;Alternatively, do you plan to make the application teams responsible for
their backups?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;What storage system can you realistically operate? Ceph? vSAN?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Having conversations around these points will help you determine the best
storage integration for you.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: Backing Up, Restoring, and Migrating Resources with Velero</title>
      
      <link>/guides/kubernetes/what-is-velero/</link>
      <pubDate>Fri, 26 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/guides/kubernetes/what-is-velero/</guid>
      <description>

        
        &lt;p&gt;Velero is an open source tool for safely backing up and restoring resources in a Kubernetes cluster, performing disaster recovery, and migrating resources and persistent volumes to another Kubernetes cluster.&lt;/p&gt;
&lt;p&gt;Velero offers key data protection features, such as scheduled backups, retention schedules, and pre- or post-backup hooks for custom actions. Velero can help protect data stored in persistent volumes and makes your entire Kubernetes cluster more resilient.&lt;/p&gt;
&lt;h1 id=&#34;velero-use-cases&#34;&gt;Velero Use Cases&lt;/h1&gt;
&lt;p&gt;Here are some of the things Velero can do:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Back up your cluster and restore it in case of loss.&lt;/li&gt;
&lt;li&gt;Recover from disaster.&lt;/li&gt;
&lt;li&gt;Copy cluster resources to other clusters.&lt;/li&gt;
&lt;li&gt;Replicate your production environment to create development and testing environments.&lt;/li&gt;
&lt;li&gt;Take a snapshot of your application&amp;rsquo;s state before upgrading a cluster.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;velero-components-and-architecture&#34;&gt;Velero Components and Architecture&lt;/h1&gt;
&lt;p&gt;Velero consists of two main components:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A server that runs on your cluster&lt;/li&gt;
&lt;li&gt;A command-line utility that runs locally&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Velero supports plug-ins to enable it to work with different storage systems and Kubernetes platforms. You can run Velero in clusters on a cloud provider or on premises.&lt;/p&gt;
&lt;h1 id=&#34;how-velero-works&#34;&gt;How Velero Works&lt;/h1&gt;
&lt;p&gt;Each Velero operation&amp;ndash;on-demand backup, scheduled backup, restoration&amp;ndash;is a custom resource that is defined with a Kubernetes custom resource definition, or CRD, and stored in &lt;code&gt;etcd&lt;/code&gt;. Velero includes controllers that process the CRDs to back up and restore resources. You can back up or restore all objects in your cluster, or you can filter objects by type, namespace, or label.&lt;/p&gt;
&lt;p&gt;Data protection is a chief concern for application owners who want to make sure that they can restore a cluster to a known good state, recover from a crashed cluster, or migrate to a new environment. Velero provides those capabilities.&lt;/p&gt;
&lt;h3 id=&#34;keep-learning&#34;&gt;Keep Learning&lt;/h3&gt;
&lt;p&gt;On the &lt;a href=&#34;https://velero.io/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Velero home page&lt;/a&gt; you can get information on the latest release and download Velero from Github.&lt;/p&gt;
&lt;p&gt;To get started using Velero read our guide, &lt;a href=&#34;/guides/kubernetes/velero-gs&#34;&gt;Getting Started with Velero&lt;/a&gt;, and watch these videos covering two of Velero’s useful features, &lt;a href=&#34;https://kube.academy/courses/cluster-operations/lessons/backuprestore&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Backup and Restore&lt;/a&gt;, and &lt;a href=&#34;https://www.youtube.com/watch?v=q2FCxheA8VI&amp;amp;list=PL7bmigfV0EqQRysvqvqOtRNk4L5S7uqwM&amp;amp;index=5&amp;amp;t=0s&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Migration&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: Workload Tenancy</title>
      
      <link>/guides/kubernetes/workload-tenancy/</link>
      <pubDate>Wed, 24 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/guides/kubernetes/workload-tenancy/</guid>
      <description>

        
        &lt;p&gt;Kubernetes is an inherently multi-tenant system. The term tenant can
have many meanings. For the purpose of this page, we consider a workload (e.g.
Kubernetes pod) to be a tenant. In most Kubernetes environments, pods are
scheduled alongside other pods on the same hosts. Kubernetes has features that
provide the illusion of workload boundaries such as namespaces. However, odds
are pods in different namespaces will run together on the same host.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/workload-tenancy/diagrams/k8s-workers-tenancy.png&#34; alt=&#34;Worker Tenancy&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;With multiple tenants running on hosts, there are several concerns to account
for.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Resource isolation between pods.&lt;/li&gt;
&lt;li&gt;Workload scheduling decisions based on resource requests.&lt;/li&gt;
&lt;li&gt;Workload scheduling decisions based on host properties.&lt;/li&gt;
&lt;li&gt;Priority of workloads (e.g. removing workloads for more important ones).&lt;/li&gt;
&lt;li&gt;Preventing cross-workload access to memory or data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Kubernetes provides constructs to ensure workloads run well side-by-side. These
configuration options have significant depth and can be especially challenging
in environments where multiple teams are consumers. Additionally, platform teams
often want to reduce cost by overcommitting resources on nodes. Without these
constructs in place, workloads will run over each other and incur platform
instability.&lt;/p&gt;
&lt;h2 id=&#34;resource-limits-and-requests&#34;&gt;Resource Limits and Requests&lt;/h2&gt;
&lt;p&gt;For resource contention, limits and requests are the most important concepts to
understand. An example snippet of a pod setting a resource limit and request is
as follows.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Pod&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;frontend&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;containers&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;db&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;mysql&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;resources&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;requests&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;memory&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;64Mi&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;cpu&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;250m&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;limits&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;memory&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;128Mi&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;cpu&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;500m&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;limits&#34;&gt;Limits&lt;/h3&gt;
&lt;p&gt;Limits are enforced on the host level. This means that a container can not use
more than its allotted resource limit. For CPU, throttling occurs when a process
exceeds its limit. For memory, a workload is killed when it exceeds its limit.&lt;/p&gt;
&lt;h3 id=&#34;request&#34;&gt;Request&lt;/h3&gt;
&lt;p&gt;Requests are used by the scheduler to make scheduling decisions based on desired
resources and availability. Requests are generally not enforced on the host. For
memory, a pod can use more than its requested amount. If the node comes under
contention, the pod may be killed. For CPU, requests are translated into CPU
shares. This means the pod can use more than its requested CPU, but if the node
comes under contention, CPU may be throttled to only what was requested.&lt;/p&gt;
&lt;h3 id=&#34;quality-of-service&#34;&gt;Quality of Service&lt;/h3&gt;
&lt;p&gt;Based on what is configured above, pods automatically get marked with a specific
quality of service.&lt;/p&gt;





&lt;table class=&#34;table&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;QoS&lt;/th&gt;
&lt;th&gt;Condition&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Best Effort&lt;/td&gt;
&lt;td&gt;No request or limit set&lt;/td&gt;
&lt;td&gt;Pod is scheduled and uses any available resources to the host.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Guaranteed&lt;/td&gt;
&lt;td&gt;Limits == Requests&lt;/td&gt;
&lt;td&gt;The amount of resources a pod is scheduled for equals what its able to consume on the host. If the host comes under contention, the pod could be throttled or killed.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Burstable&lt;/td&gt;
&lt;td&gt;Limits &amp;gt; Requests&lt;/td&gt;
&lt;td&gt;A pod can &amp;ldquo;burst&amp;rdquo; beyond its resource request but not above its limit. If the host comes under contention, the pod could be throttled or killed.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;popular-tooling-and-approaches&#34;&gt;Popular Tooling and Approaches&lt;/h2&gt;
&lt;p&gt;This section covers a multitude of configuration and our recommendations for
each. For an in-depth guide on tuning these parameters, see the &lt;a href=&#34;../workload-tenancy-cluster-tuning&#34;&gt;Cluster Tuning
Guide&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;guaranteed-pods&#34;&gt;Guaranteed Pods&lt;/h3&gt;
&lt;p&gt;Guaranteed pods are created when only &lt;a href=&#34;https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#how-pods-with-resource-limits-are-run&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Resource
limits&lt;/a&gt;
are set. Resource limits are enforced on the host level. CPU limits enforce
throttling when the limit is reached. Memory limits kill the workload when the
limit is exceeded. In the absence of resource requests, the request is
automatically set to that of the limit. This can be a good model as it ensures
the amount of resources the workload is scheduled for is exactly the amount that
will be available on the host (i.e. no overcommitting). This decreases the
complexity of workloads. The downside to only using limits is you cannot
overcommit resources. An example of overcommitting is where you set a limit
higher than a request, which means the container can use more than is requested,
unless the host comes under contention.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/workload-tenancy/guaranteed-pods.png&#34; alt=&#34;Guaranteed Pods&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Improved stability
&lt;ul&gt;
&lt;li&gt;Eliminates resource contention between workloads&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Reduced infra utilization
&lt;ul&gt;
&lt;li&gt;May result in underutilized compute resources&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Can lead to CPU throttling and OOM kills
&lt;ul&gt;
&lt;li&gt;Important to understand resource consumption profile of workloads&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;burstable-pods&#34;&gt;Burstable Pods&lt;/h3&gt;
&lt;p&gt;An advanced use case of cluster resource allocation is to allow pods to burst
beyond their resource requests and provide a high upper bounds. This enables
nodes to be overcommitted, where more workloads are scheduled than could be
handled should they all consume 100% of their CPU and memory resource requests.
This model can be complex to implement correctly, but also can be the most cost
effective.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/workload-tenancy/burstable-pod.png&#34; alt=&#34;Burstable Pods&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;When a host comes under contention, CPU is throttled back to the CPU shares
allocated via the resource request. Once resources free up, containers are able
to consume the unused resources again.&lt;/p&gt;
&lt;p&gt;We recommend you &lt;strong&gt;do not&lt;/strong&gt; overcommit memory. Overcommitting memory can cause
instability in workloads as they are killed when the host comes under resource
contention.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Possibly improved infra resource utilization&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Potential for resource contention&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pod-disruption-budgets&#34;&gt;Pod Disruption Budgets&lt;/h3&gt;
&lt;p&gt;During the life of a Kubernetes cluster, there are many voluntary events, which
can impact workloads. An example would be draining specific nodes to perform
maintenance. &lt;code&gt;PodDisruptionBudget&lt;/code&gt;s (PDB) allow you to set a minimum number of
instances available for your application to ensure Kubernetes blocks before
continuing with an operation that would break that PDB.&lt;/p&gt;
&lt;p&gt;For any workload that requires a minimum number of instances running, this is
recommended.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Improved stability through availability guarantees&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Potential to stall upgrades
&lt;ul&gt;
&lt;li&gt;If cluster resources are under contention, an upgrade may stall to respect
the disruption budget&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;limit-ranges&#34;&gt;Limit Ranges&lt;/h3&gt;
&lt;p&gt;A &lt;code&gt;LimitRange&lt;/code&gt; sets the maximum or minimum resource limits and requests a pod
can be declared to use. It also allows for configuring default values, should a
pod be submitted without these setting in place. The &lt;code&gt;LimitRange&lt;/code&gt; is namespace
scoped and recommended for administrators to enforce a sensible default and
limit per-pod.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Provides &amp;ldquo;guardrails&amp;rdquo; for development teams&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Possibly confusing for tenants that don&amp;rsquo;t know how resource values are being set&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;resource-quotas&#34;&gt;Resource Quotas&lt;/h3&gt;
&lt;p&gt;A &lt;code&gt;ResourceQuota&lt;/code&gt; enables an administrator to set the aggregate resources
available to a given namespace. It also enables the administrator to limit the
number of objects that can be created within the namespace, which is referred to
as object count quota.&lt;/p&gt;
&lt;p&gt;Below are some of the resources that can be placed under object count quota:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;count/persistentvolumeclaims&lt;/li&gt;
&lt;li&gt;count/services&lt;/li&gt;
&lt;li&gt;count/secrets&lt;/li&gt;
&lt;li&gt;count/configmaps&lt;/li&gt;
&lt;li&gt;count/replicationcontrollers&lt;/li&gt;
&lt;li&gt;count/deployments.apps&lt;/li&gt;
&lt;li&gt;count/replicasets.apps&lt;/li&gt;
&lt;li&gt;count/statefulsets.apps&lt;/li&gt;
&lt;li&gt;count/jobs.batch&lt;/li&gt;
&lt;li&gt;count/cronjobs.batch&lt;/li&gt;
&lt;li&gt;count/deployments.extensions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For multi-team environments, these constraints can be beneficial to enforce.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Provides controls over resource consumption&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Can be over-restrictive in single-tenant environments&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: Create a Multi-Cluster Monitoring Dashboard with Thanos, Grafana and Prometheus</title>
      
      <link>/guides/kubernetes/prometheus-multicluster-monitoring/</link>
      <pubDate>Wed, 11 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/guides/kubernetes/prometheus-multicluster-monitoring/</guid>
      <description>

        
        &lt;p&gt;&lt;a href=&#34;https://prometheus.io/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Prometheus&lt;/a&gt;, coupled with
&lt;a href=&#34;https://grafana.com/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Grafana&lt;/a&gt;, is a popular monitoring solution for Kubernetes
clusters. It allows SRE teams and developers to capture metrics and telemetry
data for applications running in a cluster, allowing deeper insights into
application performance and reliability.&lt;/p&gt;
&lt;p&gt;The Prometheus/Grafana combination works well for individual clusters, but as
teams scale out and start working with multiple clusters, monitoring
requirements become correspondingly more complex. For effective multi-cluster
monitoring, a &amp;ldquo;single pane of glass&amp;rdquo; with centralized real-time monitoring, time
series comparisons across and within clusters and high availability is essential
for teams operating with multiple clusters and multiple providers.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://thanos.io/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Thanos&lt;/a&gt; is a monitoring system that aggregates data from
multiple Prometheus deployments. This data can then be inspected and analyzed
using Grafana, just as with regular Prometheus metrics. Although this setup
sounds complex, it&amp;rsquo;s actually very easy to achieve with the following Bitnami
Helm charts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bitnami/charts/tree/master/bitnami/prometheus-operator&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami&amp;rsquo;s Prometheus Operator Helm chart&lt;/a&gt;
lets you deploy Prometheus in your Kubernetes cluster with an additional
Thanos sidecar container.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bitnami/charts/tree/master/bitnami/thanos&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami&amp;rsquo;s Thanos Helm chart&lt;/a&gt;
lets you deploy all the Thanos components together with MinIO and Alertmanager
so you can quickly bootstrap a Thanos deployment.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bitnami/charts/tree/master/bitnami/grafana&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami&amp;rsquo;s Grafana Helm chart&lt;/a&gt;
lets you deploy Grafana in your Kubernetes cluster.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This guide walks you through the process of using these charts to create a
Thanos deployment that aggregates data from Prometheus Operators in multiple
clusters and allows further monitoring and analysis using Grafana.&lt;/p&gt;
&lt;h2 id=&#34;assumptions-and-prerequisites&#34;&gt;Assumptions and prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;You have three separate multi-node Kubernetes clusters running on the same
cloud provider:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Two &amp;ldquo;data producer&amp;rdquo; clusters which will host Prometheus deployments and
applications that expose metrics via Prometheus.&lt;/li&gt;
&lt;li&gt;One &amp;ldquo;data aggregator&amp;rdquo; cluster which will host Thanos and aggregate the data
from the data producers. This cluster will also host Grafana for data
visualization and reporting.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You have the &lt;em&gt;kubectl&lt;/em&gt; CLI and the Helm v3.x package manager installed and configured to work with your Kubernetes clusters. &lt;a href=&#34;https://docs.bitnami.com/kubernetes/get-started-kubernetes#step-3-install-kubectl-command-line&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Learn how to install &lt;em&gt;kubectl&lt;/em&gt; and Helm v3.x&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This guide uses clusters hosted on the Google Kubernetes Engine (GKE) service
but you can use any Kubernetes provider. Learn about
&lt;a href=&#34;https://docs.bitnami.com/kubernetes/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;deploying a Kubernetes cluster on different cloud platforms&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;step-1-install-the-prometheus-operator-on-each-cluster&#34;&gt;Step 1: Install the Prometheus Operator on each cluster&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/bitnami/charts/tree/master/bitnami/prometheus-operator&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami&amp;rsquo;s Prometheus Operator chart&lt;/a&gt;
provides easy monitoring definitions for Kubernetes services and management of
Prometheus instances. It also includes an optional Thanos sidecar container,
which can be used by your Thanos deployment to access cluster metrics.&lt;/p&gt;
&lt;p&gt;Only one instance of the Prometheus Operator component should be running in a
cluster.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Add the Bitnami charts repository to Helm:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;helm repo add bitnami https://charts.bitnami.com/bitnami
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install the Prometheus Operator in the first &amp;ldquo;data producer&amp;rdquo; cluster using the command below:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;helm install prometheus-operator &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --set prometheus.thanos.create&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;true&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --set operator.service.type&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;ClusterIP &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --set prometheus.service.type&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;ClusterIP &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --set alertmanager.service.type&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;ClusterIP &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --set prometheus.thanos.service.type&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;LoadBalancer &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --set prometheus.externalLabels.cluster&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;data-producer-0&amp;#34;&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  bitnami/prometheus-operator
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The &lt;em&gt;prometheus.thanos.create&lt;/em&gt; parameter creates a Thanos sidecar container,
while the &lt;em&gt;prometheus.thanos.service.type&lt;/em&gt; parameter makes the sidecar service
available at a public load balancer IP address. Note the
&lt;em&gt;prometheus.externalLabels&lt;/em&gt; parameter which lets you define one or more unique
labels per Prometheus instance - these labels are useful to differentiate
different stores or data sources in Thanos.&lt;/p&gt;
&lt;p&gt;The command above exposes the Thanos sidecar container in each cluster at a
public IP address using a &lt;em&gt;LoadBalancer&lt;/em&gt; service. This makes it easy for
Thanos to access Prometheus metrics in different clusters without needing any
special firewall or routing configuration. However, this approach is highly
insecure and should be used only for demonstration or testing purposes. In
production environments, it is preferable to deploy an NGINX Ingress
Controller to control access from outside the cluster and further limit access
using whitelisting and other security-related configuration.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the command below to obtain the public IP address of the sidecar service.
You will use this IP address in the next step.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl get svc | grep prometheus-operator-prometheus-thanos
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Repeat the steps shown above for the second &amp;ldquo;data producer&amp;rdquo; cluster. Use a
different value for the &lt;em&gt;prometheus.externalLabels.cluster&lt;/em&gt; parameter, such as
&lt;em&gt;data-producer-1&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&#34;step-2-install-and-configure-thanos&#34;&gt;Step 2: Install and configure Thanos&lt;/h2&gt;
&lt;p&gt;The next step is to install Thanos in the &amp;ldquo;data aggregator&amp;rdquo; cluster and
integrate it with Alertmanager and MinIO as the object store.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Modify your Kubernetes context to reflect the cluster on which you wish to install Thanos.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create a &lt;em&gt;values.yaml&lt;/em&gt; file as shown below. Replace the KEY placeholder with a
hard-to-guess value and the SIDECAR-SERVICE-IP-ADDRESS-X placeholders with the
public IP addresses of the Thanos sidecar containers in the &amp;ldquo;data producer&amp;rdquo;
clusters.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;objstoreConfig&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;|-&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;  type: s3
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;  config:
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;    bucket: thanos
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;    endpoint: {{ include &amp;#34;thanos.minio.fullname&amp;#34; . }}.monitoring.svc.cluster.local:9000
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;    access_key: minio
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;    secret_key: KEY
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;    insecure: true&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;querier&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;stores&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;SIDECAR-SERVICE-IP-ADDRESS-1:10901&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;SIDECAR-SERVICE-IP-ADDRESS-2:10901&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;bucketweb&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;enabled&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;true&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;compactor&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;enabled&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;true&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;storegateway&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;enabled&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;true&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;ruler&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;enabled&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;true&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;alertmanagers&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;http://prometheus-operator-alertmanager.monitoring.svc.cluster.local:9093&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;config&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;|-&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;    groups:
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;      - name: &amp;#34;metamonitoring&amp;#34;
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;        rules:
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;          - alert: &amp;#34;PrometheusDown&amp;#34;
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;            expr: absent(up{prometheus=&amp;#34;monitoring/prometheus-operator&amp;#34;})&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;minio&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;enabled&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;true&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;accessKey&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;password&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;minio&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;secretKey&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;password&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;KEY&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;defaultBuckets&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;thanos&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install Thanos using the command below:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;helm install thanos bitnami/thanos &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --values values.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Wait for the deployment to complete and note the DNS name and port number for
the Thanos Querier service in the deployment output, as shown below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/prometheus-multicluster-monitoring/querier-service.png&#34; alt=&#34;Thanos Querier service&#34;  /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Follow the instructions shown in the chart output to connect to the Thanos
Querier Web interface and navigate to the &amp;ldquo;Stores&amp;rdquo; tab. Confirm that both
sidecar services are running and registered with Thanos, as shown below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/prometheus-multicluster-monitoring/querier-stores.png&#34; alt=&#34;Thanos Querier stores&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Confirm also that each service displays a unique &lt;em&gt;cluster&lt;/em&gt; labelset, as configured in &lt;a href=&#34;#step-1-install-the-prometheus-operator-on-each-cluster&#34;&gt;Step 1&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;step-3-install-grafana&#34;&gt;Step 3: Install Grafana&lt;/h2&gt;
&lt;p&gt;The next step is to install Grafana, also on the same &amp;ldquo;data aggregator&amp;rdquo; cluster
as Thanos.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Use the command below, replacing GRAFANA-PASSWORD with a password for the
Grafana application:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;helm install grafana bitnami/grafana &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --set service.type&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;LoadBalancer &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --set admin.password&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;GRAFANA-PASSWORD
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Wait for the deployment to complete and obtain the public IP address for the
Grafana load balancer service:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl get svc &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; grep grafana
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Confirm that you are able to access Grafana by browsing to the load balancer
IP address on port 3000 and logging in with the username &lt;em&gt;admin&lt;/em&gt; and the
configured password. Here is what you should see:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/prometheus-multicluster-monitoring/grafana-dashboard.png&#34; alt=&#34;Grafana dashboard&#34;  /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;step-4-configure-grafana-to-use-thanos-as-a-data-source&#34;&gt;Step 4: Configure Grafana to use Thanos as a data source&lt;/h2&gt;
&lt;p&gt;Follow these steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;From the Grafana dashboard, click the &amp;ldquo;Add data source&amp;rdquo; button.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;On the &amp;ldquo;Choose data source type&amp;rdquo; page, select &amp;ldquo;Prometheus&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/prometheus-multicluster-monitoring/grafana-add-data-source.png&#34; alt=&#34;Grafana data source&#34;  /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;On the &amp;ldquo;Settings&amp;rdquo; page, set the URL for the Prometheus server to
&lt;em&gt;http://NAME:PORT&lt;/em&gt;, where NAME is the DNS name for the Thanos service obtained
at the end of &lt;a href=&#34;#step-2-install-and-configure-thanos&#34;&gt;Step 2&lt;/a&gt; and PORT is the
corresponding service port. Leave all other values at their default.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/prometheus-multicluster-monitoring/grafana-thanos-url.png&#34; alt=&#34;Grafana data source configuration&#34;  /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Click &amp;ldquo;Save &amp;amp; Test&amp;rdquo; to save and test the configuration. If everything is
configured correctly, you should see a success message like the one below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/prometheus-multicluster-monitoring/grafana-success.png&#34; alt=&#34;Grafana test&#34;  /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;step-5-test-the-system&#34;&gt;Step 5: Test the system&lt;/h2&gt;
&lt;p&gt;At this point, you can start deploying applications into your &amp;ldquo;data producer&amp;rdquo;
clusters and collating the metrics in Thanos and Grafana. For demonstration
purposes, this guide will deploy a MariaDB replication cluster using Bitnami&amp;rsquo;s
MariaDB Helm chart in each &amp;ldquo;data producer&amp;rdquo; cluster and display the metrics
generated by each MariaDB service in Grafana.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Deploy MariaDB in each cluster with one master and one slave using the
production configuration with the commands below. Replace the
MARIADB-ADMIN-PASSWORD and MARIADB-REPL-PASSWORD placeholders with the
database administrator account and replication account password respectively.
You can also optionally create a MariaDB user account for application use by
specifying values for the USER-PASSWORD, USER-NAME and DB-NAME placeholders.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;helm install mariadb &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --set rootUser.password&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;MARIADB-ADMIN-PASSWORD &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --set replication.password&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;MARIADB-REPL-PASSWORD &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --set db.user&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;USER-NAME &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --set db.password&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;USER-PASSWORD &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --set db.name&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;DB-NAME &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --set slave.replicas&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --set metrics.enabled&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;true&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --set metrics.serviceMonitor.enabled&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;true&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  bitnami/mariadb 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Note the &lt;em&gt;metrics.enabled&lt;/em&gt; parameter, which enables the Prometheus exporter
for MySQL server metrics, and the &lt;em&gt;metrics.serviceMonitor.enabled&lt;/em&gt; parameter,
which creates a Prometheus Operator ServiceMonitor.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Once deployment in each cluster is complete, note the instructions to connect
to each database service.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/prometheus-multicluster-monitoring/mariadb-service.png&#34; alt=&#34;MariaDB service&#34;  /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Browse to the
&lt;a href=&#34;https://github.com/percona/grafana-dashboards/blob/master/dashboards/MySQL_Overview.json&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;MySQL Overview dashboard in the Percona GitHub repository&lt;/a&gt;
and copy the JSON model.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Log in to Grafana. From the Grafana dashboard, click the &amp;ldquo;Import -&amp;gt; Dashboard&amp;rdquo;
menu item.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;On the &amp;ldquo;Import&amp;rdquo; page, paste the JSON model into the &amp;ldquo;Or paste JSON&amp;rdquo; field.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/prometheus-multicluster-monitoring/grafana-dashboard-import.png&#34; alt=&#34;Grafana dashboard import&#34;  /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Click &amp;ldquo;Load&amp;rdquo; to load the data and then &amp;ldquo;Import&amp;rdquo; to import the dashboard. The
new dashboard should appear in Grafana, as shown below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/prometheus-multicluster-monitoring/grafana-mysql-dashboard.png&#34; alt=&#34;Grafana MySQL dashboard&#34;  /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Connect to the MariaDB service in the first &amp;ldquo;data producer&amp;rdquo; cluster and
perform some actions, such as creating a database, adding records to a table
and executing a query. Perform similar actions in the second &amp;ldquo;data producer&amp;rdquo;
cluster. You should see your activity in each cluster reflected in the MySQL
Overview chart in Grafana, as shown below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/prometheus-multicluster-monitoring/grafana-metrics.png&#34; alt=&#34;MariaDB metrics in Grafana&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;You can view metrics from individual master and slave nodes in each cluster by
selecting a different host in the &amp;ldquo;Host&amp;rdquo; drop down of the dashboard, as shown
below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/prometheus-multicluster-monitoring/grafana-hosts.png&#34; alt=&#34;MariaDB hosts in Grafana&#34;  /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can now continue adding more applications to your clusters. So long as you
enable Prometheus metrics and a Prometheus Operator ServiceMonitor for each
deployment, Thanos will continuously receive and aggregate the metrics and you
can inspect them using Grafana.&lt;/p&gt;
&lt;h2 id=&#34;useful-links&#34;&gt;Useful links&lt;/h2&gt;
&lt;p&gt;To learn more about the topics discussed in this guide, use the links below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bitnami/charts/tree/master/bitnami/prometheus-operator&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami Prometheus Operator Helm chart&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bitnami/charts/tree/master/bitnami/thanos&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami&amp;rsquo;s Thanos Helm chart&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bitnami/charts/tree/master/bitnami/grafana&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami&amp;rsquo;s Grafana Helm chart&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bitnami/charts/tree/master/bitnami/mariadb&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami MariaDB Helm chart&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/ingress/tree/master/controllers/nginx&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;NGINX Ingress controller documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.bitnami.com/tutorials/secure-kubernetes-services-with-ingress-tls-letsencrypt/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Secure Kubernetes Services with Ingress, TLS and Let&amp;rsquo;s Encrypt&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: Assign Pods to Nodes With Bitnami Helm Chart Affinity Rules</title>
      
      <link>/guides/kubernetes/assign-pods-to-nodes-with-bitnami-helm-chart-affinity-rules/</link>
      <pubDate>Tue, 13 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>/guides/kubernetes/assign-pods-to-nodes-with-bitnami-helm-chart-affinity-rules/</guid>
      <description>

        
        &lt;p&gt;First published on &lt;a href=&#34;https://docs.bitnami.com/tutorials/assign-pod-nodes-helm-affinity-rules/&#34;&gt;https://docs.bitnami.com/tutorials/assign-pod-nodes-helm-affinity-rules/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;When you install an application in a Kubernetes cluster, the &lt;a href=&#34;https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Kubernetes scheduler&lt;/a&gt; decides in which nodes the application pods will be installed unless certain constraints are defined. For example, Kubernetes scheduler may decide to install application pods in a node with more available memory. This is mostly useful except when cluster administrators prefer to distribute a group of pods across the cluster in a specific manner. For this use case, they need a tool that can force Kubernetes to follow custom rules specified by the user.&lt;/p&gt;
&lt;p&gt;Affinity rules supply a way to force the scheduler to follow specific rules that determine where pods should be distributed. To help users to implement affinity rules, Bitnami has enhanced its Helm charts by including opinionated affinities in their manifest files. Cluster administrators now only need to define the criteria to be followed by the scheduler when placing application pods in cluster nodes. They can then enable this feature via a simple install-time parameter&lt;/p&gt;
&lt;p&gt;This tutorial will demonstrate the available affinity rules and how they can be adapted to your needs.&lt;/p&gt;
&lt;h2 id=&#34;assumptions-and-prerequisites&#34;&gt;Assumptions and Prerequisites&lt;/h2&gt;
&lt;p&gt;This article assumes that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You have a Google Cloud account. &lt;a href=&#34;https://cloud.google.com/free&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Register for a Google Cloud account&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;You have a Kubernetes cluster running with Helm v3.x and &lt;code&gt;kubectl&lt;/code&gt; installed. &lt;a href=&#34;https://docs.bitnami.com/kubernetes/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Learn more about getting started with Kubernetes and Helm using different cloud providers&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&#34;callout td-box--gray-darkest p-3 mx-5 border-bottom border-right border-left border-top&#34;&gt;
    &lt;p&gt;This guide uses a Kubernetes cluster created in GKE. These steps are the same for all Kubernetes engines. They don’t work, however, in Minikube, since with Minikube you only can create single-node clusters.&lt;/p&gt;
&lt;/div&gt;

&lt;h2 id=&#34;how-affinity-rules-work-in-bitnami-helm-charts&#34;&gt;How Affinity Rules Work in Bitnami Helm Charts&lt;/h2&gt;
&lt;p&gt;All Bitnami infrastructure solutions available in the &lt;a href=&#34;https://github.com/bitnami/charts/tree/master/bitnami&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami Helm charts catalog&lt;/a&gt; now include pre-defined affinity rules exposed through the &lt;code&gt;podAffinityPreset&lt;/code&gt; and &lt;code&gt;podAntiAffinitypreset&lt;/code&gt; parameters in their &lt;code&gt;values.yml&lt;/code&gt; file:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/bitnami-helm-chart-affinity-rules/image-1.png&#34; alt=&#34;Pod affinity rules&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Pod affinity and anti-affinity rules allow you to define how the scheduler should behave when locating application pods in your cluster eligible nodes. Depending on the option you choose, the scheduler will behave as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;podAffinityPreset&lt;/code&gt; - Using the &lt;code&gt;podAffinity&lt;/code&gt; rule, the scheduler will locate a new pod on the same node where other pods with the same label are located. This approach is especially helpful to group under the same node pods that meet specific pre-defined patterns.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;podAntiAffinitypreset&lt;/code&gt; - Using the &lt;code&gt;podAntiAffinity&lt;/code&gt; parameter lets the scheduler locates one pod in each node. Thus, you will prevent locating a new pod on the same node as other pods are running. This option is convenient if your deployment will demand high availability.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Having the pods distributed across all nodes allows Kubernetes to ensure high availability of your cluster by keeping running the remaining nodes when one node fails.&lt;/p&gt;
&lt;p&gt;These are the values you can set for both pod affinity and anti-affinity rules:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Soft&lt;/strong&gt; - Use this value to make the scheduler enforce a rule wherever it can be met (best-effort approach). If the rule cannot be met, the scheduler will deploy the required pods in the nodes with enough resources.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hard&lt;/strong&gt; - Use this value to make the scheduler enforce a rule. This means that if there are remaining pods that do not comply with the pre-defined rule, they won&amp;rsquo;t be allocated in any node.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Bitnami Helm charts have the &lt;code&gt;podAntiAffinity&lt;/code&gt; rule with the &lt;code&gt;soft&lt;/code&gt; value enabled by default. Hence, if there are not enough nodes to place one pod per node, it will leave the scheduler to decide where the remaining pods should be located.&lt;/p&gt;
&lt;p&gt;The following section shows two different use cases of configuring &lt;code&gt;podAntiaffinity&lt;/code&gt; parameter.&lt;/p&gt;
&lt;h2 id=&#34;deploying-a-chart-using-the-podantiaffinity-rule&#34;&gt;Deploying a Chart Using the &lt;code&gt;podAntiAffinity&lt;/code&gt; Rule&lt;/h2&gt;
&lt;p&gt;The following examples illustrate how the &lt;code&gt;podAntiAffinity&lt;/code&gt; rule works in the context of the Bitnami MySQL Helm chart. They cover two use cases: installing the chart with the default &lt;code&gt;podAntiAffinity&lt;/code&gt; value and changing the &lt;code&gt;podAntiAffinity&lt;/code&gt; value from &lt;code&gt;soft&lt;/code&gt; to &lt;code&gt;hard&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;use-case-1-install-the-chart-with-the-default-podantiaffinity-value&#34;&gt;Use Case 1: Install the Chart with the Default &lt;code&gt;podAntiaffinity&lt;/code&gt; Value&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Install the Bitnami Helm charts repository by running:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;helm repo add bitnami https://charts.bitnami.com/bitnami 
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;Deploy the MySQL Helm chart by executing the command below. Note that the chart will deploy the cluster with three nodes and two replicas - one primary and one secondary. To make the scheduler follow the default &lt;code&gt;podAntiAffinity&lt;/code&gt; rule, set the parameter as follows:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;helm install mysql bitnami/mysql --set architecture=replication --set secondary.replicaCount=2 --set secondary.podAntiAffinityPreset=soft 
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;Verify the cluster by checking the nodes. Use the following command to list the connected nodes:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;kubectl get nodes 
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;You will see an output message like this:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/bitnami-helm-chart-affinity-rules/image-2.png&#34; alt=&#34;Example kubectl get pods output&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Three nodes are running in the cluster.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Check how the pods are distributed. Execute the command below:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;kubectl get pods -o wide 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/bitnami-helm-chart-affinity-rules/image-3.png&#34; alt=&#34;Example kubectl get pods -o wide output&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;As expected, both the primary and the secondary pods are in different nodes.&lt;/p&gt;
&lt;p&gt;To verify how the scheduler acts when the &lt;em&gt;soft&lt;/em&gt; value is defined, scale up the cluster by setting the number of secondary replicas to three instead of one. Thus, the resulting number of pods will be four, instead of two.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To scale the cluster, use the command below:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;kubectl scale sts/mysql-secondary --replicas 3 
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;Check the pods by running again the &lt;code&gt;kubectl get pods&lt;/code&gt; command. The &lt;code&gt;soft&lt;/code&gt; value left the scheduler to locate the remaining pod that didn&amp;rsquo;t comply with the &amp;ldquo;one pod per node&amp;rdquo; rule:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/bitnami-helm-chart-affinity-rules/image-4.png&#34; alt=&#34;Example kubectl get pods output&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Note that two pods are running in the same node.&lt;/p&gt;
&lt;h3 id=&#34;use-case-2-change-the-podantiaffinity-value-from-soft-to-hard&#34;&gt;Use Case 2: Change the &lt;code&gt;podAntiAffinity&lt;/code&gt; Value from Soft to Hard&lt;/h3&gt;
&lt;p&gt;To try the &lt;code&gt;hard&lt;/code&gt; type of the &lt;code&gt;podAntiAffinity&lt;/code&gt; rule, deploy the chart again by changing the &lt;code&gt;secondary.podAntiAffinityPreset&lt;/code&gt; value from &lt;code&gt;soft&lt;/code&gt; to &lt;code&gt;hard&lt;/code&gt; as shown below. The chart will deploy the cluster with three nodes and two replicas - one primary and one secondary.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;helm install mysql-hard bitnami/mysql --set architecture=replication --set secondary.replicaCount=2 --set secondary.podAntiAffinityPreset=hard
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;Check the nodes and the pods by running the &lt;code&gt;kubectl get nodes&lt;/code&gt; and the &lt;code&gt;kubectl get pods –o wide&lt;/code&gt; commands:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/bitnami-helm-chart-affinity-rules/image-5.png&#34; alt=&#34;Example kubectl get pods -o wide output&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Both the primary and secondary pods are running in the same node.&lt;/p&gt;
&lt;p&gt;To verify how the scheduler acts when the &lt;code&gt;hard&lt;/code&gt; value is defined, scale up the cluster by setting the number of secondary replicas to three instead of one. Thus, the resulting number of pods will be four, instead of two.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Scale up the cluster by executing the command below:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;kubectl scale sts/mysql-hard secondary --replicas 3 
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;When checking the pods, you will see that the scheduler has ignored the &amp;ldquo;one pod per node&amp;rdquo; rule and also located only as many pods as there are nodes. The fourth pod was not deployed as there are only three nodes available.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/bitnami-helm-chart-affinity-rules/image-6.png&#34; alt=&#34;Example kubectl get pods -o wide output&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;podAntiAffinity&lt;/code&gt; rule is an easy way to control how application pods will be distributed across the cluster nodes when installing a Helm chart. Deploy your favorite Bitnami applications and enable this feature via a simple install-time parameter.&lt;/p&gt;
&lt;h2 id=&#34;useful-links&#34;&gt;Useful Links&lt;/h2&gt;
&lt;p&gt;To learn more about the topics discussed in this article, use the links below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bitnami/charts&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami Helm charts catalog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.bitnami.com/kubernetes/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami Helm charts documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Kubernetes scheduler documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Kubernetes pod affinity documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      
      <title>Workshops: SpringOne Tour DevOps</title>
      
      <link>/workshops/lab-springone-tour-devops/</link>
      <pubDate>Fri, 09 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>/workshops/lab-springone-tour-devops/</guid>
      <description>

        
        &lt;p&gt;During this workshop you will automate the testing and deployment of a Spring Boot app and its backing database to Kubernetes, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Run the app locally to understand its behavior&lt;/li&gt;
&lt;li&gt;Review embedded testing
&lt;ul&gt;
&lt;li&gt;Unit testing&lt;/li&gt;
&lt;li&gt;Integration testing for APIs using contracts&lt;/li&gt;
&lt;li&gt;Integration testing for databases using Flyway and Testcontainers&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Automate testing using GitHub Actions&lt;/li&gt;
&lt;li&gt;Automate container builds using kpack&lt;/li&gt;
&lt;li&gt;Automate deployment to Kubernetes using GitOps and ArgoCD&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      
      <title>Blog: Kubernetes Operators: Should You Use Them?</title>
      
      <link>/blog/kubernetes-operators-should-you-use-them/</link>
      <pubDate>Fri, 02 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/kubernetes-operators-should-you-use-them/</guid>
      <description>

        
        &lt;p&gt;Kubernetes is the leading trendsetter in the future of autonomous software, having made it possible for companies throughout the world to experience a tremendous reduction in human toil when it comes to all types of software management and deployment.&lt;/p&gt;
&lt;p&gt;Kubernetes has a reputation for being a complex software system with high startup costs and an intense learning curve, yet it remains steadfastly popular among companies that made the initial investment and immediately started reaping the benefits of improved efficiency and effectiveness in delivering automated, on-demand software that accelerates time-to-value.&lt;/p&gt;
&lt;p&gt;Many of the companies that made the lucrative decision to choose Kubernetes as a distributed software system to manage their applications quickly recognized the value of expanding the power of their Kubernetes ecosystem through Kubernetes operators that reduce operational toil in platform services and tenant workloads.&lt;/p&gt;
&lt;h2 id=&#34;reduce-operational-toil-with-kubernetes-operators&#34;&gt;Reduce operational toil with Kubernetes operators&lt;/h2&gt;
&lt;p&gt;You can leverage Kubernetes operators to accomplish all types of automated tasks, including software deployments, management, troubleshooting and updates through custom resources to define the state of the system, and custom controllers to reconcile the existing state of the system with the desired state of the system defined in the custom resource.&lt;/p&gt;
&lt;p&gt;There’s also an impressive assortment of Kubernetes operators developed by contributing members of the Kubernetes community that are freely available. For more information, see &lt;a href=&#34;https://operatorhub.io/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;OperatorHub&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Kubernetes operators use level-triggered logic to bring about the desired state that you defined in custom resources by continually looping through the system until it is able to reconcile the desired state. A Kubernetes operator never quits working until it completes its directive. For example, when a Kubernetes operator encounters an unexpected condition, it doesn’t crash or fail, but stays-on-course, continually looping until the condition is remedied and it is able to successfully accomplish the desired state.&lt;/p&gt;
&lt;h2 id=&#34;is-the-upfront-engineering-effort-to-learn-kubernetes-worth-the-cost&#34;&gt;Is the upfront engineering effort to learn kubernetes worth the cost?&lt;/h2&gt;
&lt;p&gt;The upfront cost for building Kubernetes operators is the engineering effort that it’s going to take to learn Kubernetes and to build the software that will extend its control plane. The cost for the engineering effort is going to vary, depending on the Kubernetes experience of the engineers and your company’s use-cases. This will determine the number of engineering hours required to get an operator into production. The important thing to remember is your company is going to start saving money from a reduction in operational toil the moment the Kubernetes operator starts managing your software.&lt;/p&gt;
&lt;p&gt;The key to justify the upfront cost is to compare the engineering effort against the savings your company will immediately incur from the reduction in end-user operational toil. One way to find this out is to apply Kubernetes engineering principles to your use-cases. If the outcomes show a considerable reduction in operational toil from your end-users, the decision to invest in the upfront engineering effort is a wise one.&lt;/p&gt;
&lt;h3 id=&#34;alternative-solutions&#34;&gt;Alternative solutions&lt;/h3&gt;
&lt;p&gt;There are use-cases where a company does not need the immense power of a Kubernetes operator, and where the upfront engineering effort to learn Kubernetes is not worth the investment. An example would be if your company has a small number of workloads to deploy. In this case, it would be more efficient for you to use a template or an overlay because the barrier to user entry for both is very low. Do not incur the overhead of complexity where it is not justified by business requirements.&lt;/p&gt;
&lt;h2 id=&#34;use-case-examples&#34;&gt;Use-case examples&lt;/h2&gt;
&lt;p&gt;The following are examples of use-cases that show the benefits of building Kubernetes operators.&lt;/p&gt;
&lt;h3 id=&#34;platform-services&#34;&gt;Platform services&lt;/h3&gt;
&lt;p&gt;If your company uses platform services to manage cluster add-ons like ingress and monitoring, or applications to manage company and community supported workloads, the upfront cost of building Kubernetes operators is going to yield an immediate, on-going, long-term investment.&lt;/p&gt;
&lt;p&gt;Kubernetes operators are often leveraged in production to run various platform service cluster add-ons. Often, you can make use of community-supported projects like the Prometheus Operator for app metrics or Cert-Manager to help manage Transport Layer Security (TLS) assets.&lt;/p&gt;
&lt;p&gt;Using Kubernetes operators to fulfill platform services usually offers the most efficient and resilient solution since it is native to the Kubernetes substrate and uses the same engineering principles that make the system stable, self-healing and extendable.&lt;/p&gt;
&lt;h3 id=&#34;tenant-workload&#34;&gt;Tenant workload&lt;/h3&gt;
&lt;p&gt;Tenant workloads with the largest teams and the most involved architecture will benefit most from Kubernetes operators. You’ll see immediate benefits once you identify the tenant workloads where the most engineering time is spent in routine toil around deployment and maintenance that Kubernetes operators can autonomously take over for your end-users.&lt;/p&gt;
&lt;h3 id=&#34;in-house-software&#34;&gt;In-house software&lt;/h3&gt;
&lt;p&gt;There is no limit as to the amount or type of software Kubernetes operators can manage for your business. This includes in-house software that Kubernetes operators autonomously control, freeing-up time for your developers to focus on website updates and fixing existing issues. In-house software is any application developed by your company, for your company. Examples of in-house software include a billing app for an online store, a customer database for an emergency alert system, an inventory management software for an online pharmacy, and an online scheduling tool for a hospital. Any online business that provides Software as a Service (SaaS) can manage their business more efficiently and reliably with Kubernetes operators.&lt;/p&gt;
&lt;h2 id=&#34;theres-a-proper-way-to-build-kubernetes-operators&#34;&gt;There’s a proper way to build Kubernetes operators&lt;/h2&gt;
&lt;p&gt;The articles that you read about Kubernetes operators being hard to build are true. That’s why Kubernetes training and experience are essential. If you try to build one without understanding Kubernetes, you’re more likely to create problems that diminish the instant time-to-value savings that your company would have realized had the Kubernetes operator been built properly on Kubernetes engineering principles.&lt;/p&gt;
&lt;h2 id=&#34;affordable-kubernetes-training&#34;&gt;Affordable Kubernetes training&lt;/h2&gt;
&lt;p&gt;If the cost of Kubernetes training is a concern for your company, there is a viable, and fun solution. One of the more popular, affordable ways to learn how to build Kubernetes operators is to join the &lt;a href=&#34;https://kubernetes.io/community/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Kubernetes community&lt;/a&gt;, where you’ll find a myriad of support from new and experienced users and contributors who share advice, use-cases and bug fixes that make the complexity of learning Kubernetes a lot less intimidating, and much more manageable.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      
      <title>Blog: Kubeapps Meets Tanzu Kubernetes Grid: a New Release is out</title>
      
      <link>/blog/kubeapps-meets-tanzu-kubernetes-grid-a-new-release-is-out/</link>
      <pubDate>Fri, 11 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/kubeapps-meets-tanzu-kubernetes-grid-a-new-release-is-out/</guid>
      <description>

        
        &lt;p&gt;&lt;em&gt;Special thanks to Antonio Gamez and Michael Nelson, members of the VMware Kubeapps Team&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The latest version of &lt;a href=&#34;https://github.com/kubeapps/kubeapps/releases/tag/v2.3.2&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Kubeapps (v.2.3.2)&lt;/a&gt; is now available for deployment on VMware Tanzu™ Kubernetes Grid™ (TKG) workload clusters. VMware Tanzu users already benefit from deploying Kubeapps in several environments and, now with a little configuration Kubeapps can be integrated with your TKG workload cluster.&lt;/p&gt;
&lt;p&gt;In addition to this capability,  Kubeapps also features full compatibility with the latest versions of &lt;a href=&#34;https://pinniped.dev/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Pinniped&lt;/a&gt; which means that it can be used with &lt;a href=&#34;https://github.com/kubeapps/kubeapps/blob/7aa7c579251e0fb5b446ab71a67d8d847d6ce843/docs/user/using-an-OIDC-provider-with-pinniped.md#enabling-oidc-login-in-managed-clusters&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;any OIDC provider for your TKG clusters and even in managed clusters&lt;/a&gt; such as Azure Kubernetes Service (AKS) and Google Kubernetes Engine (GKE).&lt;/p&gt;
&lt;p&gt;Want to know more? Keep reading to discover the latest capabilities of Kubeapps that will enable developers and admin clusters to deploy and manage trusted open-source content in TKG clusters.&lt;/p&gt;
&lt;h2 id=&#34;a-bit-of-history-what-is-kubeapps&#34;&gt;A bit of History: What is Kubeapps?&lt;/h2&gt;
&lt;p&gt;Kubeapps is an in-cluster web-based application that enables users with a one-time installation to deploy, manage, and upgrade applications on a Kubernetes cluster.&lt;/p&gt;
&lt;p&gt;This past year, the Kubeapps team has added key new features to support different use cases and scenarios. Firstly, we added &lt;a href=&#34;https://blog.bitnami.com/2020/05/kubeapps-now-supports-private-docker-registries.html&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;support for private Helm and Docker registries&lt;/a&gt; and later, in &lt;a href=&#34;https://blog.bitnami.com/2020/10/Kubeapps-2.0.html&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Kubeapps version 2.0&lt;/a&gt;, we built support to run Kubeapps on various VMware Tanzu ™ platforms such as Tanzu ™ Mission Control, vSphere and Tanzu ™ Kubernetes Grid.&lt;/p&gt;
&lt;p&gt;With Kubeapps you can:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Customize deployments through an intuitive, form-based user interface&lt;/li&gt;
&lt;li&gt;Inspect, upgrade and delete applications installed in the cluster&lt;/li&gt;
&lt;li&gt;Browse and deploy from public or private chart repositories including VMware Marketplace™ and Bitnami Application Catalog&lt;/li&gt;
&lt;li&gt;Secure authentication to Kubeapps using an OAuth2/OIDC provider such as the VMware Cloud Service Portal&lt;/li&gt;
&lt;li&gt;Secure authorization based on Kubernetes role-based access control&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;key-features-of-kubeapps-232&#34;&gt;Key Features of Kubeapps 2.3.2&lt;/h2&gt;
&lt;p&gt;In this Kubeapps release, we have focused on delivering key user experience features including the capability to enable Tanzu users to deploy Kubeapps directly as a Helm chart in TKG workload clusters. This version is tested and validated on the latest version of TKG (v1.3.1)&lt;/p&gt;
&lt;p&gt;Once Kubeapps is up and running, cluster admins will benefit from having :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SSO for Authentication with TKG using Pinniped by configuring an OIDC provider&lt;/li&gt;
&lt;li&gt;Ability to configure VMware Tanzu™ Application Catalog (TAC) as a private Chart repository;&lt;/li&gt;
&lt;li&gt;Capability to configure VMware Marketplace ™ Catalog and the Bitnami Application Catalog as public chart repositories;&lt;/li&gt;
&lt;li&gt;A customized user interface adapted to the Tanzu look and feel.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/images/blogs/kubeapps-232-release/kubeapps-tkg.png&#34; alt=&#34;alt_text&#34;  title=&#34;Kubeapps support for SSO Authentication&#34; /&gt;
&lt;em&gt;Kubeapps support for SSO Authentication&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;All these new capabilities are designed to offer a seamless experience between Kubeapps and Tanzu Kubernetes Grid clusters.&lt;/p&gt;
&lt;h2 id=&#34;how-can-i-configure-kubeapps-to-run-in-my-tkg-clusters&#34;&gt;How can I configure Kubeapps to run in my TKG clusters?&lt;/h2&gt;
&lt;p&gt;Tanzu users can execute these simple steps to gain the maximum advantage with this new version of Kubeapps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Configure your cluster to enable SSO for Authentication with TKG using Pinniped and integrate Kubeapps with the identity management provider&lt;/li&gt;
&lt;li&gt;Adjust the Kubeapps user interface to get a customized look and feel&lt;/li&gt;
&lt;li&gt;Configure role-based access control in Kubeapps (RBAC) to manage roles and permissions among the teams in your organization&lt;/li&gt;
&lt;li&gt;Deploy Kubeapps in the cluster&lt;/li&gt;
&lt;li&gt;Add public and private repositories to Kubeapps: the public VMware Marketplace™ repository and your private &lt;a href=&#34;https://tanzu.vmware.com/application-catalog&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;VMware Tanzu Application Catalog&lt;/a&gt; for &lt;a href=&#34;https://tanzu.vmware.com/tanzu/advanced&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Tanzu Advanced&lt;/a&gt; repository&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;At this point your development team can start deploying, listing, removing and managing applications in your TKG clusters from the Kubeapps user interface with total confidence!&lt;/p&gt;
&lt;p&gt;Refer to the Kubeapps documentation to learn how to &lt;a href=&#34;https://github.com/kubeapps/kubeapps/tree/master/docs/step-by-step/kubeapps-on-tkg&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;deploy and configure Kubeapps on VMware Tanzu Kubernetes Grid.&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;support-and-resources&#34;&gt;Support and Resources&lt;/h2&gt;
&lt;p&gt;Since Kubeapps is an OSS project, support for this version of Kubeapps will be provided on a best-effort basis.&lt;/p&gt;
&lt;p&gt;For solving the problems you may have (including deployment support, operational support and bug fixes), please &lt;a href=&#34;https://github.com/kubeapps/kubeapps/issues&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;open an issue in the Kubeapps GitHub repository.&lt;/a&gt; A markdown template is provided by default to open new issues with the information requested to prioritize and respond to them as soon as possible.&lt;/p&gt;
&lt;p&gt;Also, if you want to contribute to the project, feel free to &lt;a href=&#34;https://github.com/kubeapps/kubeapps/pulls&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;send us a pull request,&lt;/a&gt; and the team will check it and guide you in the process for a successful merge.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kubeapps/kubeapps/tree/master/docs&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;The Kubeapps documentation section&lt;/a&gt; is full of useful resources to help you get the best of the chart.&lt;/p&gt;
&lt;p&gt;Check out the step-by-step guide for &lt;a href=&#34;https://github.com/kubeapps/kubeapps/tree/master/docs/step-by-step/kubeapps-on-tkg&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;deploying and configuring Kubeapps on VMware Tanzu™ Kubernetes Grid™&lt;/a&gt; and the &lt;a href=&#34;https://docs.bitnami.com/tutorials/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami documentation tutorials site&lt;/a&gt; for improving your Kubernetes skills.&lt;/p&gt;
&lt;p&gt;For more information on VMware Tanzu Kubernetes Grid, refer to &lt;a href=&#34;https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/index.html&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;its documentation page&lt;/a&gt;  where you will find handy information on managing your Kubernetes clusters.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      
      <title>Workshops: Spring Booternetes</title>
      
      <link>/workshops/lab-spring-booternetes/</link>
      <pubDate>Tue, 11 May 2021 00:00:00 +0000</pubDate>
      
      <guid>/workshops/lab-spring-booternetes/</guid>
      <description>

        
        &lt;p&gt;During this workshop you will move an application comprising 4 Spring Boot apps to Kubernetes, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Test the app locally using Spring Cloud Config Server and Eureka to understand its behavior&lt;/li&gt;
&lt;li&gt;Replatform the apps to Kubernetes using Spring Cloud Kubernetes (without Spring Cloud Config Server and Eureka)&lt;/li&gt;
&lt;li&gt;Replatform the apps to Kubernetes without using Spring Cloud Kubernetes&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      
      <title>Guides: Getting Started with kapp</title>
      
      <link>/guides/kubernetes/kapp-gs/</link>
      <pubDate>Wed, 14 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/guides/kubernetes/kapp-gs/</guid>
      <description>

        
        &lt;p&gt;&lt;a href=&#34;https://carvel.dev/kapp/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;kapp&lt;/a&gt; (part of the open source &lt;a href=&#34;https://carvel.dev&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Carvel&lt;/a&gt; suite) is a lightweight application-centric tool for deploying resources on Kubernetes. Being both explicit and application-centric it provides an easier way to deploy and view all resources created together regardless of what namespace they’re in. Being dependency-aware, it is able to wait for resources to be created, updated, or deleted, and provides a live status on the progress of the actions. Continue on to see how to get started with kapp.&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;Before you get started you will need to do the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Create a Kubernetes cluster&lt;/em&gt;:&lt;/li&gt;
&lt;li&gt;Install &lt;em&gt;&lt;code&gt;kubectl&lt;/code&gt;&lt;/em&gt; locally&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Install the &lt;code&gt;kapp&lt;/code&gt; CLI&lt;/em&gt; via one of these options:
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/vmware-tanzu/homebrew-carvel&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Homebrew&lt;/a&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;brew tap vmware-tanzu/carvel
brew install kapp
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/vmware-tanzu/carvel-kapp/releases/tag/v0.35.0&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;GitHub releases&lt;/a&gt;: move it to &lt;code&gt;/usr/local/bin&lt;/code&gt; or add it to your &lt;code&gt;$PATH&lt;/code&gt; and run &lt;code&gt;chmod +x&lt;/code&gt; to make it executable&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Note: This guide uses Kapp v0.35.0, and we suggest you download and install the same version to ensure the best experience as you follow through this guide.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;deploy-app&#34;&gt;Deploy App&lt;/h2&gt;
&lt;p&gt;Let’s go ahead and deploy our first application with kapp!&lt;/p&gt;
&lt;h3 id=&#34;first-run&#34;&gt;First Run&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Create a namespace to use as a &lt;a href=&#34;https://carvel.dev/kapp/docs/latest/state-namespace/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;state namespace&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create namespace kapp-demo
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create &lt;code&gt;kapp-spring-petclinic.yaml&lt;/code&gt; file with a &lt;a href=&#34;https://spring-petclinic.github.io/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Spring PetClinic&lt;/a&gt; namespace, deployment, and service:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;l&#34;&gt;cat &amp;lt;&amp;lt;EOF &amp;gt; kapp-spring-petclinic.yaml&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;---&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Namespace&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;spring-petclinic&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;---&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;apps/v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Deployment&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;spring-petclinic&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;namespace&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;spring-petclinic&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;selector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;matchLabels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;app&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;spring-petclinic&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;replicas&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;template&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;labels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;app&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;spring-petclinic&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;containers&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;spring-petclinic&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;springcommunity/spring-framework-petclinic&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;ports&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;http&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;containerPort&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;8080&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;---&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Service&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;spring-petclinic&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;namespace&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;spring-petclinic&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;labels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;app&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;spring-petclinic&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;ports&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;port&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;80&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;protocol&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;TCP&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;targetPort&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;8080&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;selector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;app&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;spring-petclinic&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;l&#34;&gt;EOF&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Here is where  &lt;code&gt;kapp&lt;/code&gt; gets involved. We will use it to create our deployment and service.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kapp deploy -n kapp-demo -a spring-petclinic -f kapp-spring-petclinic.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This format is fairly similar to &lt;code&gt;kubectl apply -f kapp-spring-petclinic.yaml&lt;/code&gt;, but it also has an application name. You can also provide a directory instead of a single file.&lt;/p&gt;
&lt;p&gt;The next difference you will notice is it prompts you if you want to actually run the command. You can shortcut this by adding the &lt;code&gt;-y&lt;/code&gt; flag.&lt;/p&gt;
&lt;p&gt;It should give you an output similar to the following before hit &lt;code&gt;y&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Target cluster &#39;https://35.247.3.14&#39; (nodes: demo-default-pool-0f8526ab-rb7b, 2+)

Changes

Namespace         Name              Kind        Conds.  Age  Op      Op st.  Wait to    Rs  Ri
(cluster)         spring-petclinic  Namespace   -       -    create  -       reconcile  -   -  
spring-petclinic  spring-petclinic  Deployment  -       -    create  -       reconcile  -   -  
^                 spring-petclinic  Service     -       -    create  -       reconcile  -   -  

Op:      3 create, 0 delete, 0 update, 0 noop
Wait to: 3 reconcile, 0 delete, 0 noop
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You can see that it’s creating the &lt;code&gt;spring-petclinic&lt;/code&gt; namespace, deployment, and service inside that namespace.&lt;/p&gt;
&lt;p&gt;Here is a more significant difference with &lt;code&gt;kubectl&lt;/code&gt;. &lt;code&gt;kapp&lt;/code&gt; will wait on the resources to become available before terminating and will also show the progress for each resource and tell you if it succeeded or failed. Don’t worry, I didn’t set you up for failure :).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Now that we have the deployment and service up, we can verify it’s working properly. First we can just take a look and see that they are up.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl -n spring-petclinic get all
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And then we can create a container and curl the service from within it. We need to do this right now since the service type is &lt;code&gt;ClusterIP&lt;/code&gt; (default) so we can’t see it from outside of the cluster itself.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl run -it --rm --restart=Never curl --image=curlimages/curl spring-petclinic.spring-petclinic.svc.cluster.local
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;looking-at-the-app-with-kapp&#34;&gt;Looking At the App with kapp&lt;/h3&gt;
&lt;p&gt;Okay, so let’s use kapp instead to take a look at our application.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;We can take a look at everything we have running with kapp using the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kapp list -A
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If you didn’t run anything else prior, we should just see Spring PetClinic. You will notice here kapp uses &lt;code&gt;kapp-demo&lt;/code&gt; as the state namespace. This is separate from the &lt;code&gt;spring-petclinic&lt;/code&gt; namespace(s) used in the deployment/service.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Target cluster &#39;https://35.247.3.14&#39; (nodes: demo-default-pool-0f8526ab-rb7b, 2+)

Apps in namespace &#39;kapp-demo&#39;

Name              Namespaces             Lcs    Lca
spring-petclinic  spring-petclinic       false  22h

Lcs: Last Change Successful
Lca: Last Change Age

1 apps
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If we had other applications running in the &lt;code&gt;spring-petclinic&lt;/code&gt; namespace when we ran &lt;code&gt;kubectl -n spring-petclinic get all&lt;/code&gt;, then we would see everything there and not just our application. We could add labels and filter on them, but then we have to make sure to do that with everything in that application. Well, the good news is that kapp does it for you. You can see the labels by running this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl -n spring-petclinic get all --show-labels
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And if we want to dig into one resource we can see labels there:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl -n spring-petclinic get deployment spring-petclinic -o=jsonpath=&#39;{.metadata.labels}&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To see all of the resources &lt;code&gt;kapp&lt;/code&gt; created for the app, we use &lt;code&gt;kapp inspect&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kapp -n kapp-demo inspect -a spring-petclinic
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We’ll come back to this in a bit to dig further into what kapp does to keep track of resources and changes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We can also look at logs, which is especially useful if something fails after the pods are up. &lt;code&gt;kapp logs&lt;/code&gt; will show all pod logs in the app.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kapp -n kapp-demo logs -a spring-petclinic
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;update-app&#34;&gt;Update App&lt;/h3&gt;
&lt;p&gt;Well, now we want to make it so we can share our app with other people without them needing to be in the cluster.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Let’s make a change to the YAML file and try running it again.
To reach the service externally, we can change it to type &lt;code&gt;LoadBalancer&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;echo &#39;  type: LoadBalancer&#39; &amp;gt;&amp;gt; kapp-spring-petclinic.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And let’s run our &lt;code&gt;kapp&lt;/code&gt; command again.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kapp deploy -n kapp-demo -a spring-petclinic -f kapp-spring-petclinic.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This time our output should like this where we see the change is that the service is being updated.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Target cluster &#39;https://35.247.3.14&#39; (nodes: demo-default-pool-0f8526ab-rb7b, 2+)

Changes

Namespace         Name              Kind     Conds.  Age  Op      Op st.  Wait to    Rs  Ri
spring-petclinic  spring-petclinic  Service  -       1m   update  -       reconcile  ok  -  

Op:      0 create, 0 delete, 1 update, 0 noop
Wait to: 1 reconcile, 0 delete, 0 noop
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Here we can see that kapp knows that only the service should be changing. Kapp keeps track by creating a &lt;a href=&#34;https://kubernetes.io/docs/concepts/configuration/configmap/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;ConfigMap&lt;/a&gt; when the resources are first created and then every time there is a change. To see the ConfigMaps in this namespace you can run:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl -n spring-petclinic get configmaps
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Let’s continue on.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Now if we run &lt;code&gt;kubectl get&lt;/code&gt;, we should see an &lt;code&gt;EXTERNAL_IP&lt;/code&gt; for the service.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl -n spring-petclinic get all
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You can use curl, or you can open that IP in your browser which will look like the following:
&lt;img src=&#34;/images/guides/kubernetes/kapp/spring-petclinic-main.png&#34; alt=&#34;&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Congrats, you did it!&lt;/p&gt;
&lt;p&gt;To see what else you can do, run the following or go to the &lt;a href=&#34;https://carvel.dev/kapp/docs/latest/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;docs&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kapp -h
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;cleanup&#34;&gt;Cleanup&lt;/h2&gt;
&lt;p&gt;Now, if you want, you can delete what was created in this guide.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;To delete the app you can run the following command. Make sure it is wanting to only delete what you are trying to delete before agreeing.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kapp delete -n kapp-demo -a spring-petclinic
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Mine looks like the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Target cluster &#39;https://35.247.3.14&#39; (nodes: demo-default-pool-0f8526ab-rb7b, 2+)

Changes

Namespace         Name                               Kind        Conds.  Age  Op      Op st.  Wait to  Rs  Ri
spring-petclinic  spring-petclinic                   Deployment  2/2 t   4h   delete  -       delete   ok  -
^                 spring-petclinic                   Endpoints   -       4h   -       -       delete   ok  -
^                 spring-petclinic                   Service     -       4h   delete  -       delete   ok  -
^                 spring-petclinic-6bdff5c97c        ReplicaSet  -       4h   -       -       delete   ok  -
^                 spring-petclinic-6bdff5c97c-84ksw  Pod         4/4 t   4h   -       -       delete   ok  -
^                 spring-petclinic-6bdff5c97c-bdhd5  Pod         4/4 t   4h   -       -       delete   ok  -
^                 spring-petclinic-6bdff5c97c-bdhd5  PodMetrics  -       1s   -       -       delete   ok  -

Op:      0 create, 2 delete, 0 update, 5 noop
Wait to: 0 reconcile, 7 delete, 0 noop
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Delete namespace:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl delete namespace kapp-demo
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;learn-more&#34;&gt;Learn More&lt;/h2&gt;
&lt;p&gt;To learn more, here are some resources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://carvel.dev/kapp/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;kapp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/vmware-tanzu/carvel-kapp-controller&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;kapp-controller&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cncf.io/webinars/cncf-live-webinar-how-to-manage-kubernetes-application-lifecycle-using-carvel/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;CNCF Live Webinar: How to Manage Kubernetes Application Lifecycle Using Carvel (Feb 9, 2021)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://carvel.dev/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Carvel Toolset&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      
      <title>Workshops: Spring on Kubernetes</title>
      
      <link>/workshops/spring-on-kubernetes/</link>
      <pubDate>Tue, 09 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>/workshops/spring-on-kubernetes/</guid>
      <description>

        
        &lt;p&gt;During this workshop you will learn the finer details of how to create, build, run, and debug a basic Spring Boot app on
Kubernetes by doing the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create a basic Spring Boot app&lt;/li&gt;
&lt;li&gt;Build a Docker image for the app&lt;/li&gt;
&lt;li&gt;Push the image to a Docker registry&lt;/li&gt;
&lt;li&gt;Deploy and run the app on Kubernetes&lt;/li&gt;
&lt;li&gt;Test the app using port-forwarding and ingress&lt;/li&gt;
&lt;li&gt;Use skaffold to iterate easily as you work on your app&lt;/li&gt;
&lt;li&gt;Use kustomize to manage configurations across environments&lt;/li&gt;
&lt;li&gt;Externalize application configuration using ConfigMaps&lt;/li&gt;
&lt;li&gt;Use service discovery for app-to-app communication&lt;/li&gt;
&lt;li&gt;Deploy the Spring PetClinic App with MySQL&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      
      <title>Workshops: Getting Started with Carvel (formerly k14s)</title>
      
      <link>/workshops/lab-getting-started-with-carvel/</link>
      <pubDate>Thu, 04 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>/workshops/lab-getting-started-with-carvel/</guid>
      <description>

        
        &lt;p&gt;Carvel&amp;rsquo;s GitHub contains several tools we created as a result of working with complex, multi-purpose tools like Helm. We believe that working with simple, single-purpose tools that easily interoperate with one another results in a better, workflow compared to the all-in-one approach chosen by Helm. We have found this approach to be easier to understand and debug.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      
      <title>Workshops: Getting Started with Octant</title>
      
      <link>/workshops/lab-getting-started-with-octant/</link>
      <pubDate>Thu, 04 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>/workshops/lab-getting-started-with-octant/</guid>
      <description>

        
        &lt;p&gt;Octant is an open source, developer-centric web interface for Kubernetes that lets you inspect a Kubernetes cluster and its applications. It provides an alternative to the de facto Kubernetes dashboard typically available with a Kubernetes cluster. Whereas the Kubernetes dashboard would be hosted in the cluster, Octant is deployed to your own local desktop machine. Octant works with your local Kubernetes client configuration, meaning you can use it with whatever Kubernetes cluster you’re working with.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      
      <title>Workshops: Kubernetes Fundamentals</title>
      
      <link>/workshops/lab-k8s-fundamentals/</link>
      <pubDate>Thu, 04 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>/workshops/lab-k8s-fundamentals/</guid>
      <description>

        
        &lt;p&gt;This workshop is intended to give you a quick, hands-on introduction to using Kubernetes. In the process, you’ll learn about some of the fundamental concepts of Kubernetes when deploying applications to it. The focus will be on what a developer would need to know to use the platform. It’s not a workshop on how to run the Kubernetes platform.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      
      <title>Workshops: Spring Boot Probes on Kubernetes</title>
      
      <link>/workshops/lab-spring-boot-k8s-probes/</link>
      <pubDate>Thu, 04 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>/workshops/lab-spring-boot-k8s-probes/</guid>
      <description>

        
        &lt;p&gt;This lab shows you how to add liveness and readiness probes to a Spring Boot application in Kubernetes. This lab will cover the following tasks:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Add some endpoints to a Spring Boot application and build and push a Docker image&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Configure the probes in a few lines of YAML&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Deploy the image as a container in Kubernetes&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;

      </description>
    </item>
    
  </channel>
</rss>
