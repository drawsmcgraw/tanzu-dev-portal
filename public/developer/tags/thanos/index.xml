<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>VMware Tanzu Developer Center â€“ Thanos</title>
    <link>/tags/thanos/</link>
    <description>Recent content in Thanos on VMware Tanzu Developer Center</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Wed, 11 Mar 2020 00:00:00 +0000</lastBuildDate>
    
	  <atom:link href="/tags/thanos/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      
      <title>Guides: Create a Multi-Cluster Monitoring Dashboard with Thanos, Grafana and Prometheus</title>
      
      <link>/guides/kubernetes/prometheus-multicluster-monitoring/</link>
      <pubDate>Wed, 11 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/guides/kubernetes/prometheus-multicluster-monitoring/</guid>
      <description>

        
        &lt;p&gt;&lt;a href=&#34;https://prometheus.io/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Prometheus&lt;/a&gt;, coupled with
&lt;a href=&#34;https://grafana.com/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Grafana&lt;/a&gt;, is a popular monitoring solution for Kubernetes
clusters. It allows SRE teams and developers to capture metrics and telemetry
data for applications running in a cluster, allowing deeper insights into
application performance and reliability.&lt;/p&gt;
&lt;p&gt;The Prometheus/Grafana combination works well for individual clusters, but as
teams scale out and start working with multiple clusters, monitoring
requirements become correspondingly more complex. For effective multi-cluster
monitoring, a &amp;ldquo;single pane of glass&amp;rdquo; with centralized real-time monitoring, time
series comparisons across and within clusters and high availability is essential
for teams operating with multiple clusters and multiple providers.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://thanos.io/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Thanos&lt;/a&gt; is a monitoring system that aggregates data from
multiple Prometheus deployments. This data can then be inspected and analyzed
using Grafana, just as with regular Prometheus metrics. Although this setup
sounds complex, it&amp;rsquo;s actually very easy to achieve with the following Bitnami
Helm charts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bitnami/charts/tree/master/bitnami/prometheus-operator&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami&amp;rsquo;s Prometheus Operator Helm chart&lt;/a&gt;
lets you deploy Prometheus in your Kubernetes cluster with an additional
Thanos sidecar container.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bitnami/charts/tree/master/bitnami/thanos&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami&amp;rsquo;s Thanos Helm chart&lt;/a&gt;
lets you deploy all the Thanos components together with MinIO and Alertmanager
so you can quickly bootstrap a Thanos deployment.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bitnami/charts/tree/master/bitnami/grafana&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami&amp;rsquo;s Grafana Helm chart&lt;/a&gt;
lets you deploy Grafana in your Kubernetes cluster.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This guide walks you through the process of using these charts to create a
Thanos deployment that aggregates data from Prometheus Operators in multiple
clusters and allows further monitoring and analysis using Grafana.&lt;/p&gt;
&lt;h2 id=&#34;assumptions-and-prerequisites&#34;&gt;Assumptions and prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;You have three separate multi-node Kubernetes clusters running on the same
cloud provider:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Two &amp;ldquo;data producer&amp;rdquo; clusters which will host Prometheus deployments and
applications that expose metrics via Prometheus.&lt;/li&gt;
&lt;li&gt;One &amp;ldquo;data aggregator&amp;rdquo; cluster which will host Thanos and aggregate the data
from the data producers. This cluster will also host Grafana for data
visualization and reporting.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You have the &lt;em&gt;kubectl&lt;/em&gt; CLI and the Helm v3.x package manager installed and configured to work with your Kubernetes clusters. &lt;a href=&#34;https://docs.bitnami.com/kubernetes/get-started-kubernetes#step-3-install-kubectl-command-line&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Learn how to install &lt;em&gt;kubectl&lt;/em&gt; and Helm v3.x&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This guide uses clusters hosted on the Google Kubernetes Engine (GKE) service
but you can use any Kubernetes provider. Learn about
&lt;a href=&#34;https://docs.bitnami.com/kubernetes/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;deploying a Kubernetes cluster on different cloud platforms&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;step-1-install-the-prometheus-operator-on-each-cluster&#34;&gt;Step 1: Install the Prometheus Operator on each cluster&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/bitnami/charts/tree/master/bitnami/prometheus-operator&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami&amp;rsquo;s Prometheus Operator chart&lt;/a&gt;
provides easy monitoring definitions for Kubernetes services and management of
Prometheus instances. It also includes an optional Thanos sidecar container,
which can be used by your Thanos deployment to access cluster metrics.&lt;/p&gt;
&lt;p&gt;Only one instance of the Prometheus Operator component should be running in a
cluster.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Add the Bitnami charts repository to Helm:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;helm repo add bitnami https://charts.bitnami.com/bitnami
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install the Prometheus Operator in the first &amp;ldquo;data producer&amp;rdquo; cluster using the command below:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;helm install prometheus-operator &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --set prometheus.thanos.create&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;true&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --set operator.service.type&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;ClusterIP &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --set prometheus.service.type&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;ClusterIP &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --set alertmanager.service.type&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;ClusterIP &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --set prometheus.thanos.service.type&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;LoadBalancer &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --set prometheus.externalLabels.cluster&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;data-producer-0&amp;#34;&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  bitnami/prometheus-operator
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The &lt;em&gt;prometheus.thanos.create&lt;/em&gt; parameter creates a Thanos sidecar container,
while the &lt;em&gt;prometheus.thanos.service.type&lt;/em&gt; parameter makes the sidecar service
available at a public load balancer IP address. Note the
&lt;em&gt;prometheus.externalLabels&lt;/em&gt; parameter which lets you define one or more unique
labels per Prometheus instance - these labels are useful to differentiate
different stores or data sources in Thanos.&lt;/p&gt;
&lt;p&gt;The command above exposes the Thanos sidecar container in each cluster at a
public IP address using a &lt;em&gt;LoadBalancer&lt;/em&gt; service. This makes it easy for
Thanos to access Prometheus metrics in different clusters without needing any
special firewall or routing configuration. However, this approach is highly
insecure and should be used only for demonstration or testing purposes. In
production environments, it is preferable to deploy an NGINX Ingress
Controller to control access from outside the cluster and further limit access
using whitelisting and other security-related configuration.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the command below to obtain the public IP address of the sidecar service.
You will use this IP address in the next step.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl get svc | grep prometheus-operator-prometheus-thanos
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Repeat the steps shown above for the second &amp;ldquo;data producer&amp;rdquo; cluster. Use a
different value for the &lt;em&gt;prometheus.externalLabels.cluster&lt;/em&gt; parameter, such as
&lt;em&gt;data-producer-1&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&#34;step-2-install-and-configure-thanos&#34;&gt;Step 2: Install and configure Thanos&lt;/h2&gt;
&lt;p&gt;The next step is to install Thanos in the &amp;ldquo;data aggregator&amp;rdquo; cluster and
integrate it with Alertmanager and MinIO as the object store.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Modify your Kubernetes context to reflect the cluster on which you wish to install Thanos.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create a &lt;em&gt;values.yaml&lt;/em&gt; file as shown below. Replace the KEY placeholder with a
hard-to-guess value and the SIDECAR-SERVICE-IP-ADDRESS-X placeholders with the
public IP addresses of the Thanos sidecar containers in the &amp;ldquo;data producer&amp;rdquo;
clusters.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;objstoreConfig&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;|-&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;  type: s3
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;  config:
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;    bucket: thanos
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;    endpoint: {{ include &amp;#34;thanos.minio.fullname&amp;#34; . }}.monitoring.svc.cluster.local:9000
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;    access_key: minio
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;    secret_key: KEY
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;    insecure: true&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;querier&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;stores&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;SIDECAR-SERVICE-IP-ADDRESS-1:10901&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;SIDECAR-SERVICE-IP-ADDRESS-2:10901&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;bucketweb&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;enabled&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;true&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;compactor&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;enabled&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;true&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;storegateway&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;enabled&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;true&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;ruler&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;enabled&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;true&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;alertmanagers&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;l&#34;&gt;http://prometheus-operator-alertmanager.monitoring.svc.cluster.local:9093&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;config&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;|-&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;    groups:
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;      - name: &amp;#34;metamonitoring&amp;#34;
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;        rules:
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;          - alert: &amp;#34;PrometheusDown&amp;#34;
&lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;            expr: absent(up{prometheus=&amp;#34;monitoring/prometheus-operator&amp;#34;})&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;minio&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;enabled&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;true&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;accessKey&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;password&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;minio&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;secretKey&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;password&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;KEY&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;defaultBuckets&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;thanos&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install Thanos using the command below:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;helm install thanos bitnami/thanos &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --values values.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Wait for the deployment to complete and note the DNS name and port number for
the Thanos Querier service in the deployment output, as shown below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/prometheus-multicluster-monitoring/querier-service.png&#34; alt=&#34;Thanos Querier service&#34;  /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Follow the instructions shown in the chart output to connect to the Thanos
Querier Web interface and navigate to the &amp;ldquo;Stores&amp;rdquo; tab. Confirm that both
sidecar services are running and registered with Thanos, as shown below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/prometheus-multicluster-monitoring/querier-stores.png&#34; alt=&#34;Thanos Querier stores&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;Confirm also that each service displays a unique &lt;em&gt;cluster&lt;/em&gt; labelset, as configured in &lt;a href=&#34;#step-1-install-the-prometheus-operator-on-each-cluster&#34;&gt;Step 1&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;step-3-install-grafana&#34;&gt;Step 3: Install Grafana&lt;/h2&gt;
&lt;p&gt;The next step is to install Grafana, also on the same &amp;ldquo;data aggregator&amp;rdquo; cluster
as Thanos.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Use the command below, replacing GRAFANA-PASSWORD with a password for the
Grafana application:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;helm install grafana bitnami/grafana &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --set service.type&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;LoadBalancer &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --set admin.password&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;GRAFANA-PASSWORD
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Wait for the deployment to complete and obtain the public IP address for the
Grafana load balancer service:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl get svc &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; grep grafana
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Confirm that you are able to access Grafana by browsing to the load balancer
IP address on port 3000 and logging in with the username &lt;em&gt;admin&lt;/em&gt; and the
configured password. Here is what you should see:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/prometheus-multicluster-monitoring/grafana-dashboard.png&#34; alt=&#34;Grafana dashboard&#34;  /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;step-4-configure-grafana-to-use-thanos-as-a-data-source&#34;&gt;Step 4: Configure Grafana to use Thanos as a data source&lt;/h2&gt;
&lt;p&gt;Follow these steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;From the Grafana dashboard, click the &amp;ldquo;Add data source&amp;rdquo; button.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;On the &amp;ldquo;Choose data source type&amp;rdquo; page, select &amp;ldquo;Prometheus&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/prometheus-multicluster-monitoring/grafana-add-data-source.png&#34; alt=&#34;Grafana data source&#34;  /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;On the &amp;ldquo;Settings&amp;rdquo; page, set the URL for the Prometheus server to
&lt;em&gt;http://NAME:PORT&lt;/em&gt;, where NAME is the DNS name for the Thanos service obtained
at the end of &lt;a href=&#34;#step-2-install-and-configure-thanos&#34;&gt;Step 2&lt;/a&gt; and PORT is the
corresponding service port. Leave all other values at their default.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/prometheus-multicluster-monitoring/grafana-thanos-url.png&#34; alt=&#34;Grafana data source configuration&#34;  /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Click &amp;ldquo;Save &amp;amp; Test&amp;rdquo; to save and test the configuration. If everything is
configured correctly, you should see a success message like the one below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/prometheus-multicluster-monitoring/grafana-success.png&#34; alt=&#34;Grafana test&#34;  /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;step-5-test-the-system&#34;&gt;Step 5: Test the system&lt;/h2&gt;
&lt;p&gt;At this point, you can start deploying applications into your &amp;ldquo;data producer&amp;rdquo;
clusters and collating the metrics in Thanos and Grafana. For demonstration
purposes, this guide will deploy a MariaDB replication cluster using Bitnami&amp;rsquo;s
MariaDB Helm chart in each &amp;ldquo;data producer&amp;rdquo; cluster and display the metrics
generated by each MariaDB service in Grafana.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Deploy MariaDB in each cluster with one master and one slave using the
production configuration with the commands below. Replace the
MARIADB-ADMIN-PASSWORD and MARIADB-REPL-PASSWORD placeholders with the
database administrator account and replication account password respectively.
You can also optionally create a MariaDB user account for application use by
specifying values for the USER-PASSWORD, USER-NAME and DB-NAME placeholders.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;helm install mariadb &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --set rootUser.password&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;MARIADB-ADMIN-PASSWORD &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --set replication.password&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;MARIADB-REPL-PASSWORD &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --set db.user&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;USER-NAME &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --set db.password&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;USER-PASSWORD &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --set db.name&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;DB-NAME &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --set slave.replicas&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --set metrics.enabled&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;true&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --set metrics.serviceMonitor.enabled&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;true&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  bitnami/mariadb 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Note the &lt;em&gt;metrics.enabled&lt;/em&gt; parameter, which enables the Prometheus exporter
for MySQL server metrics, and the &lt;em&gt;metrics.serviceMonitor.enabled&lt;/em&gt; parameter,
which creates a Prometheus Operator ServiceMonitor.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Once deployment in each cluster is complete, note the instructions to connect
to each database service.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/prometheus-multicluster-monitoring/mariadb-service.png&#34; alt=&#34;MariaDB service&#34;  /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Browse to the
&lt;a href=&#34;https://github.com/percona/grafana-dashboards/blob/master/dashboards/MySQL_Overview.json&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;MySQL Overview dashboard in the Percona GitHub repository&lt;/a&gt;
and copy the JSON model.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Log in to Grafana. From the Grafana dashboard, click the &amp;ldquo;Import -&amp;gt; Dashboard&amp;rdquo;
menu item.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;On the &amp;ldquo;Import&amp;rdquo; page, paste the JSON model into the &amp;ldquo;Or paste JSON&amp;rdquo; field.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/prometheus-multicluster-monitoring/grafana-dashboard-import.png&#34; alt=&#34;Grafana dashboard import&#34;  /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Click &amp;ldquo;Load&amp;rdquo; to load the data and then &amp;ldquo;Import&amp;rdquo; to import the dashboard. The
new dashboard should appear in Grafana, as shown below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/prometheus-multicluster-monitoring/grafana-mysql-dashboard.png&#34; alt=&#34;Grafana MySQL dashboard&#34;  /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Connect to the MariaDB service in the first &amp;ldquo;data producer&amp;rdquo; cluster and
perform some actions, such as creating a database, adding records to a table
and executing a query. Perform similar actions in the second &amp;ldquo;data producer&amp;rdquo;
cluster. You should see your activity in each cluster reflected in the MySQL
Overview chart in Grafana, as shown below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/prometheus-multicluster-monitoring/grafana-metrics.png&#34; alt=&#34;MariaDB metrics in Grafana&#34;  /&gt;&lt;/p&gt;
&lt;p&gt;You can view metrics from individual master and slave nodes in each cluster by
selecting a different host in the &amp;ldquo;Host&amp;rdquo; drop down of the dashboard, as shown
below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/guides/kubernetes/prometheus-multicluster-monitoring/grafana-hosts.png&#34; alt=&#34;MariaDB hosts in Grafana&#34;  /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can now continue adding more applications to your clusters. So long as you
enable Prometheus metrics and a Prometheus Operator ServiceMonitor for each
deployment, Thanos will continuously receive and aggregate the metrics and you
can inspect them using Grafana.&lt;/p&gt;
&lt;h2 id=&#34;useful-links&#34;&gt;Useful links&lt;/h2&gt;
&lt;p&gt;To learn more about the topics discussed in this guide, use the links below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bitnami/charts/tree/master/bitnami/prometheus-operator&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami Prometheus Operator Helm chart&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bitnami/charts/tree/master/bitnami/thanos&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami&amp;rsquo;s Thanos Helm chart&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bitnami/charts/tree/master/bitnami/grafana&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami&amp;rsquo;s Grafana Helm chart&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bitnami/charts/tree/master/bitnami/mariadb&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Bitnami MariaDB Helm chart&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/ingress/tree/master/controllers/nginx&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;NGINX Ingress controller documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.bitnami.com/tutorials/secure-kubernetes-services-with-ingress-tls-letsencrypt/&#34; target=&#34;_blank&#34; rel=&#34;nofollow&#34;&gt;Secure Kubernetes Services with Ingress, TLS and Let&amp;rsquo;s Encrypt&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
  </channel>
</rss>
